{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ada97a6-c372-48f5-888c-1156a2c85a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1520</td><td>application_1765289937462_1506</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1506/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-156.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1506_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a842f020abd4a49a70dfb7aefbadd6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0ec4373dd9246cd912eb2993e241552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Paths for csv\n",
    "fcrime_2010_2019 = \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv\"\n",
    "fcrime_2020_present = \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\"\n",
    "fstations = \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Police_Stations.csv\"\n",
    "fincome = \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_income_2021.csv\"\n",
    "fcodes = \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/RE_codes.csv\"\n",
    "\n",
    "# Paths for GeoJSON\n",
    "fgeo = \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Census_Blocks_2020.geojson\"\n",
    "\n",
    "mo_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/MO_codes.txt\"\n",
    "fgeofields = \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Census_Blocks_2020_fields.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb077f89-c45c-4ef1-9aa8-98f5c58896a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ef14cf5eb844176b29d5ac38e3a7c48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    StructField, StructType, StringType, IntegerType, FloatType, DoubleType, \n",
    "    DateType, TimestampType\n",
    ")\n",
    "from pyspark.sql.functions import (\n",
    "    col, sum, count, expr, coalesce, lit, when,\n",
    "    year, avg, round, row_number, to_timestamp, \n",
    "    regexp_replace, to_date ,explode,split,trim,broadcast\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "422ed9ba-fa68-4fda-aada-ee788aeb5281",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1651</td><td>application_1765289937462_1635</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1635/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-207.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1635_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1350017d549e48b7b1152cbb7af71ab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbddc0201980427693928afb514fad44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the spark session\n",
    "from sedona.spark import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Advanced DB\") \\\n",
    "    .config('spark.executor.instances','4') \\\n",
    "    .config('spark.executor.cores','1') \\\n",
    "    .config('spark.executor.memory','2g') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sedona = SedonaContext.create(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dca8bc60-cab1-417a-a671-520eff5c4955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09a07f0d7a124cdd8be04262d5394a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total lines: 3138128"
     ]
    }
   ],
   "source": [
    "# Crimes table\n",
    "\n",
    "crimes_schema = StructType([\n",
    "    StructField(\"DR_NO\", StringType()),\n",
    "    StructField(\"DateRptd\", DateType()),\n",
    "    StructField(\"DATEOCC\", StringType()),\n",
    "    StructField(\"TIMEOCC\", StringType()),\n",
    "    StructField(\"AREA\", StringType()),\n",
    "    StructField(\"AREANAME\", StringType()),\n",
    "    StructField(\"RptDistNo\", StringType()),\n",
    "    StructField(\"Part\", IntegerType()),\n",
    "    StructField(\"CrmCd\", StringType()),\n",
    "    StructField(\"Crm Cd Desc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"Vict Age\", StringType()),\n",
    "    StructField(\"VictSex\", StringType()),\n",
    "    StructField(\"VictDescent\", StringType()),\n",
    "    StructField(\"PremisCd\", StringType()),\n",
    "    StructField(\"PremisDesc\", StringType()),\n",
    "    StructField(\"WeaponUsedCd\", StringType()),\n",
    "    StructField(\"WeaponDesc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"Status Desc\", StringType()),\n",
    "    StructField(\"CrmCd1\", StringType()),\n",
    "    StructField(\"CrmCd2\", StringType()),\n",
    "    StructField(\"CrmCd3\", StringType()),\n",
    "    StructField(\"CrmCd4\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"CrossStreet\", StringType()),\n",
    "    StructField(\"LAT\", FloatType()),\n",
    "    StructField(\"LON\", FloatType()),\n",
    "])\n",
    "\n",
    "crimes_df_10_19 = spark.read.csv(fcrime_2010_2019, header=True, schema=crimes_schema, dateFormat='MM/dd/yyyy hh:mm:ss a')\n",
    "crimes_df_20_present = spark.read.csv(fcrime_2020_present, header=True, schema=crimes_schema, dateFormat='MM/dd/yyyy hh:mm:ss a')\n",
    "\n",
    "crimes_df_all = crimes_df_10_19.union(crimes_df_20_present)\n",
    "print(f\"total lines: {crimes_df_all.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6146d5-b311-4444-99aa-4d45e3a26662",
   "metadata": {},
   "source": [
    "Query 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dda95c4-4f09-486c-bb4f-c913b3c1de7e",
   "metadata": {},
   "source": [
    "Υλοποίηση με DataFrame (Χωρίς UDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74708582-e42f-4f51-b56e-4db8c3153c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1abfd3f78a3454a8749822c6b4c9655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-------------+\n",
      "|Age_Group             |Total_Victims|\n",
      "+----------------------+-------------+\n",
      "|Adults: 25 to 64      |121660       |\n",
      "|Young Adults: 18 to 24|33758        |\n",
      "|Children: < 18        |16014        |\n",
      "|Elderly: >64          |6011         |\n",
      "+----------------------+-------------+\n",
      "\n",
      "Execution time DataFrame (without UDF): 10.1763 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, when, count, sum, col, regexp_replace\n",
    "\n",
    "start_time_df = time.time()\n",
    "\n",
    "# 1. Καθαρισμός και μετατροπή της στήλης Vict Age σε αριθμητικό τύπο\n",
    "crimes_df_cleaned = crimes_df_all.withColumn(\n",
    "    \"Vict_Age_Num\",\n",
    "    regexp_replace(col(\"Vict Age\"), \"[^0-9]\", \"\").cast(IntegerType())\n",
    ").filter(col(\"Vict_Age_Num\").isNotNull()) \n",
    "\n",
    "# 2. Φιλτράρισμα για \"aggravated assault\"\n",
    "assault_df = crimes_df_cleaned.filter(\n",
    "    col(\"Crm Cd Desc\").ilike(\"%aggravated assault%\")\n",
    ")\n",
    "\n",
    "# 3. Ομαδοποίηση σε ηλικιακές κατηγορίες\n",
    "age_groups_df = assault_df.withColumn(\n",
    "    \"Age_Group\",\n",
    "    when(col(\"Vict_Age_Num\") < 18, \"Children: < 18\")\n",
    "    .when((col(\"Vict_Age_Num\") >= 18) & (col(\"Vict_Age_Num\") <= 24), \"Young Adults: 18 to 24\")\n",
    "    .when((col(\"Vict_Age_Num\") >= 25) & (col(\"Vict_Age_Num\") <= 64), \"Adults: 25 to 64\")\n",
    "    .when(col(\"Vict_Age_Num\") > 64, \"Elderly: >64\")\n",
    "    .otherwise(\"Not specified\") \n",
    ")\n",
    "\n",
    "# 4. Καταμέτρηση και ταξινόμηση\n",
    "result_df = age_groups_df.groupBy(\"Age_Group\").agg(\n",
    "    count(\"*\").alias(\"Total_Victims\")\n",
    ").orderBy(col(\"Total_Victims\").desc())\n",
    "\n",
    "# Εμφάνιση αποτελεσμάτων και μέτρηση χρόνου\n",
    "result_df.show(truncate=False)\n",
    "end_time_df = time.time()\n",
    "time_df = end_time_df - start_time_df\n",
    "print(f\"Execution time DataFrame (without UDF): {time_df:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c683d4d0-ca7d-4053-9f1a-05dbd2ead687",
   "metadata": {},
   "source": [
    "Υλοποίηση με DataFrame (Με UDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e51b006e-eafd-46a1-b989-15ba21128628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a169644999741c185a8b0102d39f2c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-------------+\n",
      "|Age_Group             |Total_Victims|\n",
      "+----------------------+-------------+\n",
      "|Adults: 25 to 64      |121660       |\n",
      "|Young Adults: 18 to 24|33758        |\n",
      "|Children: < 18        |16014        |\n",
      "|Elderly: >64          |6011         |\n",
      "+----------------------+-------------+\n",
      "\n",
      "Execution time of DataFrame API (with UDF): 9.6997 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def get_age_group(age):\n",
    "    \"\"\"Καθορίζει την ηλικιακή ομάδα βάσει της ηλικίας.\"\"\"\n",
    "    if age is None:\n",
    "        return \"Άγνωστη/Μη Καθορισμένη\"\n",
    "    if age < 18:\n",
    "        return \"Children: < 18\"\n",
    "    elif 18 <= age <= 24:\n",
    "        return \"Young Adults: 18 to 24\"\n",
    "    elif 25 <= age <= 64:\n",
    "        return \"Adults: 25 to 64\"\n",
    "    elif age > 64:\n",
    "        return \"Elderly: >64\"\n",
    "    else:\n",
    "        return \"Not specified\"\n",
    "\n",
    "# Καταχώρηση του UDF\n",
    "get_age_group_udf = udf(get_age_group, StringType())\n",
    "\n",
    "start_time_udf = time.time()\n",
    "\n",
    "# Επανάληψη βημάτων 1 & 2 από πριν\n",
    "crimes_df_cleaned_udf = crimes_df_all.withColumn(\n",
    "    \"Vict_Age_Num\",\n",
    "    regexp_replace(col(\"Vict Age\"), \"[^0-9]\", \"\").cast(IntegerType())\n",
    ").filter(col(\"Vict_Age_Num\").isNotNull())\n",
    "\n",
    "assault_df_udf = crimes_df_cleaned_udf.filter(\n",
    "    col(\"Crm Cd Desc\").ilike(\"%aggravated assault%\")\n",
    ")\n",
    "\n",
    "# 3. Ομαδοποίηση σε ηλικιακές κατηγορίες με UDF\n",
    "age_groups_df_udf = assault_df_udf.withColumn(\n",
    "    \"Age_Group\",\n",
    "    get_age_group_udf(col(\"Vict_Age_Num\"))\n",
    ")\n",
    "\n",
    "# 4. Καταμέτρηση και ταξινόμηση\n",
    "result_df_udf = age_groups_df_udf.groupBy(\"Age_Group\").agg(\n",
    "    count(\"*\").alias(\"Total_Victims\")\n",
    ").orderBy(col(\"Total_Victims\").desc())\n",
    "\n",
    "# Εμφάνιση αποτελεσμάτων και μέτρηση χρόνου\n",
    "result_df_udf.show(truncate=False)\n",
    "end_time_udf = time.time()\n",
    "time_udf = end_time_udf - start_time_udf\n",
    "print(f\"Execution time of DataFrame API (with UDF): {time_udf:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8622f52a-ee94-4366-aacc-8e98c7d97e9c",
   "metadata": {},
   "source": [
    "Υλοποίηση με RDD API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eabf11a1-12ba-4f28-af7b-451c60762a04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "070eb824788d4784bac1a0013c2d5e62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+-------------------------+-------------+\n",
      "|Age_Group                |Total_Victims|\n",
      "+-------------------------+-------------+\n",
      "|Adults: 25 to 64         |   121660    |\n",
      "|Young Adults: 18 to 24   |    33758    |\n",
      "|Children: < 18           |    16014    |\n",
      "|Elderly: >64             |    6011     |\n",
      "+-------------------------+-------------+\n",
      "Execution time of RDD API: 19.8276 seconds"
     ]
    }
   ],
   "source": [
    "start_time_rdd = time.time()\n",
    "\n",
    "# 1. Μετατροπή σε RDD και επιλογή των απαραίτητων στηλών (Vict Age, Crm Cd Desc)\n",
    "crimes_rdd = crimes_df_all.rdd.map(lambda row: (row[\"Vict Age\"], row[\"Crm Cd Desc\"]))\n",
    "\n",
    "def get_age_group_rdd(age_str):\n",
    "    \"\"\"Καθαρίζει την ηλικία και βρίσκει την ηλικιακή ομάδα.\"\"\"\n",
    "    try:\n",
    "        # Καθαρισμός και μετατροπή σε int\n",
    "        age = int(\"\".join(filter(str.isdigit, age_str)))\n",
    "    except:\n",
    "        return None # Αγνοούμε μη αριθμητικές τιμές\n",
    "\n",
    "    if age < 18:\n",
    "        return \"Children: < 18\"\n",
    "    elif 18 <= age <= 24:\n",
    "        return \"Young Adults: 18 to 24\"\n",
    "    elif 25 <= age <= 64:\n",
    "        return \"Adults: 25 to 64\"\n",
    "    elif age > 64:\n",
    "        return \"Elderly: >64\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# 2. Φιλτράρισμα για \"aggravated assault\"\n",
    "assault_rdd = crimes_rdd.filter(\n",
    "    lambda row: row[1] is not None and \"aggravated assault\" in row[1].lower()\n",
    ")\n",
    "\n",
    "# 3. Ομαδοποίηση και καταμέτρηση\n",
    "result_rdd = assault_rdd \\\n",
    "    .map(lambda row: get_age_group_rdd(row[0])) \\\n",
    "    .filter(lambda group: group is not None) \\\n",
    "    .map(lambda group: (group, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b) \\\n",
    "    .sortBy(lambda pair: pair[1], ascending=False)\n",
    "\n",
    "# Εμφάνιση αποτελεσμάτων και μέτρηση χρόνου\n",
    "print(\"\\n+-------------------------+-------------+\")\n",
    "print(\"|Age_Group                |Total_Victims|\")\n",
    "print(\"+-------------------------+-------------+\")\n",
    "for group, count in result_rdd.collect():\n",
    "    print(f\"|{group:<25}|{count:^13}|\")\n",
    "print(\"+-------------------------+-------------+\")\n",
    "end_time_rdd = time.time()\n",
    "time_rdd = end_time_rdd - start_time_rdd\n",
    "print(f\"Execution time of RDD API: {time_rdd:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd476618-0fca-4b97-b793-b94409e0dadd",
   "metadata": {},
   "source": [
    "Query 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fce3764-2fe0-4623-ac80-2cf97a1633c8",
   "metadata": {},
   "source": [
    "Υλοποίηση με DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66c834cf-f1f7-4edc-81da-0c3036c555a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "450b2cb647c04e2c9a1b53085e2e8306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------------+-----+----+\n",
      "|year|Victim Descent        |#    |%   |\n",
      "+----+----------------------+-----+----+\n",
      "|2025|Unknown               |37   |38.1|\n",
      "|2025|Hispanic/Latin/Mexican|34   |35.1|\n",
      "|2025|White                 |13   |13.4|\n",
      "|2024|Unknown               |49188|38.6|\n",
      "|2024|Hispanic/Latin/Mexican|28576|22.4|\n",
      "|2024|White                 |22958|18.0|\n",
      "|2023|Hispanic/Latin/Mexican|69401|29.9|\n",
      "|2023|Unknown               |59529|25.6|\n",
      "|2023|White                 |44615|19.2|\n",
      "|2022|Hispanic/Latin/Mexican|73111|31.1|\n",
      "|2022|Unknown               |52130|22.2|\n",
      "|2022|White                 |46695|19.8|\n",
      "|2021|Hispanic/Latin/Mexican|63676|30.3|\n",
      "|2021|Unknown               |46499|22.2|\n",
      "|2021|White                 |44523|21.2|\n",
      "|2020|Hispanic/Latin/Mexican|61606|30.8|\n",
      "|2020|Unknown               |43958|22.0|\n",
      "|2020|White                 |42638|21.3|\n",
      "|2019|Hispanic/Latin/Mexican|72458|33.1|\n",
      "|2019|White                 |48863|22.3|\n",
      "|2019|Unknown               |36893|16.9|\n",
      "|2018|Hispanic/Latin/Mexican|75958|33.1|\n",
      "|2018|White                 |52233|22.7|\n",
      "|2018|Unknown               |36319|15.8|\n",
      "|2017|Hispanic/Latin/Mexican|78308|33.8|\n",
      "|2017|White                 |52744|22.8|\n",
      "|2017|Unknown               |35307|15.2|\n",
      "|2016|Hispanic/Latin/Mexican|99135|34.9|\n",
      "|2016|White                 |63760|22.5|\n",
      "|2016|Black                 |42449|15.0|\n",
      "|2015|Hispanic/Latin/Mexican|55978|33.3|\n",
      "|2015|White                 |44102|26.2|\n",
      "|2015|Black                 |26510|15.8|\n",
      "|2014|Hispanic/Latin/Mexican|68763|35.1|\n",
      "|2014|White                 |47531|24.3|\n",
      "|2014|Black                 |32952|16.8|\n",
      "|2013|Hispanic/Latin/Mexican|66741|34.6|\n",
      "|2013|White                 |48453|25.1|\n",
      "|2013|Black                 |31975|16.6|\n",
      "|2012|Hispanic/Latin/Mexican|70338|34.8|\n",
      "|2012|White                 |51839|25.7|\n",
      "|2012|Black                 |33572|16.6|\n",
      "|2011|Hispanic/Latin/Mexican|70845|35.3|\n",
      "|2011|White                 |51219|25.5|\n",
      "|2011|Black                 |32579|16.2|\n",
      "|2010|Hispanic/Latin/Mexican|73558|35.1|\n",
      "|2010|White                 |53835|25.7|\n",
      "|2010|Black                 |33937|16.2|\n",
      "+----+----------------------+-----+----+\n",
      "\n",
      "Execution time for DataFrame API: 10.2970 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, count, col, sum, row_number, when, to_timestamp, coalesce, round, lit\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "\n",
    "start_time_df = time.time()\n",
    "\n",
    "# Νέα στήλη Effective_Date (from string to timestamp)\n",
    "df_processing = crimes_df_all \\\n",
    "    .withColumn(\"Effective_Date\", to_timestamp(col(\"DATEOCC\"), \"yyyy MMM dd hh:mm:ss a\")) \\\n",
    "    .filter(col(\"Effective_Date\").isNotNull())\n",
    "\n",
    "# Προσθήκη στήλης 'year' \n",
    "df_with_year = df_processing.withColumn(\"year\", year(col(\"Effective_Date\")))\n",
    "\n",
    "# Ομαδοποίηση των φυλών\n",
    "df_grouped_descent = df_with_year.withColumn(\n",
    "    \"VictDescent_Code\",\n",
    "    coalesce(col(\"VictDescent\"), lit(\"X\")) # Αν η τιμή είναι NULL, βάζουμε 'X'\n",
    ").withColumn(\n",
    "    \"VictDescent_Code\",\n",
    "    when(col(\"VictDescent_Code\") == \"\", \"X\") \n",
    "    .otherwise(col(\"VictDescent_Code\"))\n",
    ")\n",
    "\n",
    "# Αντιστοίχιση Κωδικού με Πλήρες Όνομα\n",
    "df_with_names = df_grouped_descent.withColumn(\n",
    "    \"VictDescent_Name\",\n",
    "    when(col(\"VictDescent_Code\") == \"A\", lit(\"Other Asian\"))\n",
    "    .when(col(\"VictDescent_Code\") == \"B\", lit(\"Black\"))\n",
    "    .when(col(\"VictDescent_Code\") == \"C\", lit(\"Chinese\"))\n",
    "    .when(col(\"VictDescent_Code\") == \"D\", lit(\"Cambodian\"))\n",
    "    .when(col(\"VictDescent_Code\") == \"F\", lit(\"Filipino\"))\n",
    "    .when(col(\"VictDescent_Code\") == \"G\", lit(\"Guamanian\"))\n",
    "    .when(col(\"VictDescent_Code\") == \"H\", lit(\"Hispanic/Latin/Mexican\"))\n",
    "    .when(col(\"VictDescent_Code\") == \"I\", lit(\"American Indian/Alaskan Native\"))\n",
    "    .when(col(\"VictDescent_Code\") == \"J\", lit(\"Japanese\"))\n",
    "    .when(col(\"VictDescent_Code\") == \"K\", lit(\"Korean\"))\n",
    "    .when(col(\"VictDescent_Code\") == \"L\", lit(\"Laotian\"))\n",
    "    .when(col(\"VictDescent_Code\") == \"O\", lit(\"Other\"))\n",
    "    .when(col(\"VictDescent_Code\") == \"P\", lit(\"Pacific Islander\"))\n",
    "    .when(col(\"VictDescent_Code\") == \"S\", lit(\"Samoan\"))\n",
    "    .when(col(\"VictDescent_Code\") == \"U\", lit(\"Hawaiian\"))\n",
    "    .when(col(\"VictDescent_Code\") == \"V\", lit(\"Vietnamese\"))\n",
    "    .when(col(\"VictDescent_Code\") == \"W\", lit(\"White\"))\n",
    "    .when(col(\"VictDescent_Code\") == \"X\", lit(\"Unknown\"))\n",
    "    .when(col(\"VictDescent_Code\") == \"Z\", lit(\"Asian Indian\"))\n",
    "    .otherwise(col(\"VictDescent_Code\")))\n",
    "    \n",
    "# Υπολογισμός Συνολικού Αριθμού Θυμάτων ανά Έτος\n",
    "total_victims_per_year = df_grouped_descent.groupBy(\"year\").agg(\n",
    "    count(\"*\").alias(\"Total_Victims_Year\")\n",
    ")\n",
    "\n",
    "# Υπολογισμός Θυμάτων ανά Έτος και Φυλετικό Γκρουπ\n",
    "victims_per_group = df_with_names.groupBy(\"year\", \"VictDescent_Name\").agg(\n",
    "    count(\"*\").alias(\"Victims_Count\")\n",
    ")\n",
    "\n",
    "# Συνένωση, Υπολογισμός Ποσοστού και Window Function\n",
    "df_joined = victims_per_group.join(total_victims_per_year, on=\"year\")\n",
    "\n",
    "df_with_rank = df_joined.withColumn(\n",
    "    \"Percentage\",\n",
    "    (col(\"Victims_Count\") / col(\"Total_Victims_Year\") * 100)\n",
    ")\n",
    "\n",
    "window_spec = Window.partitionBy(\"year\").orderBy(col(\"Victims_Count\").desc())\n",
    "\n",
    "# Εφαρμογή της λειτουργίας row_number()\n",
    "df_ranked = df_with_rank.withColumn(\n",
    "    \"rank\",\n",
    "    row_number().over(window_spec)\n",
    ")\n",
    "\n",
    "#  Τελικό Φιλτράρισμα, Ταξινόμηση και Επιλογή Στηλών\n",
    "result_df_api = df_ranked.filter(col(\"rank\") <= 3) \\\n",
    "    .orderBy(col(\"year\").desc(), col(\"Victims_Count\").desc()) \\\n",
    "    .select(\n",
    "        col(\"year\"),\n",
    "        col(\"VictDescent_Name\").alias(\"Victim Descent\"), \n",
    "        col(\"Victims_Count\").alias(\"#\"),\n",
    "        round(col(\"Percentage\"), 1).alias(\"%\")\n",
    "    )\n",
    "\n",
    "result_df_api.show(1000, truncate=False)\n",
    "end_time_df = time.time()\n",
    "time_df = end_time_df - start_time_df\n",
    "print(f\"Execution time for DataFrame API: {time_df:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acee31ba-72e8-4985-807a-c4d962457228",
   "metadata": {},
   "source": [
    "Υλοποίηση με SQL API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c15554d9-92d6-4f94-b467-642b2eea30d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190002941d1146ada434d9dcaa46c37f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------------+-----+----+\n",
      "|year|Victim Descent        |#    |%   |\n",
      "+----+----------------------+-----+----+\n",
      "|2025|Unknown               |37   |38.1|\n",
      "|2025|Hispanic/Latin/Mexican|34   |35.1|\n",
      "|2025|White                 |13   |13.4|\n",
      "|2024|Unknown               |49188|38.6|\n",
      "|2024|Hispanic/Latin/Mexican|28576|22.4|\n",
      "|2024|White                 |22958|18.0|\n",
      "|2023|Hispanic/Latin/Mexican|69401|29.9|\n",
      "|2023|Unknown               |59531|25.6|\n",
      "|2023|White                 |44615|19.2|\n",
      "|2022|Hispanic/Latin/Mexican|73111|31.1|\n",
      "|2022|Unknown               |52130|22.2|\n",
      "|2022|White                 |46695|19.8|\n",
      "|2021|Hispanic/Latin/Mexican|63676|30.3|\n",
      "|2021|Unknown               |46499|22.2|\n",
      "|2021|White                 |44523|21.2|\n",
      "|2020|Hispanic/Latin/Mexican|61606|30.8|\n",
      "|2020|Unknown               |43958|22.0|\n",
      "|2020|White                 |42638|21.3|\n",
      "|2019|Hispanic/Latin/Mexican|72458|33.1|\n",
      "|2019|White                 |48863|22.3|\n",
      "|2019|Unknown               |36894|16.9|\n",
      "|2018|Hispanic/Latin/Mexican|75958|33.1|\n",
      "|2018|White                 |52233|22.7|\n",
      "|2018|Unknown               |36320|15.8|\n",
      "|2017|Hispanic/Latin/Mexican|78308|33.8|\n",
      "|2017|White                 |52744|22.8|\n",
      "|2017|Unknown               |35307|15.2|\n",
      "|2016|Hispanic/Latin/Mexican|99135|34.9|\n",
      "|2016|White                 |63760|22.5|\n",
      "|2016|Black                 |42449|15.0|\n",
      "|2015|Hispanic/Latin/Mexican|55978|33.3|\n",
      "|2015|White                 |44102|26.2|\n",
      "|2015|Black                 |26510|15.8|\n",
      "|2014|Hispanic/Latin/Mexican|68763|35.1|\n",
      "|2014|White                 |47531|24.3|\n",
      "|2014|Black                 |32952|16.8|\n",
      "|2013|Hispanic/Latin/Mexican|66741|34.6|\n",
      "|2013|White                 |48453|25.1|\n",
      "|2013|Black                 |31975|16.6|\n",
      "|2012|Hispanic/Latin/Mexican|70338|34.8|\n",
      "|2012|White                 |51839|25.7|\n",
      "|2012|Black                 |33572|16.6|\n",
      "|2011|Hispanic/Latin/Mexican|70845|35.3|\n",
      "|2011|White                 |51219|25.5|\n",
      "|2011|Black                 |32579|16.2|\n",
      "|2010|Hispanic/Latin/Mexican|73558|35.1|\n",
      "|2010|White                 |53835|25.7|\n",
      "|2010|Black                 |33937|16.2|\n",
      "+----+----------------------+-----+----+\n",
      "\n",
      "Execution time for SQL API: 5.8134 seconds"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "start_time_sql = time.time()\n",
    "\n",
    "# Καταχώρηση του ενωμένου DataFrame ως προσωρινό πίνακα\n",
    "crimes_df_all.createOrReplaceTempView(\"crimes_table\")\n",
    "\n",
    "sql_query_final = \"\"\"\n",
    "WITH Cleaned_Crimes AS (\n",
    "    -- Διόρθωση ημερομηνίας \n",
    "    SELECT\n",
    "        YEAR(TO_TIMESTAMP(DATEOCC, 'yyyy MMM dd hh:mm:ss a')) AS year,\n",
    "        \n",
    "        -- Εξαγωγή του κωδικού φυλής\n",
    "        CASE\n",
    "            WHEN COALESCE(VictDescent, 'X') = '' THEN 'X'\n",
    "            ELSE COALESCE(VictDescent, 'X')\n",
    "        END AS VictDescent_Code,\n",
    "        \n",
    "        -- Αντιστοίχιση κωδικού σε πλήρες όνομα\n",
    "        CASE COALESCE(VictDescent, 'X')\n",
    "            WHEN 'A' THEN 'Other Asian'\n",
    "            WHEN 'B' THEN 'Black'\n",
    "            WHEN 'C' THEN 'Chinese'\n",
    "            WHEN 'D' THEN 'Cambodian'\n",
    "            WHEN 'F' THEN 'Filipino'\n",
    "            WHEN 'G' THEN 'Guamanian'\n",
    "            WHEN 'H' THEN 'Hispanic/Latin/Mexican'\n",
    "            WHEN 'I' THEN 'American Indian/Alaskan Native'\n",
    "            WHEN 'J' THEN 'Japanese'\n",
    "            WHEN 'K' THEN 'Korean'\n",
    "            WHEN 'L' THEN 'Laotian'\n",
    "            WHEN 'O' THEN 'Other'\n",
    "            WHEN 'P' THEN 'Pacific Islander'\n",
    "            WHEN 'S' THEN 'Samoan'\n",
    "            WHEN 'U' THEN 'Hawaiian'\n",
    "            WHEN 'V' THEN 'Vietnamese'\n",
    "            WHEN 'W' THEN 'White'\n",
    "            WHEN 'Z' THEN 'Asian Indian'\n",
    "            ELSE 'Unknown' \n",
    "        END AS VictDescent_Name\n",
    "    FROM\n",
    "        crimes_table\n",
    "    WHERE\n",
    "        -- Φιλτράρουμε τις μη έγκυρες ημερομηνίες\n",
    "        TO_TIMESTAMP(DATEOCC, 'yyyy MMM dd hh:mm:ss a') IS NOT NULL\n",
    "),\n",
    "Victims_Per_Group AS (\n",
    "    -- Υπολογισμός θυμάτων ανά έτος και πλήρες όνομα, και σύνολο θυμάτων ανά έτος \n",
    "    SELECT\n",
    "        year,\n",
    "        VictDescent_Name,\n",
    "        COUNT(*) AS Victims_Count,\n",
    "        SUM(COUNT(*)) OVER (PARTITION BY year) AS Total_Victims_Year\n",
    "    FROM\n",
    "        Cleaned_Crimes\n",
    "    GROUP BY\n",
    "        year, VictDescent_Name\n",
    "),\n",
    "Victims_With_Rank AS (\n",
    "    -- Υπολογισμός ποσοστού και κατάταξης \n",
    "    SELECT\n",
    "        year,\n",
    "        VictDescent_Name,\n",
    "        Victims_Count,\n",
    "        (Victims_Count * 100.0 / Total_Victims_Year) AS Percentage,\n",
    "        ROW_NUMBER() OVER (\n",
    "            PARTITION BY year\n",
    "            ORDER BY Victims_Count DESC\n",
    "        ) AS rank\n",
    "    FROM\n",
    "        Victims_Per_Group\n",
    ")\n",
    "-- Τελικό Φιλτράρισμα και Ταξινόμηση\n",
    "SELECT\n",
    "    year,\n",
    "    VictDescent_Name AS `Victim Descent`,\n",
    "    Victims_Count AS `#`,\n",
    "    ROUND(Percentage, 1) AS `%`\n",
    "FROM\n",
    "    Victims_With_Rank\n",
    "WHERE\n",
    "    rank <= 3\n",
    "ORDER BY\n",
    "    year DESC, Victims_Count DESC\n",
    "\"\"\"\n",
    "\n",
    "result_df_sql = spark.sql(sql_query_final)\n",
    "\n",
    "result_df_sql.show(1000, truncate=False)\n",
    "end_time_sql = time.time()\n",
    "time_sql = end_time_sql - start_time_sql\n",
    "print(f\"Execution time for SQL API: {time_sql:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5ab856-af47-4ce4-a456-269f16c92ad0",
   "metadata": {},
   "source": [
    "Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb166369-6ebd-4f28-a6e3-0c5e36579a9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31687c039fa94061b6016056d465de02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Paths\n",
    "crime_paths = [\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv\",\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\"\n",
    "]\n",
    "mo_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/MO_codes.txt\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48e727f9-4238-43c5-afc2-6555e88e083f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f9a60539426451b9594c38f7fc4d2d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.85282301902771\n",
      "+------+-----------+--------------------------------------------------------------------------------+\n",
      "|MOCODE|total_count|Description                                                                     |\n",
      "+------+-----------+--------------------------------------------------------------------------------+\n",
      "|0344  |1002900    |Removes vict property                                                           |\n",
      "|1822  |548422     |Stranger                                                                        |\n",
      "|0416  |404773     |Hit-Hit w/ weapon                                                               |\n",
      "|0329  |377536     |Vandalized                                                                      |\n",
      "|0913  |278618     |Victim knew Suspect                                                             |\n",
      "|2000  |256188     |Domestic violence                                                               |\n",
      "|1300  |219082     |Vehicle involved                                                                |\n",
      "|0400  |213165     |Force used                                                                      |\n",
      "|1402  |177470     |Evidence Booked (any crime)                                                     |\n",
      "|1609  |131229     |Smashed                                                                         |\n",
      "|1309  |122108     |Susp uses vehicle                                                               |\n",
      "|1202  |120238     |Victim was aged (60 & over) or blind/physically disabled/unable to care for self|\n",
      "|0325  |120159     |Took merchandise                                                                |\n",
      "|1814  |118073     |Susp is/was current/former boyfriend/girlfriend                                 |\n",
      "|0444  |116763     |Pushed                                                                          |\n",
      "|1501  |115589     |Other MO (see rpt)                                                              |\n",
      "|1307  |113609     |Breaks window                                                                   |\n",
      "|0334  |105665     |Brandishes weapon                                                               |\n",
      "|2004  |93426      |Suspect is homeless/transient                                                   |\n",
      "|0432  |83562      |Intimidation                                                                    |\n",
      "+------+-----------+--------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [total_count#76L DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(total_count#76L DESC NULLS LAST, 1000), ENSURE_REQUIREMENTS, [plan_id=574]\n",
      "      +- Project [MOCODE#68, total_count#76L, Description#150]\n",
      "         +- BroadcastHashJoin [MOCODE#68], [MOCODE#83], Inner, BuildRight, false\n",
      "            :- HashAggregate(keys=[MOCODE#68], functions=[count(1)], schema specialized)\n",
      "            :  +- Exchange hashpartitioning(MOCODE#68, 1000), ENSURE_REQUIREMENTS, [plan_id=561]\n",
      "            :     +- HashAggregate(keys=[MOCODE#68], functions=[partial_count(1)], schema specialized)\n",
      "            :        +- Filter (NOT (trim(MOCODE#68, None) = ) AND NOT (MOCODE#68 = ))\n",
      "            :           +- Generate explode(split(regexp_replace(MOCODES#10, [,\\t;],  , 1),  , -1)), false, [MOCODE#68]\n",
      "            :              +- FileScan csv [Mocodes#10] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(2 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Mocodes:string>\n",
      "            +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=570]\n",
      "               +- SortAggregate(key=[MOCODE#83], functions=[first(Description#91, false)])\n",
      "                  +- Sort [MOCODE#83 ASC NULLS FIRST], false, 0\n",
      "                     +- Exchange hashpartitioning(MOCODE#83, 1000), ENSURE_REQUIREMENTS, [plan_id=566]\n",
      "                        +- SortAggregate(key=[MOCODE#83], functions=[partial_first(Description#91, false)])\n",
      "                           +- Sort [MOCODE#83 ASC NULLS FIRST], false, 0\n",
      "                              +- Project [MOCODE#83, CASE WHEN (Description#87 = ) THEN Unknown ELSE Description#87 END AS Description#91]\n",
      "                                 +- Project [split(value#81,  , -1)[0] AS MOCODE#83, trim(regexp_replace(value#81, ^\\S+\\s+, , 1), None) AS Description#87]\n",
      "                                    +- Filter (((isnotnull(value#81) AND isnotnull(split(value#81,  , -1)[0])) AND NOT (split(value#81,  , -1)[0] = )) AND NOT (trim(split(value#81,  , -1)[0], None) = ))\n",
      "                                       +- FileScan text [value#81] Batched: false, DataFilters: [isnotnull(value#81), isnotnull(split(value#81,  , -1)[0]), NOT (split(value#81,  , -1)[0] = ), N..., Format: Text, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/MO_c..., PartitionFilters: [], PushedFilters: [IsNotNull(value)], ReadSchema: struct<value:string>"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode,split,trim,broadcast\n",
    "\n",
    "\n",
    "start_time=time.time();\n",
    "\n",
    "crimes_schema = StructType([\n",
    "    StructField(\"DR_NO\", StringType()),\n",
    "    StructField(\"DateRptd\", DateType()),\n",
    "    StructField(\"DATEOCC\", StringType()),\n",
    "    StructField(\"TIMEOCC\", StringType()),\n",
    "    StructField(\"AREA\", StringType()),\n",
    "    StructField(\"AREANAME\", StringType()),\n",
    "    StructField(\"RptDistNo\", StringType()),\n",
    "    StructField(\"Part\", IntegerType()),\n",
    "    StructField(\"CrmCd\", StringType()),\n",
    "    StructField(\"Crm Cd Desc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"Vict Age\", StringType()),\n",
    "    StructField(\"VictSex\", StringType()),\n",
    "    StructField(\"VictDescent\", StringType()),\n",
    "    StructField(\"PremisCd\", StringType()),\n",
    "    StructField(\"PremisDesc\", StringType()),\n",
    "    StructField(\"WeaponUsedCd\", StringType()),\n",
    "    StructField(\"WeaponDesc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"Status Desc\", StringType()),\n",
    "    StructField(\"CrmCd1\", StringType()),\n",
    "    StructField(\"CrmCd2\", StringType()),\n",
    "    StructField(\"CrmCd3\", StringType()),\n",
    "    StructField(\"CrmCd4\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"CrossStreet\", StringType()),\n",
    "    StructField(\"LAT\", FloatType()),\n",
    "    StructField(\"LON\", FloatType()),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load and union CSVs (infer schema automatically)\n",
    "crime_df = spark.read.csv(\n",
    "    crime_paths,\n",
    "    header=True,\n",
    "    schema=crimes_schema,\n",
    "    timestampFormat='yyyy MMM dd hh:mm:ss a',\n",
    "    quote='\"',\n",
    "    escape='\"'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "mocodes_df = crime_df.select(\n",
    "    explode(\n",
    "        split(\n",
    "            regexp_replace(col(\"MOCODES\"), r\"[,\\t;]\", \" \"),\n",
    "            \" \"\n",
    "        )\n",
    "    ).alias(\"MOCODE\")\n",
    ").filter(\n",
    "    col(\"MOCODE\").isNotNull() & (trim(col(\"MOCODE\")) != \"\")\n",
    ")\n",
    "\n",
    "# Count occurrences\n",
    "mocode_counts_df = mocodes_df.groupBy(\"MOCODE\").count().withColumnRenamed(\"count\", \"total_count\")\n",
    "\n",
    "# Load descriptions\n",
    "mocode_raw = spark.read.text(mo_path)\n",
    "\n",
    "\n",
    "# Load raw MOCODE descriptions\n",
    "mocode_raw = spark.read.text(mo_path)\n",
    "\n",
    "mo_codes = (\n",
    "    mocode_raw\n",
    "    .withColumn(\"MOCODE\", split(col(\"value\"), \" \")[0])\n",
    "    # Extract Description as before\n",
    "    .withColumn(\"Description\", trim(regexp_replace(col(\"value\"), r\"^\\S+\\s+\", \"\")))\n",
    "    \n",
    "    .filter(col(\"MOCODE\").isNotNull()) \n",
    "    .filter(col(\"MOCODE\") != \"\") \n",
    "    \n",
    "    # Handle the Description edge case (if you still want \"Unknown\" for a missing description)\n",
    "    .withColumn(\"Description\", when(col(\"Description\") == \"\", lit(\"Unknown\")).otherwise(col(\"Description\")))\n",
    "    \n",
    "    # Select and Deduplicate as before\n",
    "    .select(\"MOCODE\", \"Description\")\n",
    "    .dropDuplicates([\"MOCODE\"])\n",
    ")\n",
    "\n",
    "\n",
    "# Join counts with descriptions\n",
    "joined_df= mocode_counts_df.join(mo_codes,\n",
    "    on=\"MOCODE\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "mocode_with_desc_df = joined_df.orderBy(col(\"total_count\").desc())\n",
    "mocode_with_desc_df.count()\n",
    "\n",
    "# --- Show top 20 MOCODES with descriptions ---\n",
    "end_time=time.time();\n",
    "time_elapsed=end_time-start_time;\n",
    "print(time_elapsed);\n",
    "mocode_with_desc_df.show(20,truncate=False)\n",
    "mocode_with_desc_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3cbeeb5-57ca-453c-a70b-b6423a7e4ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee3ca5d14a840bf826ba620965063fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BROADCAST join time: 12.5029 sec\n",
      "BROADCAST Join Top 20:\n",
      "+------+-----------+--------------------------------------------------------------------------------+\n",
      "|MOCODE|total_count|Description                                                                     |\n",
      "+------+-----------+--------------------------------------------------------------------------------+\n",
      "|0344  |1002900    |Removes vict property                                                           |\n",
      "|1822  |548422     |Stranger                                                                        |\n",
      "|0416  |404773     |Hit-Hit w/ weapon                                                               |\n",
      "|0329  |377536     |Vandalized                                                                      |\n",
      "|0913  |278618     |Victim knew Suspect                                                             |\n",
      "|2000  |256188     |Domestic violence                                                               |\n",
      "|1300  |219082     |Vehicle involved                                                                |\n",
      "|0400  |213165     |Force used                                                                      |\n",
      "|1402  |177470     |Evidence Booked (any crime)                                                     |\n",
      "|1609  |131229     |Smashed                                                                         |\n",
      "|1309  |122108     |Susp uses vehicle                                                               |\n",
      "|1202  |120238     |Victim was aged (60 & over) or blind/physically disabled/unable to care for self|\n",
      "|0325  |120159     |Took merchandise                                                                |\n",
      "|1814  |118073     |Susp is/was current/former boyfriend/girlfriend                                 |\n",
      "|0444  |116763     |Pushed                                                                          |\n",
      "|1501  |115589     |Other MO (see rpt)                                                              |\n",
      "|1307  |113609     |Breaks window                                                                   |\n",
      "|0334  |105665     |Brandishes weapon                                                               |\n",
      "|2004  |93426      |Suspect is homeless/transient                                                   |\n",
      "|0432  |83562      |Intimidation                                                                    |\n",
      "+------+-----------+--------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [total_count#89L DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(total_count#89L DESC NULLS LAST, 1000), ENSURE_REQUIREMENTS, [plan_id=602]\n",
      "      +- Project [MOCODE#81, total_count#89L, Description#161]\n",
      "         +- BroadcastHashJoin [MOCODE#81], [MOCODE#94], Inner, BuildRight, false\n",
      "            :- HashAggregate(keys=[MOCODE#81], functions=[count(1)], schema specialized)\n",
      "            :  +- Exchange hashpartitioning(MOCODE#81, 1000), ENSURE_REQUIREMENTS, [plan_id=589]\n",
      "            :     +- HashAggregate(keys=[MOCODE#81], functions=[partial_count(1)], schema specialized)\n",
      "            :        +- Filter (NOT (trim(MOCODE#81, None) = ) AND NOT (MOCODE#81 = ))\n",
      "            :           +- Generate explode(split(regexp_replace(MOCODES#34, [,\\t;],  , 1),  , -1)), false, [MOCODE#81]\n",
      "            :              +- FileScan csv [Mocodes#34] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(2 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Mocodes:string>\n",
      "            +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=598]\n",
      "               +- SortAggregate(key=[MOCODE#94], functions=[first(Description#102, false)])\n",
      "                  +- Sort [MOCODE#94 ASC NULLS FIRST], false, 0\n",
      "                     +- Exchange hashpartitioning(MOCODE#94, 1000), ENSURE_REQUIREMENTS, [plan_id=594]\n",
      "                        +- SortAggregate(key=[MOCODE#94], functions=[partial_first(Description#102, false)])\n",
      "                           +- Sort [MOCODE#94 ASC NULLS FIRST], false, 0\n",
      "                              +- Project [MOCODE#94, CASE WHEN (Description#98 = ) THEN Unknown ELSE Description#98 END AS Description#102]\n",
      "                                 +- Project [split(value#92,  , -1)[0] AS MOCODE#94, trim(regexp_replace(value#92, ^\\S+\\s+, , 1), None) AS Description#98]\n",
      "                                    +- Filter (((isnotnull(value#92) AND isnotnull(split(value#92,  , -1)[0])) AND NOT (split(value#92,  , -1)[0] = )) AND NOT (trim(split(value#92,  , -1)[0], None) = ))\n",
      "                                       +- FileScan text [value#92] Batched: false, DataFilters: [isnotnull(value#92), isnotnull(split(value#92,  , -1)[0]), NOT (split(value#92,  , -1)[0] = ), N..., Format: Text, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/MO_c..., PartitionFilters: [], PushedFilters: [IsNotNull(value)], ReadSchema: struct<value:string>"
     ]
    }
   ],
   "source": [
    "# --- Setup & Read Data ---\n",
    "\n",
    "start_time=time.time();\n",
    "\n",
    "crimes_schema = StructType([\n",
    "    StructField(\"DR_NO\", StringType()),\n",
    "    StructField(\"DateRptd\", DateType()),\n",
    "    StructField(\"DATEOCC\", StringType()),\n",
    "    StructField(\"TIMEOCC\", StringType()),\n",
    "    StructField(\"AREA\", StringType()),\n",
    "    StructField(\"AREANAME\", StringType()),\n",
    "    StructField(\"RptDistNo\", StringType()),\n",
    "    StructField(\"Part\", IntegerType()),\n",
    "    StructField(\"CrmCd\", StringType()),\n",
    "    StructField(\"Crm Cd Desc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"Vict Age\", StringType()),\n",
    "    StructField(\"VictSex\", StringType()),\n",
    "    StructField(\"VictDescent\", StringType()),\n",
    "    StructField(\"PremisCd\", StringType()),\n",
    "    StructField(\"PremisDesc\", StringType()),\n",
    "    StructField(\"WeaponUsedCd\", StringType()),\n",
    "    StructField(\"WeaponDesc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"Status Desc\", StringType()),\n",
    "    StructField(\"CrmCd1\", StringType()),\n",
    "    StructField(\"CrmCd2\", StringType()),\n",
    "    StructField(\"CrmCd3\", StringType()),\n",
    "    StructField(\"CrmCd4\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"CrossStreet\", StringType()),\n",
    "    StructField(\"LAT\", FloatType()),\n",
    "    StructField(\"LON\", FloatType()),\n",
    "])\n",
    "\n",
    "crime_df = spark.read.csv(\n",
    "    crime_paths,\n",
    "    header=True,\n",
    "    schema=crimes_schema,\n",
    "    timestampFormat='yyyy MMM dd hh:mm:ss a',\n",
    "    quote='\"',\n",
    "    escape='\"'\n",
    ")\n",
    "\n",
    "mocodes_df = crime_df.select(\n",
    "    explode(\n",
    "        split(\n",
    "            regexp_replace(col(\"MOCODES\"), r\"[,\\t;]\", \" \"),\n",
    "            \" \"\n",
    "        )\n",
    "    ).alias(\"MOCODE\")\n",
    ").filter(\n",
    "    col(\"MOCODE\").isNotNull() & (trim(col(\"MOCODE\")) != \"\")\n",
    ")\n",
    "\n",
    "mocode_counts_df = mocodes_df.groupBy(\"MOCODE\").count().withColumnRenamed(\"count\", \"total_count\")\n",
    "\n",
    "mocode_raw = spark.read.text(mo_path)\n",
    "\n",
    "mo_codes = (\n",
    "    mocode_raw\n",
    "    .withColumn(\"MOCODE\", split(col(\"value\"), \" \")[0])\n",
    "    # Extract Description as before\n",
    "    .withColumn(\"Description\", trim(regexp_replace(col(\"value\"), r\"^\\S+\\s+\", \"\")))\n",
    "    \n",
    "    .filter(col(\"MOCODE\").isNotNull()) \n",
    "    .filter(col(\"MOCODE\") != \"\") \n",
    "    \n",
    "    # Handle the Description edge case (if you still want \"Unknown\" for a missing description)\n",
    "    .withColumn(\"Description\", when(col(\"Description\") == \"\", lit(\"Unknown\")).otherwise(col(\"Description\")))\n",
    "    \n",
    "    # Select and Deduplicate as before\n",
    "    .select(\"MOCODE\", \"Description\")\n",
    "    .dropDuplicates([\"MOCODE\"])\n",
    ")\n",
    "# --- Join with Broadcast ---\n",
    "joined_df = mocode_counts_df.join(broadcast(mo_codes), on=\"MOCODE\", how=\"inner\")\n",
    "mocode_with_desc_df = joined_df.orderBy(col(\"total_count\").desc())\n",
    "\n",
    "mocode_with_desc_df.count()\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"BROADCAST join time: {elapsed:.4f} sec\")\n",
    "print(\"BROADCAST Join Top 20:\")\n",
    "mocode_with_desc_df.show(20, truncate=False)\n",
    "mocode_with_desc_df.explain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b325f4b7-33bb-4d25-803b-1c119a94b2c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "958c7236e4ec4e519796b71d44c235f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MERGE join time: 18.3418 sec\n",
      "MERGE Join Top 20:\n",
      "+------+-----------+--------------------------------------------------------------------------------+\n",
      "|MOCODE|total_count|Description                                                                     |\n",
      "+------+-----------+--------------------------------------------------------------------------------+\n",
      "|0344  |1002900    |Removes vict property                                                           |\n",
      "|1822  |548422     |Stranger                                                                        |\n",
      "|0416  |404773     |Hit-Hit w/ weapon                                                               |\n",
      "|0329  |377536     |Vandalized                                                                      |\n",
      "|0913  |278618     |Victim knew Suspect                                                             |\n",
      "|2000  |256188     |Domestic violence                                                               |\n",
      "|1300  |219082     |Vehicle involved                                                                |\n",
      "|0400  |213165     |Force used                                                                      |\n",
      "|1402  |177470     |Evidence Booked (any crime)                                                     |\n",
      "|1609  |131229     |Smashed                                                                         |\n",
      "|1309  |122108     |Susp uses vehicle                                                               |\n",
      "|1202  |120238     |Victim was aged (60 & over) or blind/physically disabled/unable to care for self|\n",
      "|0325  |120159     |Took merchandise                                                                |\n",
      "|1814  |118073     |Susp is/was current/former boyfriend/girlfriend                                 |\n",
      "|0444  |116763     |Pushed                                                                          |\n",
      "|1501  |115589     |Other MO (see rpt)                                                              |\n",
      "|1307  |113609     |Breaks window                                                                   |\n",
      "|0334  |105665     |Brandishes weapon                                                               |\n",
      "|2004  |93426      |Suspect is homeless/transient                                                   |\n",
      "|0432  |83562      |Intimidation                                                                    |\n",
      "+------+-----------+--------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [total_count#89L DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(total_count#89L DESC NULLS LAST, 1000), ENSURE_REQUIREMENTS, [plan_id=605]\n",
      "      +- Project [MOCODE#81, total_count#89L, Description#161]\n",
      "         +- SortMergeJoin [MOCODE#81], [MOCODE#94], Inner\n",
      "            :- Sort [MOCODE#81 ASC NULLS FIRST], false, 0\n",
      "            :  +- HashAggregate(keys=[MOCODE#81], functions=[count(1)], schema specialized)\n",
      "            :     +- Exchange hashpartitioning(MOCODE#81, 1000), ENSURE_REQUIREMENTS, [plan_id=591]\n",
      "            :        +- HashAggregate(keys=[MOCODE#81], functions=[partial_count(1)], schema specialized)\n",
      "            :           +- Filter (NOT (trim(MOCODE#81, None) = ) AND NOT (MOCODE#81 = ))\n",
      "            :              +- Generate explode(split(regexp_replace(MOCODES#34, [,\\t;],  , 1),  , -1)), false, [MOCODE#81]\n",
      "            :                 +- FileScan csv [Mocodes#34] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(2 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Mocodes:string>\n",
      "            +- SortAggregate(key=[MOCODE#94], functions=[first(Description#102, false)])\n",
      "               +- Sort [MOCODE#94 ASC NULLS FIRST], false, 0\n",
      "                  +- Exchange hashpartitioning(MOCODE#94, 1000), ENSURE_REQUIREMENTS, [plan_id=596]\n",
      "                     +- SortAggregate(key=[MOCODE#94], functions=[partial_first(Description#102, false)])\n",
      "                        +- Sort [MOCODE#94 ASC NULLS FIRST], false, 0\n",
      "                           +- Project [MOCODE#94, CASE WHEN (Description#98 = ) THEN Unknown ELSE Description#98 END AS Description#102]\n",
      "                              +- Project [split(value#92,  , -1)[0] AS MOCODE#94, trim(regexp_replace(value#92, ^\\S+\\s+, , 1), None) AS Description#98]\n",
      "                                 +- Filter (((isnotnull(value#92) AND isnotnull(split(value#92,  , -1)[0])) AND NOT (split(value#92,  , -1)[0] = )) AND NOT (trim(split(value#92,  , -1)[0], None) = ))\n",
      "                                    +- FileScan text [value#92] Batched: false, DataFilters: [isnotnull(value#92), isnotnull(split(value#92,  , -1)[0]), NOT (split(value#92,  , -1)[0] = ), N..., Format: Text, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/MO_c..., PartitionFilters: [], PushedFilters: [IsNotNull(value)], ReadSchema: struct<value:string>"
     ]
    }
   ],
   "source": [
    "# --- Setup & Read Data ---\n",
    "\n",
    "start_time=time.time();\n",
    "\n",
    "\n",
    "crimes_schema = StructType([\n",
    "    StructField(\"DR_NO\", StringType()),\n",
    "    StructField(\"DateRptd\", DateType()),\n",
    "    StructField(\"DATEOCC\", StringType()),\n",
    "    StructField(\"TIMEOCC\", StringType()),\n",
    "    StructField(\"AREA\", StringType()),\n",
    "    StructField(\"AREANAME\", StringType()),\n",
    "    StructField(\"RptDistNo\", StringType()),\n",
    "    StructField(\"Part\", IntegerType()),\n",
    "    StructField(\"CrmCd\", StringType()),\n",
    "    StructField(\"Crm Cd Desc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"Vict Age\", StringType()),\n",
    "    StructField(\"VictSex\", StringType()),\n",
    "    StructField(\"VictDescent\", StringType()),\n",
    "    StructField(\"PremisCd\", StringType()),\n",
    "    StructField(\"PremisDesc\", StringType()),\n",
    "    StructField(\"WeaponUsedCd\", StringType()),\n",
    "    StructField(\"WeaponDesc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"Status Desc\", StringType()),\n",
    "    StructField(\"CrmCd1\", StringType()),\n",
    "    StructField(\"CrmCd2\", StringType()),\n",
    "    StructField(\"CrmCd3\", StringType()),\n",
    "    StructField(\"CrmCd4\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"CrossStreet\", StringType()),\n",
    "    StructField(\"LAT\", FloatType()),\n",
    "    StructField(\"LON\", FloatType()),\n",
    "])\n",
    "\n",
    "crime_df = spark.read.csv(\n",
    "    crime_paths,\n",
    "    header=True,\n",
    "    schema=crimes_schema,\n",
    "    timestampFormat='yyyy MMM dd hh:mm:ss a',\n",
    "    quote='\"',\n",
    "    escape='\"'\n",
    ")\n",
    "\n",
    "mocodes_df = crime_df.select(\n",
    "    explode(\n",
    "        split(\n",
    "            regexp_replace(col(\"MOCODES\"), r\"[,\\t;]\", \" \"),\n",
    "            \" \"\n",
    "        )\n",
    "    ).alias(\"MOCODE\")\n",
    ").filter(\n",
    "    col(\"MOCODE\").isNotNull() & (trim(col(\"MOCODE\")) != \"\")\n",
    ")\n",
    "\n",
    "mocode_counts_df = mocodes_df.groupBy(\"MOCODE\").count().withColumnRenamed(\"count\", \"total_count\")\n",
    "\n",
    "mocode_raw = spark.read.text(mo_path)\n",
    "\n",
    "mo_codes = (\n",
    "    mocode_raw\n",
    "    .withColumn(\"MOCODE\", split(col(\"value\"), \" \")[0])\n",
    "    # Extract Description as before\n",
    "    .withColumn(\"Description\", trim(regexp_replace(col(\"value\"), r\"^\\S+\\s+\", \"\")))\n",
    "    \n",
    "    # Filter out rows where MOCODE is null or an empty string\n",
    "    .filter(col(\"MOCODE\").isNotNull()) \n",
    "    .filter(col(\"MOCODE\") != \"\") \n",
    "    \n",
    "    # Handle the Description edge case (if you still want \"Unknown\" for a missing description)\n",
    "    .withColumn(\"Description\", when(col(\"Description\") == \"\", lit(\"Unknown\")).otherwise(col(\"Description\")))\n",
    "    \n",
    "    # Select and Deduplicate as before\n",
    "    .select(\"MOCODE\", \"Description\")\n",
    "    .dropDuplicates([\"MOCODE\"])\n",
    ")\n",
    "\n",
    "# --- Join with Merge ---\n",
    "joined_df = mocode_counts_df.join(mo_codes.hint(\"merge\"), on=\"MOCODE\", how=\"inner\")\n",
    "mocode_with_desc_df = joined_df.orderBy(col(\"total_count\").desc())\n",
    "mocode_with_desc_df.count()\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"MERGE join time: {elapsed:.4f} sec\")\n",
    "print(\"MERGE Join Top 20:\")\n",
    "mocode_with_desc_df.show(20, truncate=False)\n",
    "mocode_with_desc_df.explain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e425190-78b7-4abb-b18f-25b8609a5405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5976ba083b7d4bf6bd2885387107db87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHUFFLE_HASH Join Top 20:\n",
      "+------+-----------+--------------------------------------------------------------------------------+\n",
      "|MOCODE|total_count|Description                                                                     |\n",
      "+------+-----------+--------------------------------------------------------------------------------+\n",
      "|0344  |1002900    |Removes vict property                                                           |\n",
      "|1822  |548422     |Stranger                                                                        |\n",
      "|0416  |404773     |Hit-Hit w/ weapon                                                               |\n",
      "|0329  |377536     |Vandalized                                                                      |\n",
      "|0913  |278618     |Victim knew Suspect                                                             |\n",
      "|2000  |256188     |Domestic violence                                                               |\n",
      "|1300  |219082     |Vehicle involved                                                                |\n",
      "|0400  |213165     |Force used                                                                      |\n",
      "|1402  |177470     |Evidence Booked (any crime)                                                     |\n",
      "|1609  |131229     |Smashed                                                                         |\n",
      "|1309  |122108     |Susp uses vehicle                                                               |\n",
      "|1202  |120238     |Victim was aged (60 & over) or blind/physically disabled/unable to care for self|\n",
      "|0325  |120159     |Took merchandise                                                                |\n",
      "|1814  |118073     |Susp is/was current/former boyfriend/girlfriend                                 |\n",
      "|0444  |116763     |Pushed                                                                          |\n",
      "|1501  |115589     |Other MO (see rpt)                                                              |\n",
      "|1307  |113609     |Breaks window                                                                   |\n",
      "|0334  |105665     |Brandishes weapon                                                               |\n",
      "|2004  |93426      |Suspect is homeless/transient                                                   |\n",
      "|0432  |83562      |Intimidation                                                                    |\n",
      "+------+-----------+--------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "SHUFFLE_HASH join time: 15.5652 sec\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [total_count#76L DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(total_count#76L DESC NULLS LAST, 1000), ENSURE_REQUIREMENTS, [plan_id=522]\n",
      "      +- Project [MOCODE#68, total_count#76L, Description#148]\n",
      "         +- ShuffledHashJoin [MOCODE#68], [MOCODE#81], Inner, BuildRight\n",
      "            :- HashAggregate(keys=[MOCODE#68], functions=[count(1)], schema specialized)\n",
      "            :  +- Exchange hashpartitioning(MOCODE#68, 1000), ENSURE_REQUIREMENTS, [plan_id=510]\n",
      "            :     +- HashAggregate(keys=[MOCODE#68], functions=[partial_count(1)], schema specialized)\n",
      "            :        +- Filter (NOT (trim(MOCODE#68, None) = ) AND NOT (MOCODE#68 = ))\n",
      "            :           +- Generate explode(split(regexp_replace(MOCODES#10, [,\\t;],  , 1),  , -1)), false, [MOCODE#68]\n",
      "            :              +- FileScan csv [Mocodes#10] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(2 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Mocodes:string>\n",
      "            +- SortAggregate(key=[MOCODE#81], functions=[first(Description#89, false)])\n",
      "               +- Sort [MOCODE#81 ASC NULLS FIRST], false, 0\n",
      "                  +- Exchange hashpartitioning(MOCODE#81, 1000), ENSURE_REQUIREMENTS, [plan_id=515]\n",
      "                     +- SortAggregate(key=[MOCODE#81], functions=[partial_first(Description#89, false)])\n",
      "                        +- Sort [MOCODE#81 ASC NULLS FIRST], false, 0\n",
      "                           +- Project [MOCODE#81, CASE WHEN (Description#85 = ) THEN Unknown ELSE Description#85 END AS Description#89]\n",
      "                              +- Project [split(value#79,  , -1)[0] AS MOCODE#81, trim(regexp_replace(value#79, ^\\S+\\s+, , 1), None) AS Description#85]\n",
      "                                 +- Filter (((isnotnull(value#79) AND isnotnull(split(value#79,  , -1)[0])) AND NOT (split(value#79,  , -1)[0] = )) AND NOT (trim(split(value#79,  , -1)[0], None) = ))\n",
      "                                    +- FileScan text [value#79] Batched: false, DataFilters: [isnotnull(value#79), isnotnull(split(value#79,  , -1)[0]), NOT (split(value#79,  , -1)[0] = ), N..., Format: Text, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/MO_c..., PartitionFilters: [], PushedFilters: [IsNotNull(value)], ReadSchema: struct<value:string>"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Setup & Read Data ---\n",
    "\n",
    "start_time=time.time();\n",
    "\n",
    "\n",
    "crimes_schema = StructType([\n",
    "    StructField(\"DR_NO\", StringType()),\n",
    "    StructField(\"DateRptd\", DateType()),\n",
    "    StructField(\"DATEOCC\", StringType()),\n",
    "    StructField(\"TIMEOCC\", StringType()),\n",
    "    StructField(\"AREA\", StringType()),\n",
    "    StructField(\"AREANAME\", StringType()),\n",
    "    StructField(\"RptDistNo\", StringType()),\n",
    "    StructField(\"Part\", IntegerType()),\n",
    "    StructField(\"CrmCd\", StringType()),\n",
    "    StructField(\"Crm Cd Desc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"Vict Age\", StringType()),\n",
    "    StructField(\"VictSex\", StringType()),\n",
    "    StructField(\"VictDescent\", StringType()),\n",
    "    StructField(\"PremisCd\", StringType()),\n",
    "    StructField(\"PremisDesc\", StringType()),\n",
    "    StructField(\"WeaponUsedCd\", StringType()),\n",
    "    StructField(\"WeaponDesc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"Status Desc\", StringType()),\n",
    "    StructField(\"CrmCd1\", StringType()),\n",
    "    StructField(\"CrmCd2\", StringType()),\n",
    "    StructField(\"CrmCd3\", StringType()),\n",
    "    StructField(\"CrmCd4\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"CrossStreet\", StringType()),\n",
    "    StructField(\"LAT\", FloatType()),\n",
    "    StructField(\"LON\", FloatType()),\n",
    "])\n",
    "\n",
    "\n",
    "crime_df = spark.read.csv(\n",
    "    crime_paths,\n",
    "    header=True,\n",
    "    schema=crimes_schema,\n",
    "    timestampFormat='yyyy MMM dd hh:mm:ss a',\n",
    "    quote='\"',\n",
    "    escape='\"'\n",
    ")\n",
    "\n",
    "mocodes_df = crime_df.select(\n",
    "    explode(\n",
    "        split(\n",
    "            regexp_replace(col(\"MOCODES\"), r\"[,\\t;]\", \" \"),\n",
    "            \" \"\n",
    "        )\n",
    "    ).alias(\"MOCODE\")\n",
    ").filter(\n",
    "    col(\"MOCODE\").isNotNull() & (trim(col(\"MOCODE\")) != \"\")\n",
    ")\n",
    "\n",
    "mocode_counts_df = mocodes_df.groupBy(\"MOCODE\").count().withColumnRenamed(\"count\", \"total_count\")\n",
    "\n",
    "mocode_raw = spark.read.text(mo_path)\n",
    "\n",
    "mo_codes = (\n",
    "    mocode_raw\n",
    "    .withColumn(\"MOCODE\", split(col(\"value\"), \" \")[0])\n",
    "    # Extract Description as before\n",
    "    .withColumn(\"Description\", trim(regexp_replace(col(\"value\"), r\"^\\S+\\s+\", \"\")))\n",
    "    \n",
    "    .filter(col(\"MOCODE\").isNotNull()) \n",
    "    .filter(col(\"MOCODE\") != \"\") \n",
    "    \n",
    "    # Handle the Description edge case (if you still want \"Unknown\" for a missing description)\n",
    "    .withColumn(\"Description\", when(col(\"Description\") == \"\", lit(\"Unknown\")).otherwise(col(\"Description\")))\n",
    "    \n",
    "    # Select and Deduplicate as before\n",
    "    .select(\"MOCODE\", \"Description\")\n",
    "    .dropDuplicates([\"MOCODE\"])\n",
    ")\n",
    "# --- Join with Shuffle Hash ---\n",
    "joined_df = mocode_counts_df.join(mo_codes.hint(\"shuffle_hash\"), on=\"MOCODE\", how=\"inner\")\n",
    "mocode_with_desc_df = joined_df.orderBy(col(\"total_count\").desc())\n",
    "mocode_with_desc_df.count()\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(\"SHUFFLE_HASH Join Top 20:\")\n",
    "mocode_with_desc_df.show(20, truncate=False)\n",
    "print(f\"SHUFFLE_HASH join time: {elapsed:.4f} sec\")\n",
    "mocode_with_desc_df.explain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "955262ed-0509-4e65-ae96-fe67abc65c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c98de3b6a9d7472281e4b31808a64f3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHUFFLE_REPLICATE_NL Join Top 20:\n",
      "+------+-----------+--------------------------------------------------------------------------------+\n",
      "|MOCODE|total_count|Description                                                                     |\n",
      "+------+-----------+--------------------------------------------------------------------------------+\n",
      "|0344  |1002900    |Removes vict property                                                           |\n",
      "|1822  |548422     |Stranger                                                                        |\n",
      "|0416  |404773     |Hit-Hit w/ weapon                                                               |\n",
      "|0329  |377536     |Vandalized                                                                      |\n",
      "|0913  |278618     |Victim knew Suspect                                                             |\n",
      "|2000  |256188     |Domestic violence                                                               |\n",
      "|1300  |219082     |Vehicle involved                                                                |\n",
      "|0400  |213165     |Force used                                                                      |\n",
      "|1402  |177470     |Evidence Booked (any crime)                                                     |\n",
      "|1609  |131229     |Smashed                                                                         |\n",
      "|1309  |122108     |Susp uses vehicle                                                               |\n",
      "|1202  |120238     |Victim was aged (60 & over) or blind/physically disabled/unable to care for self|\n",
      "|0325  |120159     |Took merchandise                                                                |\n",
      "|1814  |118073     |Susp is/was current/former boyfriend/girlfriend                                 |\n",
      "|0444  |116763     |Pushed                                                                          |\n",
      "|1501  |115589     |Other MO (see rpt)                                                              |\n",
      "|1307  |113609     |Breaks window                                                                   |\n",
      "|0334  |105665     |Brandishes weapon                                                               |\n",
      "|2004  |93426      |Suspect is homeless/transient                                                   |\n",
      "|0432  |83562      |Intimidation                                                                    |\n",
      "+------+-----------+--------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "SHUFFLE_REPLICATE_NL join time: 19.0149 sec\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [total_count#76L DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(total_count#76L DESC NULLS LAST, 1000), ENSURE_REQUIREMENTS, [plan_id=500]\n",
      "      +- Project [MOCODE#68, total_count#76L, Description#148]\n",
      "         +- CartesianProduct (MOCODE#68 = MOCODE#81)\n",
      "            :- HashAggregate(keys=[MOCODE#68], functions=[count(1)], schema specialized)\n",
      "            :  +- Exchange hashpartitioning(MOCODE#68, 1000), ENSURE_REQUIREMENTS, [plan_id=489]\n",
      "            :     +- HashAggregate(keys=[MOCODE#68], functions=[partial_count(1)], schema specialized)\n",
      "            :        +- Filter (NOT (trim(MOCODE#68, None) = ) AND NOT (MOCODE#68 = ))\n",
      "            :           +- Generate explode(split(regexp_replace(MOCODES#10, [,\\t;],  , 1),  , -1)), false, [MOCODE#68]\n",
      "            :              +- FileScan csv [Mocodes#10] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(2 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Mocodes:string>\n",
      "            +- SortAggregate(key=[MOCODE#81], functions=[first(Description#89, false)])\n",
      "               +- Sort [MOCODE#81 ASC NULLS FIRST], false, 0\n",
      "                  +- Exchange hashpartitioning(MOCODE#81, 1000), ENSURE_REQUIREMENTS, [plan_id=494]\n",
      "                     +- SortAggregate(key=[MOCODE#81], functions=[partial_first(Description#89, false)])\n",
      "                        +- Sort [MOCODE#81 ASC NULLS FIRST], false, 0\n",
      "                           +- Project [MOCODE#81, CASE WHEN (Description#85 = ) THEN Unknown ELSE Description#85 END AS Description#89]\n",
      "                              +- Project [split(value#79,  , -1)[0] AS MOCODE#81, trim(regexp_replace(value#79, ^\\S+\\s+, , 1), None) AS Description#85]\n",
      "                                 +- Filter (((isnotnull(value#79) AND isnotnull(split(value#79,  , -1)[0])) AND NOT (split(value#79,  , -1)[0] = )) AND NOT (trim(split(value#79,  , -1)[0], None) = ))\n",
      "                                    +- FileScan text [value#79] Batched: false, DataFilters: [isnotnull(value#79), isnotnull(split(value#79,  , -1)[0]), NOT (split(value#79,  , -1)[0] = ), N..., Format: Text, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/MO_c..., PartitionFilters: [], PushedFilters: [IsNotNull(value)], ReadSchema: struct<value:string>"
     ]
    }
   ],
   "source": [
    "# --- Setup & Read Data ---\n",
    "\n",
    "start_time=time.time();\n",
    "\n",
    "crimes_schema = StructType([\n",
    "    StructField(\"DR_NO\", StringType()),\n",
    "    StructField(\"DateRptd\", DateType()),\n",
    "    StructField(\"DATEOCC\", StringType()),\n",
    "    StructField(\"TIMEOCC\", StringType()),\n",
    "    StructField(\"AREA\", StringType()),\n",
    "    StructField(\"AREANAME\", StringType()),\n",
    "    StructField(\"RptDistNo\", StringType()),\n",
    "    StructField(\"Part\", IntegerType()),\n",
    "    StructField(\"CrmCd\", StringType()),\n",
    "    StructField(\"Crm Cd Desc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"Vict Age\", StringType()),\n",
    "    StructField(\"VictSex\", StringType()),\n",
    "    StructField(\"VictDescent\", StringType()),\n",
    "    StructField(\"PremisCd\", StringType()),\n",
    "    StructField(\"PremisDesc\", StringType()),\n",
    "    StructField(\"WeaponUsedCd\", StringType()),\n",
    "    StructField(\"WeaponDesc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"Status Desc\", StringType()),\n",
    "    StructField(\"CrmCd1\", StringType()),\n",
    "    StructField(\"CrmCd2\", StringType()),\n",
    "    StructField(\"CrmCd3\", StringType()),\n",
    "    StructField(\"CrmCd4\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"CrossStreet\", StringType()),\n",
    "    StructField(\"LAT\", FloatType()),\n",
    "    StructField(\"LON\", FloatType()),\n",
    "])\n",
    "\n",
    "crime_df = spark.read.csv(\n",
    "    crime_paths,\n",
    "    header=True,\n",
    "    schema=crimes_schema,\n",
    "    timestampFormat='yyyy MMM dd hh:mm:ss a',\n",
    "    quote='\"',\n",
    "    escape='\"'\n",
    ")\n",
    "\n",
    "mocodes_df = crime_df.select(\n",
    "    explode(\n",
    "        split(\n",
    "            regexp_replace(col(\"MOCODES\"), r\"[,\\t;]\", \" \"),\n",
    "            \" \"\n",
    "        )\n",
    "    ).alias(\"MOCODE\")\n",
    ").filter(\n",
    "    col(\"MOCODE\").isNotNull() & (trim(col(\"MOCODE\")) != \"\")\n",
    ")\n",
    "\n",
    "mocode_counts_df = mocodes_df.groupBy(\"MOCODE\").count().withColumnRenamed(\"count\", \"total_count\")\n",
    "\n",
    "mocode_raw = spark.read.text(mo_path)\n",
    "\n",
    "mo_codes = (\n",
    "    mocode_raw\n",
    "    .withColumn(\"MOCODE\", split(col(\"value\"), \" \")[0])\n",
    "    # Extract Description as before\n",
    "    .withColumn(\"Description\", trim(regexp_replace(col(\"value\"), r\"^\\S+\\s+\", \"\")))\n",
    "    \n",
    "    .filter(col(\"MOCODE\").isNotNull()) \n",
    "    .filter(col(\"MOCODE\") != \"\") \n",
    "    \n",
    "    # Handle the Description edge case (if you still want \"Unknown\" for a missing description)\n",
    "    .withColumn(\"Description\", when(col(\"Description\") == \"\", lit(\"Unknown\")).otherwise(col(\"Description\")))\n",
    "    \n",
    "    # Select and Deduplicate as before\n",
    "    .select(\"MOCODE\", \"Description\")\n",
    "    .dropDuplicates([\"MOCODE\"])\n",
    ")\n",
    "\n",
    "# --- Join with Shuffle Replicate NL ---\n",
    "joined_df = mocode_counts_df.join(mo_codes.hint(\"shuffle_replicate_nl\"), on=\"MOCODE\", how=\"inner\")\n",
    "mocode_with_desc_df = joined_df.orderBy(col(\"total_count\").desc())\n",
    "mocode_with_desc_df.count()\n",
    "elapsed = time.time() - start_time\n",
    "print(\"SHUFFLE_REPLICATE_NL Join Top 20:\")\n",
    "mocode_with_desc_df.show(20, truncate=False)\n",
    "print(f\"SHUFFLE_REPLICATE_NL join time: {elapsed:.4f} sec\")\n",
    "mocode_with_desc_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d045db5-3b65-4e6d-a08b-08bc8fc91997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ac7bcb7d7e1456583f9bd152b3b9a23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0344 - Removes vict property: 1002900\n",
      "1822 - Stranger: 548422\n",
      "0416 - Hit-Hit w/ weapon: 404773\n",
      "0329 - Vandalized: 377536\n",
      "0913 - Victim knew Suspect: 278618\n",
      "2000 - Domestic violence: 256188\n",
      "1300 - Vehicle involved: 219082\n",
      "0400 - Force used: 213165\n",
      "1402 - Evidence Booked (any crime): 177470\n",
      "1609 - Smashed: 131229\n",
      "1309 - Susp uses vehicle: 122108\n",
      "1202 - Victim was aged (60 & over) or blind/physically disabled/unable to care for self: 120238\n",
      "0325 - Took merchandise: 120159\n",
      "1814 - Susp is/was current/former boyfriend/girlfriend: 118073\n",
      "0444 - Pushed: 116763\n",
      "1501 - Other MO (see rpt): 115589\n",
      "1307 - Breaks window: 113609\n",
      "0334 - Brandishes weapon: 105665\n",
      "2004 - Suspect is homeless/transient: 93426\n",
      "0432 - Intimidation: 83562\n",
      "\n",
      "Total rows: 774, Time = 30.16 sec"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import csv\n",
    "from io import StringIO\n",
    "from pyspark import SparkContext\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Read CSV files as RDD ,every line is a string\n",
    "crime_rdd = sc.textFile(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv,\" +\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\"\n",
    ")\n",
    "\n",
    "# CSV line parser,handle commas,quotes,escapes\n",
    "def parse_csv_line(line):\n",
    "    return next(csv.reader([line]))  #iterator\n",
    "\n",
    "#each string a list of fields\n",
    "parsed_rdd = crime_rdd.map(parse_csv_line)\n",
    "\n",
    "#  Extract header and name for column and dictionary to refer to its column with its name\n",
    "header = parsed_rdd.first()\n",
    "col_index = {name: idx for idx, name in enumerate(header)}\n",
    "\n",
    "# Now you can do: col_index[\"Mocodes\"]\n",
    "\n",
    "# Remove header row \n",
    "data_rdd = parsed_rdd.filter(lambda row: row != header)\n",
    "\n",
    "#  Extract and explode MOCODES \n",
    "cleaned_data_rdd = data_rdd.filter(\n",
    "    lambda row: (\n",
    "        len(row) > col_index[\"Mocodes\"] and   #ruined columns\n",
    "        row[col_index[\"Mocodes\"]] is not None and\n",
    "        row[col_index[\"Mocodes\"]].strip() != \"\" and  #after removing blanksandwhitespaces if there are still blanks\n",
    "        row[col_index[\"Mocodes\"]].strip().upper() != \"NULL\"\n",
    "    )\n",
    ")\n",
    "#flatmap:each mocode an element of the RDD\n",
    "\n",
    "mocode_rdd = cleaned_data_rdd.flatMap(\n",
    "    lambda row: [\n",
    "        code.strip() \n",
    "        for code in row[col_index[\"Mocodes\"]].split()  # split for multiple codes\n",
    "        if code.strip() != \"\"  # απορρίπτουμε κενά στοιχεία\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "#  Load descriptions \n",
    "mo_dict_rdd = sc.textFile(\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/MO_codes.txt\")\n",
    "mo_dict = (\n",
    "    mo_dict_rdd\n",
    "    .map(lambda line: line.strip())\n",
    "    .filter(lambda line: line != \"\")\n",
    "    .map(lambda line: line.split(\" \", 1))  #seperate the line in the first space, to have mocode and its description\n",
    "    .filter(lambda parts: len(parts) == 2)  #avoid ruined lines with no description\n",
    "    .map(lambda parts: (parts[0].strip(), parts[1].strip()))  #tuple:(mocode,description)\n",
    "    .collectAsMap() #collect RDD as Python dictionary\n",
    ")\n",
    "\n",
    "# Count occurrences , each mocode is a tuple(mocode,1) \n",
    "mocode_counts = mocode_rdd.map(lambda code: (code, 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "#  Attach descriptions \n",
    "mocode_with_desc = mocode_counts.map(\n",
    "    lambda x: (x[0], mo_dict.get(x[0], \"Unknown\"), x[1])\n",
    ")\n",
    "sorted_rdd = mocode_with_desc.sortBy(lambda x: x[2], ascending=False)\n",
    "# Trigger execution\n",
    "total = sorted_rdd.count()\n",
    "end = time.time()\n",
    "top20 = sorted_rdd.take(20)\n",
    "# Show results\n",
    "for code, desc, count in top20:\n",
    "    print(f\"{code} - {desc}: {count}\")\n",
    "\n",
    "print(f\"\\nTotal rows: {total}, Time = {end - start:.2f} sec\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd25068-f768-4475-9f9a-17c8951d74aa",
   "metadata": {},
   "source": [
    "Query 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29ed928-8e9a-4c44-aa0f-0b448736dfee",
   "metadata": {},
   "source": [
    "1 core, 2 GB memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38dd89bd-792d-4dac-8ca8-8c37d4356655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1527</td><td>application_1765289937462_1513</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1513/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-207.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1513_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa3f9d403a064965b2e4e6b857b79d52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '2', 'spark.executor.memory': '2g', 'spark.executor.cores': '1'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1495</td><td>application_1765289937462_1481</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1481/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-61.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1481_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1512</td><td>application_1765289937462_1498</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1498/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-251.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1498_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1517</td><td>application_1765289937462_1503</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1503/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-154.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1503_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1522</td><td>application_1765289937462_1508</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1508/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-30.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1508_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1524</td><td>application_1765289937462_1510</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1510/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-213.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1510_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1527</td><td>application_1765289937462_1513</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1513/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-207.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1513_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr><tr><td>1528</td><td>application_1765289937462_1514</td><td>pyspark</td><td>starting</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1514/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-91.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1514_01_000002/livy\">Link</a></td><td>None</td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\":{\n",
    "        \"spark.executor.instances\": \"2\",\n",
    "        \"spark.executor.memory\": \"2g\",\n",
    "        \"spark.executor.cores\": \"1\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84b272d8-a1a7-4678-a169-b7852ca6d801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b91c0c7c0d74d54b19d9e19a04b61a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total lines: 3138128"
     ]
    }
   ],
   "source": [
    "# Paths for csv\n",
    "fcrime_2010_2019 = \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv\"\n",
    "fcrime_2020_present = \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\"\n",
    "fstations = \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Police_Stations.csv\"\n",
    "fincome = \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_income_2021.csv\"\n",
    "fcodes = \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/RE_codes.csv\"\n",
    "\n",
    "# Paths for GeoJSON\n",
    "fgeo = \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Census_Blocks_2020.geojson\"\n",
    "fgeofields = \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Census_Blocks_2020_fields.csv\"\n",
    "\n",
    "# Imports again\n",
    "\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, FloatType, DateType\n",
    "from pyspark.sql.functions import year, when, count, sum, col, row_number, to_timestamp, regexp_replace, lit, expr, avg, round\n",
    "from pyspark.sql.window import Window\n",
    "from sedona.spark import *\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# Crimes table\n",
    "\n",
    "crimes_schema = StructType([\n",
    "    StructField(\"DR_NO\", StringType()),\n",
    "    StructField(\"DateRptd\", DateType()),\n",
    "    StructField(\"DATEOCC\", StringType()),\n",
    "    StructField(\"TIMEOCC\", StringType()),\n",
    "    StructField(\"AREA\", StringType()),\n",
    "    StructField(\"AREANAME\", StringType()),\n",
    "    StructField(\"RptDistNo\", StringType()),\n",
    "    StructField(\"Part\", IntegerType()),\n",
    "    StructField(\"CrmCd\", StringType()),\n",
    "    StructField(\"Crm Cd Desc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"Vict Age\", StringType()),\n",
    "    StructField(\"VictSex\", StringType()),\n",
    "    StructField(\"VictDescent\", StringType()),\n",
    "    StructField(\"PremisCd\", StringType()),\n",
    "    StructField(\"PremisDesc\", StringType()),\n",
    "    StructField(\"WeaponUsedCd\", StringType()),\n",
    "    StructField(\"WeaponDesc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"Status Desc\", StringType()),\n",
    "    StructField(\"CrmCd1\", StringType()),\n",
    "    StructField(\"CrmCd2\", StringType()),\n",
    "    StructField(\"CrmCd3\", StringType()),\n",
    "    StructField(\"CrmCd4\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"CrossStreet\", StringType()),\n",
    "    StructField(\"LAT\", FloatType()),\n",
    "    StructField(\"LON\", FloatType()),\n",
    "])\n",
    "\n",
    "crimes_df_10_19 = spark.read.csv(fcrime_2010_2019, header=True, schema=crimes_schema, dateFormat='MM/dd/yyyy hh:mm:ss a')\n",
    "crimes_df_20_present = spark.read.csv(fcrime_2020_present, header=True, schema=crimes_schema, dateFormat='MM/dd/yyyy hh:mm:ss a')\n",
    "\n",
    "crimes_df_all = crimes_df_10_19.union(crimes_df_20_present)\n",
    "print(f\"total lines: {crimes_df_all.count()}\")\n",
    "\n",
    "stations_schema = StructType([\n",
    "    StructField(\"X\", FloatType()),\n",
    "    StructField(\"Y\", FloatType()),\n",
    "    StructField(\"FID\", IntegerType()),\n",
    "    StructField(\"DIVISION\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"PREC\", IntegerType())\n",
    "])\n",
    "\n",
    "stations_df = spark.read.csv(fstations, header=True, schema=stations_schema)\n",
    "\n",
    "sedona = SedonaContext.create(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65ef004e-8327-4420-9324-ab4d8cd6bb30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ee8d7d7d6744c7187578f7f3759535b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Spark Explain Output ---\n",
      "== Parsed Logical Plan ==\n",
      "'Sort ['# DESC NULLS LAST], true\n",
      "+- Aggregate [division#279], [division#279, count(1) AS ##419L, round(avg(distance#314), 3) AS average_distance#421]\n",
      "   +- Filter (rank#349 = 1)\n",
      "      +- Project [DR_NO#0, DateRptd#1, DATEOCC#2, TIMEOCC#3, AREA#4, AREANAME#5, RptDistNo#6, Part#7, CrmCd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, VictSex#12, VictDescent#13, PremisCd#14, PremisDesc#15, WeaponUsedCd#16, WeaponDesc#17, Status#18, Status Desc#19, CrmCd1#20, CrmCd2#21, CrmCd3#22, CrmCd4#23, ... 10 more fields]\n",
      "         +- Project [DR_NO#0, DateRptd#1, DATEOCC#2, TIMEOCC#3, AREA#4, AREANAME#5, RptDistNo#6, Part#7, CrmCd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, VictSex#12, VictDescent#13, PremisCd#14, PremisDesc#15, WeaponUsedCd#16, WeaponDesc#17, Status#18, Status Desc#19, CrmCd1#20, CrmCd2#21, CrmCd3#22, CrmCd4#23, ... 11 more fields]\n",
      "            +- Window [row_number() windowspecdefinition(crime_id#209, distance#314 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#349], [crime_id#209], [distance#314 ASC NULLS FIRST]\n",
      "               +- Project [DR_NO#0, DateRptd#1, DATEOCC#2, TIMEOCC#3, AREA#4, AREANAME#5, RptDistNo#6, Part#7, CrmCd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, VictSex#12, VictDescent#13, PremisCd#14, PremisDesc#15, WeaponUsedCd#16, WeaponDesc#17, Status#18, Status Desc#19, CrmCd1#20, CrmCd2#21, CrmCd3#22, CrmCd4#23, ... 9 more fields]\n",
      "                  +- Project [DR_NO#0, DateRptd#1, DATEOCC#2, TIMEOCC#3, AREA#4, AREANAME#5, RptDistNo#6, Part#7, CrmCd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, VictSex#12, VictDescent#13, PremisCd#14, PremisDesc#15, WeaponUsedCd#16, WeaponDesc#17, Status#18, Status Desc#19, CrmCd1#20, CrmCd2#21, CrmCd3#22, CrmCd4#23, ... 9 more fields]\n",
      "                     +- Join Cross\n",
      "                        :- Project [DR_NO#0, DateRptd#1, DATEOCC#2, TIMEOCC#3, AREA#4, AREANAME#5, RptDistNo#6, Part#7, CrmCd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, VictSex#12, VictDescent#13, PremisCd#14, PremisDesc#15, WeaponUsedCd#16, WeaponDesc#17, Status#18, Status Desc#19, CrmCd1#20, CrmCd2#21, CrmCd3#22, CrmCd4#23, ... 6 more fields]\n",
      "                        :  +- Filter NOT ((((cast(LAT#26 as double) = 0.0) AND (cast(LON#27 as double) = 0.0)) OR isnull(LAT#26)) OR isnull(LON#27))\n",
      "                        :     +- Project [DR_NO#0, DateRptd#1, DATEOCC#2, TIMEOCC#3, AREA#4, AREANAME#5, RptDistNo#6, Part#7, CrmCd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, VictSex#12, VictDescent#13, PremisCd#14, PremisDesc#15, WeaponUsedCd#16, WeaponDesc#17, Status#18, Status Desc#19, CrmCd1#20, CrmCd2#21, CrmCd3#22, CrmCd4#23, ... 5 more fields]\n",
      "                        :        +- Union false, false\n",
      "                        :           :- Relation [DR_NO#0,DateRptd#1,DATEOCC#2,TIMEOCC#3,AREA#4,AREANAME#5,RptDistNo#6,Part#7,CrmCd#8,Crm Cd Desc#9,Mocodes#10,Vict Age#11,VictSex#12,VictDescent#13,PremisCd#14,PremisDesc#15,WeaponUsedCd#16,WeaponDesc#17,Status#18,Status Desc#19,CrmCd1#20,CrmCd2#21,CrmCd3#22,CrmCd4#23,... 4 more fields] csv\n",
      "                        :           +- Relation [DR_NO#67,DateRptd#68,DATEOCC#69,TIMEOCC#70,AREA#71,AREANAME#72,RptDistNo#73,Part#74,CrmCd#75,Crm Cd Desc#76,Mocodes#77,Vict Age#78,VictSex#79,VictDescent#80,PremisCd#81,PremisDesc#82,WeaponUsedCd#83,WeaponDesc#84,Status#85,Status Desc#86,CrmCd1#87,CrmCd2#88,CrmCd3#89,CrmCd4#90,... 4 more fields] csv\n",
      "                        +- Project [DIVISION#187 AS division#279, pd_geom#270]\n",
      "                           +- Project [X#184, Y#185, FID#186, DIVISION#187, LOCATION#188, PREC#189,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS pd_geom#270]\n",
      "                              +- Relation [X#184,Y#185,FID#186,DIVISION#187,LOCATION#188,PREC#189] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "division: string, #: bigint, average_distance: double\n",
      "Sort [##419L DESC NULLS LAST], true\n",
      "+- Aggregate [division#279], [division#279, count(1) AS ##419L, round(avg(distance#314), 3) AS average_distance#421]\n",
      "   +- Filter (rank#349 = 1)\n",
      "      +- Project [DR_NO#0, DateRptd#1, DATEOCC#2, TIMEOCC#3, AREA#4, AREANAME#5, RptDistNo#6, Part#7, CrmCd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, VictSex#12, VictDescent#13, PremisCd#14, PremisDesc#15, WeaponUsedCd#16, WeaponDesc#17, Status#18, Status Desc#19, CrmCd1#20, CrmCd2#21, CrmCd3#22, CrmCd4#23, ... 10 more fields]\n",
      "         +- Project [DR_NO#0, DateRptd#1, DATEOCC#2, TIMEOCC#3, AREA#4, AREANAME#5, RptDistNo#6, Part#7, CrmCd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, VictSex#12, VictDescent#13, PremisCd#14, PremisDesc#15, WeaponUsedCd#16, WeaponDesc#17, Status#18, Status Desc#19, CrmCd1#20, CrmCd2#21, CrmCd3#22, CrmCd4#23, ... 11 more fields]\n",
      "            +- Window [row_number() windowspecdefinition(crime_id#209, distance#314 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#349], [crime_id#209], [distance#314 ASC NULLS FIRST]\n",
      "               +- Project [DR_NO#0, DateRptd#1, DATEOCC#2, TIMEOCC#3, AREA#4, AREANAME#5, RptDistNo#6, Part#7, CrmCd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, VictSex#12, VictDescent#13, PremisCd#14, PremisDesc#15, WeaponUsedCd#16, WeaponDesc#17, Status#18, Status Desc#19, CrmCd1#20, CrmCd2#21, CrmCd3#22, CrmCd4#23, ... 9 more fields]\n",
      "                  +- Project [DR_NO#0, DateRptd#1, DATEOCC#2, TIMEOCC#3, AREA#4, AREANAME#5, RptDistNo#6, Part#7, CrmCd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, VictSex#12, VictDescent#13, PremisCd#14, PremisDesc#15, WeaponUsedCd#16, WeaponDesc#17, Status#18, Status Desc#19, CrmCd1#20, CrmCd2#21, CrmCd3#22, CrmCd4#23, ... 9 more fields]\n",
      "                     +- Join Cross\n",
      "                        :- Project [DR_NO#0, DateRptd#1, DATEOCC#2, TIMEOCC#3, AREA#4, AREANAME#5, RptDistNo#6, Part#7, CrmCd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, VictSex#12, VictDescent#13, PremisCd#14, PremisDesc#15, WeaponUsedCd#16, WeaponDesc#17, Status#18, Status Desc#19, CrmCd1#20, CrmCd2#21, CrmCd3#22, CrmCd4#23, ... 6 more fields]\n",
      "                        :  +- Filter NOT ((((cast(LAT#26 as double) = 0.0) AND (cast(LON#27 as double) = 0.0)) OR isnull(LAT#26)) OR isnull(LON#27))\n",
      "                        :     +- Project [DR_NO#0, DateRptd#1, DATEOCC#2, TIMEOCC#3, AREA#4, AREANAME#5, RptDistNo#6, Part#7, CrmCd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, VictSex#12, VictDescent#13, PremisCd#14, PremisDesc#15, WeaponUsedCd#16, WeaponDesc#17, Status#18, Status Desc#19, CrmCd1#20, CrmCd2#21, CrmCd3#22, CrmCd4#23, ... 5 more fields]\n",
      "                        :        +- Union false, false\n",
      "                        :           :- Relation [DR_NO#0,DateRptd#1,DATEOCC#2,TIMEOCC#3,AREA#4,AREANAME#5,RptDistNo#6,Part#7,CrmCd#8,Crm Cd Desc#9,Mocodes#10,Vict Age#11,VictSex#12,VictDescent#13,PremisCd#14,PremisDesc#15,WeaponUsedCd#16,WeaponDesc#17,Status#18,Status Desc#19,CrmCd1#20,CrmCd2#21,CrmCd3#22,CrmCd4#23,... 4 more fields] csv\n",
      "                        :           +- Relation [DR_NO#67,DateRptd#68,DATEOCC#69,TIMEOCC#70,AREA#71,AREANAME#72,RptDistNo#73,Part#74,CrmCd#75,Crm Cd Desc#76,Mocodes#77,Vict Age#78,VictSex#79,VictDescent#80,PremisCd#81,PremisDesc#82,WeaponUsedCd#83,WeaponDesc#84,Status#85,Status Desc#86,CrmCd1#87,CrmCd2#88,CrmCd3#89,CrmCd4#90,... 4 more fields] csv\n",
      "                        +- Project [DIVISION#187 AS division#279, pd_geom#270]\n",
      "                           +- Project [X#184, Y#185, FID#186, DIVISION#187, LOCATION#188, PREC#189,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS pd_geom#270]\n",
      "                              +- Relation [X#184,Y#185,FID#186,DIVISION#187,LOCATION#188,PREC#189] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [##419L DESC NULLS LAST], true\n",
      "+- Aggregate [division#279], [division#279, count(1) AS ##419L, round(avg(distance#314), 3) AS average_distance#421]\n",
      "   +- Project [division#279, distance#314]\n",
      "      +- Filter (rank#349 = 1)\n",
      "         +- Window [row_number() windowspecdefinition(crime_id#209, distance#314 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#349], [crime_id#209], [distance#314 ASC NULLS FIRST]\n",
      "            +- WindowGroupLimit [crime_id#209], [distance#314 ASC NULLS FIRST], row_number(), 1\n",
      "               +- Project [crime_id#209, division#279, ( **org.apache.spark.sql.sedona_sql.expressions.ST_DistanceSphere**   / 1000.0) AS distance#314]\n",
      "                  +- Join Cross\n",
      "                     :- Project [crime_id#209,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_geom#239]\n",
      "                     :  +- Filter (((NOT (LAT#26 = 0.0) OR NOT (LON#27 = 0.0)) AND isnotnull(LAT#26)) AND isnotnull(LON#27))\n",
      "                     :     +- Project [LAT#26, LON#27, uuid(Some(5900878890388655255)) AS crime_id#209]\n",
      "                     :        +- Union false, false\n",
      "                     :           :- Project [LAT#26, LON#27]\n",
      "                     :           :  +- Relation [DR_NO#0,DateRptd#1,DATEOCC#2,TIMEOCC#3,AREA#4,AREANAME#5,RptDistNo#6,Part#7,CrmCd#8,Crm Cd Desc#9,Mocodes#10,Vict Age#11,VictSex#12,VictDescent#13,PremisCd#14,PremisDesc#15,WeaponUsedCd#16,WeaponDesc#17,Status#18,Status Desc#19,CrmCd1#20,CrmCd2#21,CrmCd3#22,CrmCd4#23,... 4 more fields] csv\n",
      "                     :           +- Project [LAT#93, LON#94]\n",
      "                     :              +- Relation [DR_NO#67,DateRptd#68,DATEOCC#69,TIMEOCC#70,AREA#71,AREANAME#72,RptDistNo#73,Part#74,CrmCd#75,Crm Cd Desc#76,Mocodes#77,Vict Age#78,VictSex#79,VictDescent#80,PremisCd#81,PremisDesc#82,WeaponUsedCd#83,WeaponDesc#84,Status#85,Status Desc#86,CrmCd1#87,CrmCd2#88,CrmCd3#89,CrmCd4#90,... 4 more fields] csv\n",
      "                     +- Project [DIVISION#187 AS division#279,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS pd_geom#270]\n",
      "                        +- Relation [X#184,Y#185,FID#186,DIVISION#187,LOCATION#188,PREC#189] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [##419L DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(##419L DESC NULLS LAST, 1000), ENSURE_REQUIREMENTS, [plan_id=166]\n",
      "      +- HashAggregate(keys=[division#279], functions=[count(1), avg(distance#314)], output=[division#279, ##419L, average_distance#421], schema specialized)\n",
      "         +- Exchange hashpartitioning(division#279, 1000), ENSURE_REQUIREMENTS, [plan_id=163]\n",
      "            +- HashAggregate(keys=[division#279], functions=[partial_count(1), partial_avg(distance#314)], output=[division#279, count#426L, sum#429, count#430L], schema specialized)\n",
      "               +- Project [division#279, distance#314]\n",
      "                  +- Filter (rank#349 = 1)\n",
      "                     +- Window [row_number() windowspecdefinition(crime_id#209, distance#314 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#349], [crime_id#209], [distance#314 ASC NULLS FIRST]\n",
      "                        +- WindowGroupLimit [crime_id#209], [distance#314 ASC NULLS FIRST], row_number(), 1, Final\n",
      "                           +- Sort [crime_id#209 ASC NULLS FIRST, distance#314 ASC NULLS FIRST], false, 0\n",
      "                              +- Exchange hashpartitioning(crime_id#209, 1000), ENSURE_REQUIREMENTS, [plan_id=155]\n",
      "                                 +- WindowGroupLimit [crime_id#209], [distance#314 ASC NULLS FIRST], row_number(), 1, Partial\n",
      "                                    +- Sort [crime_id#209 ASC NULLS FIRST, distance#314 ASC NULLS FIRST], false, 0\n",
      "                                       +- Project [crime_id#209, division#279, ( **org.apache.spark.sql.sedona_sql.expressions.ST_DistanceSphere**   / 1000.0) AS distance#314]\n",
      "                                          +- BroadcastNestedLoopJoin BuildRight, Cross\n",
      "                                             :- Project [crime_id#209,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_geom#239]\n",
      "                                             :  +- Filter (((NOT (LAT#26 = 0.0) OR NOT (LON#27 = 0.0)) AND isnotnull(LAT#26)) AND isnotnull(LON#27))\n",
      "                                             :     +- Project [LAT#26, LON#27, uuid(Some(5900878890388655255)) AS crime_id#209]\n",
      "                                             :        +- Union\n",
      "                                             :           :- FileScan csv [LAT#26,LON#27] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<LAT:float,LON:float>\n",
      "                                             :           +- FileScan csv [LAT#93,LON#94] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<LAT:float,LON:float>\n",
      "                                             +- BroadcastExchange IdentityBroadcastMode, [plan_id=148]\n",
      "                                                +- Project [DIVISION#187 AS division#279,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS pd_geom#270]\n",
      "                                                   +- FileScan csv [X#184,Y#185,DIVISION#187] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_P..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<X:float,Y:float,DIVISION:string>\n",
      "\n",
      "+----------------+------+----------------+\n",
      "|division        |#     |average_distance|\n",
      "+----------------+------+----------------+\n",
      "|HOLLYWOOD       |225515|2.076           |\n",
      "|VAN NUYS        |211130|2.953           |\n",
      "|SOUTHWEST       |189572|2.191           |\n",
      "|WILSHIRE        |187054|2.593           |\n",
      "|77TH STREET     |172558|1.717           |\n",
      "|OLYMPIC         |172353|1.725           |\n",
      "|NORTH HOLLYWOOD |168655|2.643           |\n",
      "|PACIFIC         |162514|3.853           |\n",
      "|CENTRAL         |154952|0.993           |\n",
      "|SOUTHEAST       |153746|2.422           |\n",
      "|RAMPART         |153690|1.535           |\n",
      "|TOPANGA         |141070|3.298           |\n",
      "|WEST VALLEY     |139820|3.039           |\n",
      "|FOOTHILL        |135381|4.251           |\n",
      "|HARBOR          |127370|3.702           |\n",
      "|HOLLENBECK      |116558|2.677           |\n",
      "|WEST LOS ANGELES|116308|2.79            |\n",
      "|NEWTON          |111628|1.635           |\n",
      "|NORTHEAST       |108549|3.623           |\n",
      "|MISSION         |105331|3.685           |\n",
      "|DEVONSHIRE      |81226 |2.824           |\n",
      "+----------------+------+----------------+\n",
      "\n",
      "Execution time : 86.9510 seconds"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Δημιουργία ID για το partition (χρήσιμο για το window function)\n",
    "crimes_df_prep = crimes_df_all.withColumn(\"crime_id\", expr(\"uuid()\"))\n",
    "\n",
    "crimes_filtered = crimes_df_prep.filter(\n",
    "    ~((col(\"LAT\") == 0.0) & (col(\"LON\") == 0.0) | col(\"LAT\").isNull() | col(\"LON\").isNull())\n",
    ")\n",
    "\n",
    "# Μετατροπή των συντεταγμένων σε Sedona Point Geometry\n",
    "crimes_with_geom = crimes_filtered.withColumn(\n",
    "    \"crime_geom\",\n",
    "    ST_Point(col(\"LON\"), col(\"LAT\"))\n",
    ")\n",
    "\n",
    "pd_with_geom = stations_df.withColumn(\n",
    "    \"pd_geom\",\n",
    "    ST_Point(col(\"X\"), col(\"Y\"))\n",
    ").select(\n",
    "    col(\"DIVISION\").alias(\"division\"),\n",
    "    col(\"pd_geom\")\n",
    ")\n",
    "\n",
    "# Κάθε έγκλημα συνδέεται με κάθε αστυνομικό τμήμα\n",
    "cross_joined_df = crimes_with_geom.crossJoin(pd_with_geom)\n",
    "\n",
    "# Υπολογισμός της απόστασης \n",
    "df_with_distance = cross_joined_df.withColumn(\n",
    "    \"distance\",\n",
    "    (ST_DistanceSphere(col(\"crime_geom\"), col(\"pd_geom\"))/1000)\n",
    ")\n",
    "\n",
    "# Διαμέριση ανά crime_id και ταξινόμηση ανά distance\n",
    "window_spec = Window.partitionBy(\"crime_id\").orderBy(col(\"distance\").asc())\n",
    "\n",
    "df_ranked = df_with_distance.withColumn(\n",
    "    \"rank\",\n",
    "    row_number().over(window_spec)\n",
    ")\n",
    "\n",
    "closest_crimes_df = df_ranked.filter(col(\"rank\") == 1)\n",
    "\n",
    "# Τελικός υπολογισμός: Αριθμός εγκλημάτων και μέση απόσταση ανά τμήμα\n",
    "final_result_df = closest_crimes_df.groupBy(col(\"division\")).agg(\n",
    "    count(\"*\").alias(\"#\"),\n",
    "    round(avg(col(\"distance\")), 3).alias(\"average_distance\")\n",
    ")\n",
    "\n",
    "final_sorted_result = final_result_df.orderBy(col(\"#\").desc())\n",
    "\n",
    "print(\"\\n--- Spark Explain Output ---\")\n",
    "final_sorted_result.explain(True)\n",
    "\n",
    "final_sorted_result.show(final_sorted_result.count(), truncate=False)\n",
    "\n",
    "end_time = time.time()\n",
    "time = end_time - start_time\n",
    "print(f\"Execution time : {time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813b7d14-6a67-40ba-99ac-fa072d36cd90",
   "metadata": {},
   "source": [
    " 2 cores, 4GB memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bdafb341-2d2a-4fdf-bcfb-c2d334501e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>595</td><td>application_1765289937462_0588</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0588/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-141.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0588_01_000002/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ceb8707d417473f8c0ffa219b86bf65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '2', 'spark.executor.memory': '4g', 'spark.executor.cores': '2'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>550</td><td>application_1765289937462_0543</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0543/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-48.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0543_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>558</td><td>application_1765289937462_0551</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0551/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-207.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0551_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>571</td><td>application_1765289937462_0564</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0564/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-115.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0564_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>573</td><td>application_1765289937462_0566</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0566/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-115.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0566_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>575</td><td>application_1765289937462_0568</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0568/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-213.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0568_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>576</td><td>application_1765289937462_0569</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0569/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-154.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0569_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>584</td><td>application_1765289937462_0577</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0577/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-207.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0577_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>589</td><td>application_1765289937462_0582</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0582/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-250.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0582_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>592</td><td>application_1765289937462_0585</td><td>pyspark</td><td>busy</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0585/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-103.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0585_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>593</td><td>application_1765289937462_0586</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0586/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-156.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0586_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>595</td><td>application_1765289937462_0588</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0588/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-141.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_0588_01_000002/livy\">Link</a></td><td>None</td><td>✔</td></tr><tr><td>596</td><td>application_1765289937462_0589</td><td>pyspark</td><td>starting</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_0589/\">Link</a></td><td></td><td>None</td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\":{\n",
    "        \"spark.executor.instances\": \"2\",\n",
    "        \"spark.executor.memory\": \"4g\",\n",
    "        \"spark.executor.cores\": \"2\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "64213af2-3d81-453e-aee1-2b6e0a0e4a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73989104f1a24e54be2bb2d67079164f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total lines: 3138128"
     ]
    }
   ],
   "source": [
    "# Paths for csv\n",
    "fcrime_2010_2019 = \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv\"\n",
    "fcrime_2020_present = \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\"\n",
    "fstations = \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Police_Stations.csv\"\n",
    "fincome = \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_income_2021.csv\"\n",
    "fcodes = \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/RE_codes.csv\"\n",
    "\n",
    "# Paths for GeoJSON\n",
    "fgeo = \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Census_Blocks_2020.geojson\"\n",
    "fgeofields = \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Census_Blocks_2020_fields.csv\"\n",
    "\n",
    "# Imports again\n",
    "\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, FloatType, DateType\n",
    "from pyspark.sql.functions import year, when, count, sum, col, row_number, to_timestamp, regexp_replace, lit, expr, avg, round\n",
    "from pyspark.sql.window import Window\n",
    "from sedona.spark import *\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# Crimes table\n",
    "\n",
    "crimes_schema = StructType([\n",
    "    StructField(\"DR_NO\", StringType()),\n",
    "    StructField(\"DateRptd\", DateType()),\n",
    "    StructField(\"DATEOCC\", StringType()),\n",
    "    StructField(\"TIMEOCC\", StringType()),\n",
    "    StructField(\"AREA\", StringType()),\n",
    "    StructField(\"AREANAME\", StringType()),\n",
    "    StructField(\"RptDistNo\", StringType()),\n",
    "    StructField(\"Part\", IntegerType()),\n",
    "    StructField(\"CrmCd\", StringType()),\n",
    "    StructField(\"Crm Cd Desc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"Vict Age\", StringType()),\n",
    "    StructField(\"VictSex\", StringType()),\n",
    "    StructField(\"VictDescent\", StringType()),\n",
    "    StructField(\"PremisCd\", StringType()),\n",
    "    StructField(\"PremisDesc\", StringType()),\n",
    "    StructField(\"WeaponUsedCd\", StringType()),\n",
    "    StructField(\"WeaponDesc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"Status Desc\", StringType()),\n",
    "    StructField(\"CrmCd1\", StringType()),\n",
    "    StructField(\"CrmCd2\", StringType()),\n",
    "    StructField(\"CrmCd3\", StringType()),\n",
    "    StructField(\"CrmCd4\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"CrossStreet\", StringType()),\n",
    "    StructField(\"LAT\", FloatType()),\n",
    "    StructField(\"LON\", FloatType()),\n",
    "])\n",
    "\n",
    "crimes_df_10_19 = spark.read.csv(fcrime_2010_2019, header=True, schema=crimes_schema, dateFormat='MM/dd/yyyy hh:mm:ss a')\n",
    "crimes_df_20_present = spark.read.csv(fcrime_2020_present, header=True, schema=crimes_schema, dateFormat='MM/dd/yyyy hh:mm:ss a')\n",
    "\n",
    "crimes_df_all = crimes_df_10_19.union(crimes_df_20_present)\n",
    "print(f\"total lines: {crimes_df_all.count()}\")\n",
    "\n",
    "stations_schema = StructType([\n",
    "    StructField(\"X\", FloatType()),\n",
    "    StructField(\"Y\", FloatType()),\n",
    "    StructField(\"FID\", IntegerType()),\n",
    "    StructField(\"DIVISION\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"PREC\", IntegerType())\n",
    "])\n",
    "\n",
    "stations_df = spark.read.csv(fstations, header=True, schema=stations_schema)\n",
    "\n",
    "sedona = SedonaContext.create(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "583f3f1f-b21e-4a2c-846c-1f6188103060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "857194da7a6e4c0793b334fff3b820b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Spark Explain Output ---\n",
      "== Parsed Logical Plan ==\n",
      "'Sort ['# DESC NULLS LAST], true\n",
      "+- Aggregate [division#279], [division#279, count(1) AS ##419L, round(avg(distance#314), 3) AS average_distance#421]\n",
      "   +- Filter (rank#349 = 1)\n",
      "      +- Project [DR_NO#0, DateRptd#1, DATEOCC#2, TIMEOCC#3, AREA#4, AREANAME#5, RptDistNo#6, Part#7, CrmCd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, VictSex#12, VictDescent#13, PremisCd#14, PremisDesc#15, WeaponUsedCd#16, WeaponDesc#17, Status#18, Status Desc#19, CrmCd1#20, CrmCd2#21, CrmCd3#22, CrmCd4#23, ... 10 more fields]\n",
      "         +- Project [DR_NO#0, DateRptd#1, DATEOCC#2, TIMEOCC#3, AREA#4, AREANAME#5, RptDistNo#6, Part#7, CrmCd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, VictSex#12, VictDescent#13, PremisCd#14, PremisDesc#15, WeaponUsedCd#16, WeaponDesc#17, Status#18, Status Desc#19, CrmCd1#20, CrmCd2#21, CrmCd3#22, CrmCd4#23, ... 11 more fields]\n",
      "            +- Window [row_number() windowspecdefinition(crime_id#209, distance#314 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#349], [crime_id#209], [distance#314 ASC NULLS FIRST]\n",
      "               +- Project [DR_NO#0, DateRptd#1, DATEOCC#2, TIMEOCC#3, AREA#4, AREANAME#5, RptDistNo#6, Part#7, CrmCd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, VictSex#12, VictDescent#13, PremisCd#14, PremisDesc#15, WeaponUsedCd#16, WeaponDesc#17, Status#18, Status Desc#19, CrmCd1#20, CrmCd2#21, CrmCd3#22, CrmCd4#23, ... 9 more fields]\n",
      "                  +- Project [DR_NO#0, DateRptd#1, DATEOCC#2, TIMEOCC#3, AREA#4, AREANAME#5, RptDistNo#6, Part#7, CrmCd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, VictSex#12, VictDescent#13, PremisCd#14, PremisDesc#15, WeaponUsedCd#16, WeaponDesc#17, Status#18, Status Desc#19, CrmCd1#20, CrmCd2#21, CrmCd3#22, CrmCd4#23, ... 9 more fields]\n",
      "                     +- Join Cross\n",
      "                        :- Project [DR_NO#0, DateRptd#1, DATEOCC#2, TIMEOCC#3, AREA#4, AREANAME#5, RptDistNo#6, Part#7, CrmCd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, VictSex#12, VictDescent#13, PremisCd#14, PremisDesc#15, WeaponUsedCd#16, WeaponDesc#17, Status#18, Status Desc#19, CrmCd1#20, CrmCd2#21, CrmCd3#22, CrmCd4#23, ... 6 more fields]\n",
      "                        :  +- Filter NOT ((((cast(LAT#26 as double) = 0.0) AND (cast(LON#27 as double) = 0.0)) OR isnull(LAT#26)) OR isnull(LON#27))\n",
      "                        :     +- Project [DR_NO#0, DateRptd#1, DATEOCC#2, TIMEOCC#3, AREA#4, AREANAME#5, RptDistNo#6, Part#7, CrmCd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, VictSex#12, VictDescent#13, PremisCd#14, PremisDesc#15, WeaponUsedCd#16, WeaponDesc#17, Status#18, Status Desc#19, CrmCd1#20, CrmCd2#21, CrmCd3#22, CrmCd4#23, ... 5 more fields]\n",
      "                        :        +- Union false, false\n",
      "                        :           :- Relation [DR_NO#0,DateRptd#1,DATEOCC#2,TIMEOCC#3,AREA#4,AREANAME#5,RptDistNo#6,Part#7,CrmCd#8,Crm Cd Desc#9,Mocodes#10,Vict Age#11,VictSex#12,VictDescent#13,PremisCd#14,PremisDesc#15,WeaponUsedCd#16,WeaponDesc#17,Status#18,Status Desc#19,CrmCd1#20,CrmCd2#21,CrmCd3#22,CrmCd4#23,... 4 more fields] csv\n",
      "                        :           +- Relation [DR_NO#67,DateRptd#68,DATEOCC#69,TIMEOCC#70,AREA#71,AREANAME#72,RptDistNo#73,Part#74,CrmCd#75,Crm Cd Desc#76,Mocodes#77,Vict Age#78,VictSex#79,VictDescent#80,PremisCd#81,PremisDesc#82,WeaponUsedCd#83,WeaponDesc#84,Status#85,Status Desc#86,CrmCd1#87,CrmCd2#88,CrmCd3#89,CrmCd4#90,... 4 more fields] csv\n",
      "                        +- Project [DIVISION#187 AS division#279, pd_geom#270]\n",
      "                           +- Project [X#184, Y#185, FID#186, DIVISION#187, LOCATION#188, PREC#189,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS pd_geom#270]\n",
      "                              +- Relation [X#184,Y#185,FID#186,DIVISION#187,LOCATION#188,PREC#189] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "division: string, #: bigint, average_distance: double\n",
      "Sort [##419L DESC NULLS LAST], true\n",
      "+- Aggregate [division#279], [division#279, count(1) AS ##419L, round(avg(distance#314), 3) AS average_distance#421]\n",
      "   +- Filter (rank#349 = 1)\n",
      "      +- Project [DR_NO#0, DateRptd#1, DATEOCC#2, TIMEOCC#3, AREA#4, AREANAME#5, RptDistNo#6, Part#7, CrmCd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, VictSex#12, VictDescent#13, PremisCd#14, PremisDesc#15, WeaponUsedCd#16, WeaponDesc#17, Status#18, Status Desc#19, CrmCd1#20, CrmCd2#21, CrmCd3#22, CrmCd4#23, ... 10 more fields]\n",
      "         +- Project [DR_NO#0, DateRptd#1, DATEOCC#2, TIMEOCC#3, AREA#4, AREANAME#5, RptDistNo#6, Part#7, CrmCd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, VictSex#12, VictDescent#13, PremisCd#14, PremisDesc#15, WeaponUsedCd#16, WeaponDesc#17, Status#18, Status Desc#19, CrmCd1#20, CrmCd2#21, CrmCd3#22, CrmCd4#23, ... 11 more fields]\n",
      "            +- Window [row_number() windowspecdefinition(crime_id#209, distance#314 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#349], [crime_id#209], [distance#314 ASC NULLS FIRST]\n",
      "               +- Project [DR_NO#0, DateRptd#1, DATEOCC#2, TIMEOCC#3, AREA#4, AREANAME#5, RptDistNo#6, Part#7, CrmCd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, VictSex#12, VictDescent#13, PremisCd#14, PremisDesc#15, WeaponUsedCd#16, WeaponDesc#17, Status#18, Status Desc#19, CrmCd1#20, CrmCd2#21, CrmCd3#22, CrmCd4#23, ... 9 more fields]\n",
      "                  +- Project [DR_NO#0, DateRptd#1, DATEOCC#2, TIMEOCC#3, AREA#4, AREANAME#5, RptDistNo#6, Part#7, CrmCd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, VictSex#12, VictDescent#13, PremisCd#14, PremisDesc#15, WeaponUsedCd#16, WeaponDesc#17, Status#18, Status Desc#19, CrmCd1#20, CrmCd2#21, CrmCd3#22, CrmCd4#23, ... 9 more fields]\n",
      "                     +- Join Cross\n",
      "                        :- Project [DR_NO#0, DateRptd#1, DATEOCC#2, TIMEOCC#3, AREA#4, AREANAME#5, RptDistNo#6, Part#7, CrmCd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, VictSex#12, VictDescent#13, PremisCd#14, PremisDesc#15, WeaponUsedCd#16, WeaponDesc#17, Status#18, Status Desc#19, CrmCd1#20, CrmCd2#21, CrmCd3#22, CrmCd4#23, ... 6 more fields]\n",
      "                        :  +- Filter NOT ((((cast(LAT#26 as double) = 0.0) AND (cast(LON#27 as double) = 0.0)) OR isnull(LAT#26)) OR isnull(LON#27))\n",
      "                        :     +- Project [DR_NO#0, DateRptd#1, DATEOCC#2, TIMEOCC#3, AREA#4, AREANAME#5, RptDistNo#6, Part#7, CrmCd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, VictSex#12, VictDescent#13, PremisCd#14, PremisDesc#15, WeaponUsedCd#16, WeaponDesc#17, Status#18, Status Desc#19, CrmCd1#20, CrmCd2#21, CrmCd3#22, CrmCd4#23, ... 5 more fields]\n",
      "                        :        +- Union false, false\n",
      "                        :           :- Relation [DR_NO#0,DateRptd#1,DATEOCC#2,TIMEOCC#3,AREA#4,AREANAME#5,RptDistNo#6,Part#7,CrmCd#8,Crm Cd Desc#9,Mocodes#10,Vict Age#11,VictSex#12,VictDescent#13,PremisCd#14,PremisDesc#15,WeaponUsedCd#16,WeaponDesc#17,Status#18,Status Desc#19,CrmCd1#20,CrmCd2#21,CrmCd3#22,CrmCd4#23,... 4 more fields] csv\n",
      "                        :           +- Relation [DR_NO#67,DateRptd#68,DATEOCC#69,TIMEOCC#70,AREA#71,AREANAME#72,RptDistNo#73,Part#74,CrmCd#75,Crm Cd Desc#76,Mocodes#77,Vict Age#78,VictSex#79,VictDescent#80,PremisCd#81,PremisDesc#82,WeaponUsedCd#83,WeaponDesc#84,Status#85,Status Desc#86,CrmCd1#87,CrmCd2#88,CrmCd3#89,CrmCd4#90,... 4 more fields] csv\n",
      "                        +- Project [DIVISION#187 AS division#279, pd_geom#270]\n",
      "                           +- Project [X#184, Y#185, FID#186, DIVISION#187, LOCATION#188, PREC#189,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS pd_geom#270]\n",
      "                              +- Relation [X#184,Y#185,FID#186,DIVISION#187,LOCATION#188,PREC#189] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [##419L DESC NULLS LAST], true\n",
      "+- Aggregate [division#279], [division#279, count(1) AS ##419L, round(avg(distance#314), 3) AS average_distance#421]\n",
      "   +- Project [division#279, distance#314]\n",
      "      +- Filter (rank#349 = 1)\n",
      "         +- Window [row_number() windowspecdefinition(crime_id#209, distance#314 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#349], [crime_id#209], [distance#314 ASC NULLS FIRST]\n",
      "            +- WindowGroupLimit [crime_id#209], [distance#314 ASC NULLS FIRST], row_number(), 1\n",
      "               +- Project [crime_id#209, division#279, ( **org.apache.spark.sql.sedona_sql.expressions.ST_DistanceSphere**   / 1000.0) AS distance#314]\n",
      "                  +- Join Cross\n",
      "                     :- Project [crime_id#209,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_geom#239]\n",
      "                     :  +- Filter (((NOT (LAT#26 = 0.0) OR NOT (LON#27 = 0.0)) AND isnotnull(LAT#26)) AND isnotnull(LON#27))\n",
      "                     :     +- Project [LAT#26, LON#27, uuid(Some(208099601882077651)) AS crime_id#209]\n",
      "                     :        +- Union false, false\n",
      "                     :           :- Project [LAT#26, LON#27]\n",
      "                     :           :  +- Relation [DR_NO#0,DateRptd#1,DATEOCC#2,TIMEOCC#3,AREA#4,AREANAME#5,RptDistNo#6,Part#7,CrmCd#8,Crm Cd Desc#9,Mocodes#10,Vict Age#11,VictSex#12,VictDescent#13,PremisCd#14,PremisDesc#15,WeaponUsedCd#16,WeaponDesc#17,Status#18,Status Desc#19,CrmCd1#20,CrmCd2#21,CrmCd3#22,CrmCd4#23,... 4 more fields] csv\n",
      "                     :           +- Project [LAT#93, LON#94]\n",
      "                     :              +- Relation [DR_NO#67,DateRptd#68,DATEOCC#69,TIMEOCC#70,AREA#71,AREANAME#72,RptDistNo#73,Part#74,CrmCd#75,Crm Cd Desc#76,Mocodes#77,Vict Age#78,VictSex#79,VictDescent#80,PremisCd#81,PremisDesc#82,WeaponUsedCd#83,WeaponDesc#84,Status#85,Status Desc#86,CrmCd1#87,CrmCd2#88,CrmCd3#89,CrmCd4#90,... 4 more fields] csv\n",
      "                     +- Project [DIVISION#187 AS division#279,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS pd_geom#270]\n",
      "                        +- Relation [X#184,Y#185,FID#186,DIVISION#187,LOCATION#188,PREC#189] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [##419L DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(##419L DESC NULLS LAST, 1000), ENSURE_REQUIREMENTS, [plan_id=166]\n",
      "      +- HashAggregate(keys=[division#279], functions=[count(1), avg(distance#314)], output=[division#279, ##419L, average_distance#421], schema specialized)\n",
      "         +- Exchange hashpartitioning(division#279, 1000), ENSURE_REQUIREMENTS, [plan_id=163]\n",
      "            +- HashAggregate(keys=[division#279], functions=[partial_count(1), partial_avg(distance#314)], output=[division#279, count#426L, sum#429, count#430L], schema specialized)\n",
      "               +- Project [division#279, distance#314]\n",
      "                  +- Filter (rank#349 = 1)\n",
      "                     +- Window [row_number() windowspecdefinition(crime_id#209, distance#314 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#349], [crime_id#209], [distance#314 ASC NULLS FIRST]\n",
      "                        +- WindowGroupLimit [crime_id#209], [distance#314 ASC NULLS FIRST], row_number(), 1, Final\n",
      "                           +- Sort [crime_id#209 ASC NULLS FIRST, distance#314 ASC NULLS FIRST], false, 0\n",
      "                              +- Exchange hashpartitioning(crime_id#209, 1000), ENSURE_REQUIREMENTS, [plan_id=155]\n",
      "                                 +- WindowGroupLimit [crime_id#209], [distance#314 ASC NULLS FIRST], row_number(), 1, Partial\n",
      "                                    +- Sort [crime_id#209 ASC NULLS FIRST, distance#314 ASC NULLS FIRST], false, 0\n",
      "                                       +- Project [crime_id#209, division#279, ( **org.apache.spark.sql.sedona_sql.expressions.ST_DistanceSphere**   / 1000.0) AS distance#314]\n",
      "                                          +- BroadcastNestedLoopJoin BuildRight, Cross\n",
      "                                             :- Project [crime_id#209,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_geom#239]\n",
      "                                             :  +- Filter (((NOT (LAT#26 = 0.0) OR NOT (LON#27 = 0.0)) AND isnotnull(LAT#26)) AND isnotnull(LON#27))\n",
      "                                             :     +- Project [LAT#26, LON#27, uuid(Some(208099601882077651)) AS crime_id#209]\n",
      "                                             :        +- Union\n",
      "                                             :           :- FileScan csv [LAT#26,LON#27] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<LAT:float,LON:float>\n",
      "                                             :           +- FileScan csv [LAT#93,LON#94] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<LAT:float,LON:float>\n",
      "                                             +- BroadcastExchange IdentityBroadcastMode, [plan_id=148]\n",
      "                                                +- Project [DIVISION#187 AS division#279,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS pd_geom#270]\n",
      "                                                   +- FileScan csv [X#184,Y#185,DIVISION#187] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_P..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<X:float,Y:float,DIVISION:string>\n",
      "\n",
      "+----------------+------+----------------+\n",
      "|division        |#     |average_distance|\n",
      "+----------------+------+----------------+\n",
      "|HOLLYWOOD       |225515|2.076           |\n",
      "|VAN NUYS        |211130|2.953           |\n",
      "|SOUTHWEST       |189572|2.191           |\n",
      "|WILSHIRE        |187054|2.593           |\n",
      "|77TH STREET     |172558|1.717           |\n",
      "|OLYMPIC         |172353|1.725           |\n",
      "|NORTH HOLLYWOOD |168655|2.643           |\n",
      "|PACIFIC         |162514|3.853           |\n",
      "|CENTRAL         |154952|0.993           |\n",
      "|SOUTHEAST       |153746|2.422           |\n",
      "|RAMPART         |153690|1.535           |\n",
      "|TOPANGA         |141070|3.298           |\n",
      "|WEST VALLEY     |139820|3.039           |\n",
      "|FOOTHILL        |135381|4.251           |\n",
      "|HARBOR          |127370|3.702           |\n",
      "|HOLLENBECK      |116558|2.677           |\n",
      "|WEST LOS ANGELES|116308|2.79            |\n",
      "|NEWTON          |111628|1.635           |\n",
      "|NORTHEAST       |108549|3.623           |\n",
      "|MISSION         |105331|3.685           |\n",
      "|DEVONSHIRE      |81226 |2.824           |\n",
      "+----------------+------+----------------+\n",
      "\n",
      "Execution time : 46.0770 seconds"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Δημιουργία ID για το partition (χρήσιμο για το window function)\n",
    "crimes_df_prep = crimes_df_all.withColumn(\"crime_id\", expr(\"uuid()\"))\n",
    "\n",
    "crimes_filtered = crimes_df_prep.filter(\n",
    "    ~((col(\"LAT\") == 0.0) & (col(\"LON\") == 0.0) | col(\"LAT\").isNull() | col(\"LON\").isNull())\n",
    ")\n",
    "\n",
    "# Μετατροπή των συντεταγμένων σε Sedona Point Geometry\n",
    "crimes_with_geom = crimes_filtered.withColumn(\n",
    "    \"crime_geom\",\n",
    "    ST_Point(col(\"LON\"), col(\"LAT\"))\n",
    ")\n",
    "\n",
    "pd_with_geom = stations_df.withColumn(\n",
    "    \"pd_geom\",\n",
    "    ST_Point(col(\"X\"), col(\"Y\"))\n",
    ").select(\n",
    "    col(\"DIVISION\").alias(\"division\"),\n",
    "    col(\"pd_geom\")\n",
    ")\n",
    "\n",
    "# Κάθε έγκλημα συνδέεται με κάθε αστυνομικό τμήμα\n",
    "cross_joined_df = crimes_with_geom.crossJoin(pd_with_geom)\n",
    "\n",
    "# Υπολογισμός της απόστασης \n",
    "df_with_distance = cross_joined_df.withColumn(\n",
    "    \"distance\",\n",
    "    (ST_DistanceSphere(col(\"crime_geom\"), col(\"pd_geom\"))/1000)\n",
    ")\n",
    "\n",
    "# Διαμέριση ανά crime_id και ταξινόμηση ανά distance\n",
    "window_spec = Window.partitionBy(\"crime_id\").orderBy(col(\"distance\").asc())\n",
    "\n",
    "df_ranked = df_with_distance.withColumn(\n",
    "    \"rank\",\n",
    "    row_number().over(window_spec)\n",
    ")\n",
    "\n",
    "closest_crimes_df = df_ranked.filter(col(\"rank\") == 1)\n",
    "\n",
    "# Τελικός υπολογισμός: Αριθμός εγκλημάτων και μέση απόσταση ανά τμήμα\n",
    "final_result_df = closest_crimes_df.groupBy(col(\"division\")).agg(\n",
    "    count(\"*\").alias(\"#\"),\n",
    "    round(avg(col(\"distance\")), 3).alias(\"average_distance\")\n",
    ")\n",
    "\n",
    "final_sorted_result = final_result_df.orderBy(col(\"#\").desc())\n",
    "\n",
    "print(\"\\n--- Spark Explain Output ---\")\n",
    "final_sorted_result.explain(True)\n",
    "\n",
    "final_sorted_result.show(final_sorted_result.count(), truncate=False)\n",
    "\n",
    "end_time = time.time()\n",
    "time = end_time - start_time\n",
    "print(f\"Execution time : {time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21920024-3670-47cb-a569-eede44d50c89",
   "metadata": {},
   "source": [
    "4 cores, 8GB memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94316543-794f-47ae-8a67-0d674f681388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1535</td><td>application_1765289937462_1521</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1521/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-48.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1521_01_000002/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe8f72f7fe4d4415946f29c6511ed580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '2', 'spark.executor.memory': '8g', 'spark.executor.cores': '4'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1512</td><td>application_1765289937462_1498</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1498/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-251.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1498_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1517</td><td>application_1765289937462_1503</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1503/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-154.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1503_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1522</td><td>application_1765289937462_1508</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1508/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-30.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1508_01_000002/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1531</td><td>application_1765289937462_1517</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1517/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-55.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1517_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1532</td><td>application_1765289937462_1518</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1518/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-156.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1518_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1534</td><td>application_1765289937462_1520</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1520/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-103.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1520_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1535</td><td>application_1765289937462_1521</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1521/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-48.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1521_01_000002/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\":{\n",
    "        \"spark.executor.instances\": \"2\",\n",
    "        \"spark.executor.memory\": \"8g\",\n",
    "        \"spark.executor.cores\": \"4\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02dd13af-0fd0-40d6-b14e-0de7039fc515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5aa1f36f03440b0a089e5205e3130d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total lines: 3138128"
     ]
    }
   ],
   "source": [
    "# Paths for csv\n",
    "fcrime_2010_2019 = \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2010_2019.csv\"\n",
    "fcrime_2020_present = \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\"\n",
    "fstations = \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Police_Stations.csv\"\n",
    "fincome = \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_income_2021.csv\"\n",
    "fcodes = \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/RE_codes.csv\"\n",
    "\n",
    "# Paths for GeoJSON\n",
    "fgeo = \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Census_Blocks_2020.geojson\"\n",
    "fgeofields = \"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Census_Blocks_2020_fields.csv\"\n",
    "\n",
    "# Imports again\n",
    "\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, FloatType, DateType\n",
    "from pyspark.sql.functions import year, when, count, sum, col, row_number, to_timestamp, regexp_replace, lit, expr, avg, round\n",
    "from pyspark.sql.window import Window\n",
    "from sedona.spark import *\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# Crimes table\n",
    "\n",
    "crimes_schema = StructType([\n",
    "    StructField(\"DR_NO\", StringType()),\n",
    "    StructField(\"DateRptd\", DateType()),\n",
    "    StructField(\"DATEOCC\", StringType()),\n",
    "    StructField(\"TIMEOCC\", StringType()),\n",
    "    StructField(\"AREA\", StringType()),\n",
    "    StructField(\"AREANAME\", StringType()),\n",
    "    StructField(\"RptDistNo\", StringType()),\n",
    "    StructField(\"Part\", IntegerType()),\n",
    "    StructField(\"CrmCd\", StringType()),\n",
    "    StructField(\"Crm Cd Desc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"Vict Age\", StringType()),\n",
    "    StructField(\"VictSex\", StringType()),\n",
    "    StructField(\"VictDescent\", StringType()),\n",
    "    StructField(\"PremisCd\", StringType()),\n",
    "    StructField(\"PremisDesc\", StringType()),\n",
    "    StructField(\"WeaponUsedCd\", StringType()),\n",
    "    StructField(\"WeaponDesc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"Status Desc\", StringType()),\n",
    "    StructField(\"CrmCd1\", StringType()),\n",
    "    StructField(\"CrmCd2\", StringType()),\n",
    "    StructField(\"CrmCd3\", StringType()),\n",
    "    StructField(\"CrmCd4\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"CrossStreet\", StringType()),\n",
    "    StructField(\"LAT\", FloatType()),\n",
    "    StructField(\"LON\", FloatType()),\n",
    "])\n",
    "\n",
    "crimes_df_10_19 = spark.read.csv(fcrime_2010_2019, header=True, schema=crimes_schema, dateFormat='MM/dd/yyyy hh:mm:ss a')\n",
    "crimes_df_20_present = spark.read.csv(fcrime_2020_present, header=True, schema=crimes_schema, dateFormat='MM/dd/yyyy hh:mm:ss a')\n",
    "\n",
    "crimes_df_all = crimes_df_10_19.union(crimes_df_20_present)\n",
    "print(f\"total lines: {crimes_df_all.count()}\")\n",
    "\n",
    "stations_schema = StructType([\n",
    "    StructField(\"X\", FloatType()),\n",
    "    StructField(\"Y\", FloatType()),\n",
    "    StructField(\"FID\", IntegerType()),\n",
    "    StructField(\"DIVISION\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"PREC\", IntegerType())\n",
    "])\n",
    "\n",
    "stations_df = spark.read.csv(fstations, header=True, schema=stations_schema)\n",
    "\n",
    "sedona = SedonaContext.create(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5bc4a70-073e-4f18-8489-281ad7eeb012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46a77136befb48e7a6fd6342311db8b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Spark Explain Output ---\n",
      "== Parsed Logical Plan ==\n",
      "'Sort ['# DESC NULLS LAST], true\n",
      "+- Aggregate [division#279], [division#279, count(1) AS ##419L, round(avg(distance#314), 3) AS average_distance#421]\n",
      "   +- Filter (rank#349 = 1)\n",
      "      +- Project [DR_NO#0, DateRptd#1, DATEOCC#2, TIMEOCC#3, AREA#4, AREANAME#5, RptDistNo#6, Part#7, CrmCd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, VictSex#12, VictDescent#13, PremisCd#14, PremisDesc#15, WeaponUsedCd#16, WeaponDesc#17, Status#18, Status Desc#19, CrmCd1#20, CrmCd2#21, CrmCd3#22, CrmCd4#23, ... 10 more fields]\n",
      "         +- Project [DR_NO#0, DateRptd#1, DATEOCC#2, TIMEOCC#3, AREA#4, AREANAME#5, RptDistNo#6, Part#7, CrmCd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, VictSex#12, VictDescent#13, PremisCd#14, PremisDesc#15, WeaponUsedCd#16, WeaponDesc#17, Status#18, Status Desc#19, CrmCd1#20, CrmCd2#21, CrmCd3#22, CrmCd4#23, ... 11 more fields]\n",
      "            +- Window [row_number() windowspecdefinition(crime_id#209, distance#314 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#349], [crime_id#209], [distance#314 ASC NULLS FIRST]\n",
      "               +- Project [DR_NO#0, DateRptd#1, DATEOCC#2, TIMEOCC#3, AREA#4, AREANAME#5, RptDistNo#6, Part#7, CrmCd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, VictSex#12, VictDescent#13, PremisCd#14, PremisDesc#15, WeaponUsedCd#16, WeaponDesc#17, Status#18, Status Desc#19, CrmCd1#20, CrmCd2#21, CrmCd3#22, CrmCd4#23, ... 9 more fields]\n",
      "                  +- Project [DR_NO#0, DateRptd#1, DATEOCC#2, TIMEOCC#3, AREA#4, AREANAME#5, RptDistNo#6, Part#7, CrmCd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, VictSex#12, VictDescent#13, PremisCd#14, PremisDesc#15, WeaponUsedCd#16, WeaponDesc#17, Status#18, Status Desc#19, CrmCd1#20, CrmCd2#21, CrmCd3#22, CrmCd4#23, ... 9 more fields]\n",
      "                     +- Join Cross\n",
      "                        :- Project [DR_NO#0, DateRptd#1, DATEOCC#2, TIMEOCC#3, AREA#4, AREANAME#5, RptDistNo#6, Part#7, CrmCd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, VictSex#12, VictDescent#13, PremisCd#14, PremisDesc#15, WeaponUsedCd#16, WeaponDesc#17, Status#18, Status Desc#19, CrmCd1#20, CrmCd2#21, CrmCd3#22, CrmCd4#23, ... 6 more fields]\n",
      "                        :  +- Filter NOT ((((cast(LAT#26 as double) = 0.0) AND (cast(LON#27 as double) = 0.0)) OR isnull(LAT#26)) OR isnull(LON#27))\n",
      "                        :     +- Project [DR_NO#0, DateRptd#1, DATEOCC#2, TIMEOCC#3, AREA#4, AREANAME#5, RptDistNo#6, Part#7, CrmCd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, VictSex#12, VictDescent#13, PremisCd#14, PremisDesc#15, WeaponUsedCd#16, WeaponDesc#17, Status#18, Status Desc#19, CrmCd1#20, CrmCd2#21, CrmCd3#22, CrmCd4#23, ... 5 more fields]\n",
      "                        :        +- Union false, false\n",
      "                        :           :- Relation [DR_NO#0,DateRptd#1,DATEOCC#2,TIMEOCC#3,AREA#4,AREANAME#5,RptDistNo#6,Part#7,CrmCd#8,Crm Cd Desc#9,Mocodes#10,Vict Age#11,VictSex#12,VictDescent#13,PremisCd#14,PremisDesc#15,WeaponUsedCd#16,WeaponDesc#17,Status#18,Status Desc#19,CrmCd1#20,CrmCd2#21,CrmCd3#22,CrmCd4#23,... 4 more fields] csv\n",
      "                        :           +- Relation [DR_NO#67,DateRptd#68,DATEOCC#69,TIMEOCC#70,AREA#71,AREANAME#72,RptDistNo#73,Part#74,CrmCd#75,Crm Cd Desc#76,Mocodes#77,Vict Age#78,VictSex#79,VictDescent#80,PremisCd#81,PremisDesc#82,WeaponUsedCd#83,WeaponDesc#84,Status#85,Status Desc#86,CrmCd1#87,CrmCd2#88,CrmCd3#89,CrmCd4#90,... 4 more fields] csv\n",
      "                        +- Project [DIVISION#187 AS division#279, pd_geom#270]\n",
      "                           +- Project [X#184, Y#185, FID#186, DIVISION#187, LOCATION#188, PREC#189,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS pd_geom#270]\n",
      "                              +- Relation [X#184,Y#185,FID#186,DIVISION#187,LOCATION#188,PREC#189] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "division: string, #: bigint, average_distance: double\n",
      "Sort [##419L DESC NULLS LAST], true\n",
      "+- Aggregate [division#279], [division#279, count(1) AS ##419L, round(avg(distance#314), 3) AS average_distance#421]\n",
      "   +- Filter (rank#349 = 1)\n",
      "      +- Project [DR_NO#0, DateRptd#1, DATEOCC#2, TIMEOCC#3, AREA#4, AREANAME#5, RptDistNo#6, Part#7, CrmCd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, VictSex#12, VictDescent#13, PremisCd#14, PremisDesc#15, WeaponUsedCd#16, WeaponDesc#17, Status#18, Status Desc#19, CrmCd1#20, CrmCd2#21, CrmCd3#22, CrmCd4#23, ... 10 more fields]\n",
      "         +- Project [DR_NO#0, DateRptd#1, DATEOCC#2, TIMEOCC#3, AREA#4, AREANAME#5, RptDistNo#6, Part#7, CrmCd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, VictSex#12, VictDescent#13, PremisCd#14, PremisDesc#15, WeaponUsedCd#16, WeaponDesc#17, Status#18, Status Desc#19, CrmCd1#20, CrmCd2#21, CrmCd3#22, CrmCd4#23, ... 11 more fields]\n",
      "            +- Window [row_number() windowspecdefinition(crime_id#209, distance#314 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#349], [crime_id#209], [distance#314 ASC NULLS FIRST]\n",
      "               +- Project [DR_NO#0, DateRptd#1, DATEOCC#2, TIMEOCC#3, AREA#4, AREANAME#5, RptDistNo#6, Part#7, CrmCd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, VictSex#12, VictDescent#13, PremisCd#14, PremisDesc#15, WeaponUsedCd#16, WeaponDesc#17, Status#18, Status Desc#19, CrmCd1#20, CrmCd2#21, CrmCd3#22, CrmCd4#23, ... 9 more fields]\n",
      "                  +- Project [DR_NO#0, DateRptd#1, DATEOCC#2, TIMEOCC#3, AREA#4, AREANAME#5, RptDistNo#6, Part#7, CrmCd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, VictSex#12, VictDescent#13, PremisCd#14, PremisDesc#15, WeaponUsedCd#16, WeaponDesc#17, Status#18, Status Desc#19, CrmCd1#20, CrmCd2#21, CrmCd3#22, CrmCd4#23, ... 9 more fields]\n",
      "                     +- Join Cross\n",
      "                        :- Project [DR_NO#0, DateRptd#1, DATEOCC#2, TIMEOCC#3, AREA#4, AREANAME#5, RptDistNo#6, Part#7, CrmCd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, VictSex#12, VictDescent#13, PremisCd#14, PremisDesc#15, WeaponUsedCd#16, WeaponDesc#17, Status#18, Status Desc#19, CrmCd1#20, CrmCd2#21, CrmCd3#22, CrmCd4#23, ... 6 more fields]\n",
      "                        :  +- Filter NOT ((((cast(LAT#26 as double) = 0.0) AND (cast(LON#27 as double) = 0.0)) OR isnull(LAT#26)) OR isnull(LON#27))\n",
      "                        :     +- Project [DR_NO#0, DateRptd#1, DATEOCC#2, TIMEOCC#3, AREA#4, AREANAME#5, RptDistNo#6, Part#7, CrmCd#8, Crm Cd Desc#9, Mocodes#10, Vict Age#11, VictSex#12, VictDescent#13, PremisCd#14, PremisDesc#15, WeaponUsedCd#16, WeaponDesc#17, Status#18, Status Desc#19, CrmCd1#20, CrmCd2#21, CrmCd3#22, CrmCd4#23, ... 5 more fields]\n",
      "                        :        +- Union false, false\n",
      "                        :           :- Relation [DR_NO#0,DateRptd#1,DATEOCC#2,TIMEOCC#3,AREA#4,AREANAME#5,RptDistNo#6,Part#7,CrmCd#8,Crm Cd Desc#9,Mocodes#10,Vict Age#11,VictSex#12,VictDescent#13,PremisCd#14,PremisDesc#15,WeaponUsedCd#16,WeaponDesc#17,Status#18,Status Desc#19,CrmCd1#20,CrmCd2#21,CrmCd3#22,CrmCd4#23,... 4 more fields] csv\n",
      "                        :           +- Relation [DR_NO#67,DateRptd#68,DATEOCC#69,TIMEOCC#70,AREA#71,AREANAME#72,RptDistNo#73,Part#74,CrmCd#75,Crm Cd Desc#76,Mocodes#77,Vict Age#78,VictSex#79,VictDescent#80,PremisCd#81,PremisDesc#82,WeaponUsedCd#83,WeaponDesc#84,Status#85,Status Desc#86,CrmCd1#87,CrmCd2#88,CrmCd3#89,CrmCd4#90,... 4 more fields] csv\n",
      "                        +- Project [DIVISION#187 AS division#279, pd_geom#270]\n",
      "                           +- Project [X#184, Y#185, FID#186, DIVISION#187, LOCATION#188, PREC#189,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS pd_geom#270]\n",
      "                              +- Relation [X#184,Y#185,FID#186,DIVISION#187,LOCATION#188,PREC#189] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [##419L DESC NULLS LAST], true\n",
      "+- Aggregate [division#279], [division#279, count(1) AS ##419L, round(avg(distance#314), 3) AS average_distance#421]\n",
      "   +- Project [division#279, distance#314]\n",
      "      +- Filter (rank#349 = 1)\n",
      "         +- Window [row_number() windowspecdefinition(crime_id#209, distance#314 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#349], [crime_id#209], [distance#314 ASC NULLS FIRST]\n",
      "            +- WindowGroupLimit [crime_id#209], [distance#314 ASC NULLS FIRST], row_number(), 1\n",
      "               +- Project [crime_id#209, division#279, ( **org.apache.spark.sql.sedona_sql.expressions.ST_DistanceSphere**   / 1000.0) AS distance#314]\n",
      "                  +- Join Cross\n",
      "                     :- Project [crime_id#209,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_geom#239]\n",
      "                     :  +- Filter (((NOT (LAT#26 = 0.0) OR NOT (LON#27 = 0.0)) AND isnotnull(LAT#26)) AND isnotnull(LON#27))\n",
      "                     :     +- Project [LAT#26, LON#27, uuid(Some(-1182618204453832925)) AS crime_id#209]\n",
      "                     :        +- Union false, false\n",
      "                     :           :- Project [LAT#26, LON#27]\n",
      "                     :           :  +- Relation [DR_NO#0,DateRptd#1,DATEOCC#2,TIMEOCC#3,AREA#4,AREANAME#5,RptDistNo#6,Part#7,CrmCd#8,Crm Cd Desc#9,Mocodes#10,Vict Age#11,VictSex#12,VictDescent#13,PremisCd#14,PremisDesc#15,WeaponUsedCd#16,WeaponDesc#17,Status#18,Status Desc#19,CrmCd1#20,CrmCd2#21,CrmCd3#22,CrmCd4#23,... 4 more fields] csv\n",
      "                     :           +- Project [LAT#93, LON#94]\n",
      "                     :              +- Relation [DR_NO#67,DateRptd#68,DATEOCC#69,TIMEOCC#70,AREA#71,AREANAME#72,RptDistNo#73,Part#74,CrmCd#75,Crm Cd Desc#76,Mocodes#77,Vict Age#78,VictSex#79,VictDescent#80,PremisCd#81,PremisDesc#82,WeaponUsedCd#83,WeaponDesc#84,Status#85,Status Desc#86,CrmCd1#87,CrmCd2#88,CrmCd3#89,CrmCd4#90,... 4 more fields] csv\n",
      "                     +- Project [DIVISION#187 AS division#279,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS pd_geom#270]\n",
      "                        +- Relation [X#184,Y#185,FID#186,DIVISION#187,LOCATION#188,PREC#189] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [##419L DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(##419L DESC NULLS LAST, 1000), ENSURE_REQUIREMENTS, [plan_id=166]\n",
      "      +- HashAggregate(keys=[division#279], functions=[count(1), avg(distance#314)], output=[division#279, ##419L, average_distance#421], schema specialized)\n",
      "         +- Exchange hashpartitioning(division#279, 1000), ENSURE_REQUIREMENTS, [plan_id=163]\n",
      "            +- HashAggregate(keys=[division#279], functions=[partial_count(1), partial_avg(distance#314)], output=[division#279, count#426L, sum#429, count#430L], schema specialized)\n",
      "               +- Project [division#279, distance#314]\n",
      "                  +- Filter (rank#349 = 1)\n",
      "                     +- Window [row_number() windowspecdefinition(crime_id#209, distance#314 ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS rank#349], [crime_id#209], [distance#314 ASC NULLS FIRST]\n",
      "                        +- WindowGroupLimit [crime_id#209], [distance#314 ASC NULLS FIRST], row_number(), 1, Final\n",
      "                           +- Sort [crime_id#209 ASC NULLS FIRST, distance#314 ASC NULLS FIRST], false, 0\n",
      "                              +- Exchange hashpartitioning(crime_id#209, 1000), ENSURE_REQUIREMENTS, [plan_id=155]\n",
      "                                 +- WindowGroupLimit [crime_id#209], [distance#314 ASC NULLS FIRST], row_number(), 1, Partial\n",
      "                                    +- Sort [crime_id#209 ASC NULLS FIRST, distance#314 ASC NULLS FIRST], false, 0\n",
      "                                       +- Project [crime_id#209, division#279, ( **org.apache.spark.sql.sedona_sql.expressions.ST_DistanceSphere**   / 1000.0) AS distance#314]\n",
      "                                          +- BroadcastNestedLoopJoin BuildRight, Cross\n",
      "                                             :- Project [crime_id#209,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_geom#239]\n",
      "                                             :  +- Filter (((NOT (LAT#26 = 0.0) OR NOT (LON#27 = 0.0)) AND isnotnull(LAT#26)) AND isnotnull(LON#27))\n",
      "                                             :     +- Project [LAT#26, LON#27, uuid(Some(-1182618204453832925)) AS crime_id#209]\n",
      "                                             :        +- Union\n",
      "                                             :           :- FileScan csv [LAT#26,LON#27] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<LAT:float,LON:float>\n",
      "                                             :           +- FileScan csv [LAT#93,LON#94] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<LAT:float,LON:float>\n",
      "                                             +- BroadcastExchange IdentityBroadcastMode, [plan_id=148]\n",
      "                                                +- Project [DIVISION#187 AS division#279,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS pd_geom#270]\n",
      "                                                   +- FileScan csv [X#184,Y#185,DIVISION#187] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_P..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<X:float,Y:float,DIVISION:string>\n",
      "\n",
      "+----------------+------+----------------+\n",
      "|division        |#     |average_distance|\n",
      "+----------------+------+----------------+\n",
      "|HOLLYWOOD       |225515|2.076           |\n",
      "|VAN NUYS        |211130|2.953           |\n",
      "|SOUTHWEST       |189572|2.191           |\n",
      "|WILSHIRE        |187054|2.593           |\n",
      "|77TH STREET     |172558|1.717           |\n",
      "|OLYMPIC         |172353|1.725           |\n",
      "|NORTH HOLLYWOOD |168655|2.643           |\n",
      "|PACIFIC         |162514|3.853           |\n",
      "|CENTRAL         |154952|0.993           |\n",
      "|SOUTHEAST       |153746|2.422           |\n",
      "|RAMPART         |153690|1.535           |\n",
      "|TOPANGA         |141070|3.298           |\n",
      "|WEST VALLEY     |139820|3.039           |\n",
      "|FOOTHILL        |135381|4.251           |\n",
      "|HARBOR          |127370|3.702           |\n",
      "|HOLLENBECK      |116558|2.677           |\n",
      "|WEST LOS ANGELES|116308|2.79            |\n",
      "|NEWTON          |111628|1.635           |\n",
      "|NORTHEAST       |108549|3.623           |\n",
      "|MISSION         |105331|3.685           |\n",
      "|DEVONSHIRE      |81226 |2.824           |\n",
      "+----------------+------+----------------+\n",
      "\n",
      "Execution time : 29.2317 seconds"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Δημιουργία ID για το partition (χρήσιμο για το window function)\n",
    "crimes_df_prep = crimes_df_all.withColumn(\"crime_id\", expr(\"uuid()\"))\n",
    "\n",
    "crimes_filtered = crimes_df_prep.filter(\n",
    "    ~((col(\"LAT\") == 0.0) & (col(\"LON\") == 0.0) | col(\"LAT\").isNull() | col(\"LON\").isNull())\n",
    ")\n",
    "\n",
    "# Μετατροπή των συντεταγμένων σε Sedona Point Geometry\n",
    "crimes_with_geom = crimes_filtered.withColumn(\n",
    "    \"crime_geom\",\n",
    "    ST_Point(col(\"LON\"), col(\"LAT\"))\n",
    ")\n",
    "\n",
    "pd_with_geom = stations_df.withColumn(\n",
    "    \"pd_geom\",\n",
    "    ST_Point(col(\"X\"), col(\"Y\"))\n",
    ").select(\n",
    "    col(\"DIVISION\").alias(\"division\"),\n",
    "    col(\"pd_geom\")\n",
    ")\n",
    "\n",
    "# Κάθε έγκλημα συνδέεται με κάθε αστυνομικό τμήμα\n",
    "cross_joined_df = crimes_with_geom.crossJoin(pd_with_geom)\n",
    "\n",
    "# Υπολογισμός της απόστασης \n",
    "df_with_distance = cross_joined_df.withColumn(\n",
    "    \"distance\",\n",
    "    (ST_DistanceSphere(col(\"crime_geom\"), col(\"pd_geom\"))/1000)\n",
    ")\n",
    "\n",
    "# Διαμέριση ανά crime_id και ταξινόμηση ανά distance\n",
    "window_spec = Window.partitionBy(\"crime_id\").orderBy(col(\"distance\").asc())\n",
    "\n",
    "df_ranked = df_with_distance.withColumn(\n",
    "    \"rank\",\n",
    "    row_number().over(window_spec)\n",
    ")\n",
    "\n",
    "closest_crimes_df = df_ranked.filter(col(\"rank\") == 1)\n",
    "\n",
    "# Τελικός υπολογισμός: Αριθμός εγκλημάτων και μέση απόσταση ανά τμήμα\n",
    "final_result_df = closest_crimes_df.groupBy(col(\"division\")).agg(\n",
    "    count(\"*\").alias(\"#\"),\n",
    "    round(avg(col(\"distance\")), 3).alias(\"average_distance\")\n",
    ")\n",
    "\n",
    "final_sorted_result = final_result_df.orderBy(col(\"#\").desc())\n",
    "\n",
    "print(\"\\n--- Spark Explain Output ---\")\n",
    "final_sorted_result.explain(True)\n",
    "\n",
    "final_sorted_result.show(final_sorted_result.count(), truncate=False)\n",
    "\n",
    "end_time = time.time()\n",
    "time = end_time - start_time\n",
    "print(f\"Execution time : {time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d166d2f8-10d1-4fb1-8cf6-2cef108891d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1563</td><td>application_1765289937462_1549</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1549/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-250.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1549_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1264311dc48041faa2481ad4599515f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "339722e2a8bc45de8bdc1e30f9b27237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sedona.spark import *\n",
    "\n",
    "spark = spark.newSession().builder \\\n",
    "    .appName(\"Query 5 \") \\\n",
    "    .config('spark.executor.instances', '2') \\\n",
    "    .config('spark.executor.cores', '8') \\\n",
    "    .config('spark.executor.memory', '4g') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Initialize SedonaContext\n",
    "sedona = SedonaContext.create(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6293787-d0a3-44e2-b784-306c1d0bbc1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c74e3909c5e2409f8648d668900dad75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    StructField, StructType, StringType, IntegerType, \n",
    "    FloatType, DoubleType, DateType, TimestampType  \n",
    ")\n",
    "from pyspark.sql.functions import (\n",
    "    year, when, count, sum, col, row_number, \n",
    "    to_timestamp, regexp_replace, to_date, expr,avg,broadcast,first    \n",
    ")\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, sum, count, expr, coalesce, lit, when,explode,collect_set\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import  to_date, year\n",
    "import time\n",
    "from pyspark.sql.functions import col, regexp_replace, trim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3391cbd4-08f9-4336-a191-cc4773cfe7e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4d56b173ec443cbaf91b064c70b1830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fgeo=\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Census_Blocks_2020.geojson\"\n",
    "fincome=\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_income_2021.csv\"\n",
    "fcrime=\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ba5cb2d-9467-4b3d-93a6-836a7036aa41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca06cfd0a7024f578101d371f660d76f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "crimes_schema = StructType([\n",
    "    StructField(\"DR_NO\", StringType()),\n",
    "    StructField(\"DateRptd\", TimestampType()),\n",
    "    StructField(\"DATEOCC\", TimestampType()),\n",
    "    StructField(\"TIMEOCC\", StringType()),\n",
    "    StructField(\"AREA\", StringType()),\n",
    "    StructField(\"AREANAME\", StringType()),\n",
    "    StructField(\"RptDistNo\", StringType()),\n",
    "    StructField(\"Part\", IntegerType()),\n",
    "    StructField(\"CrmCd\", StringType()),\n",
    "    StructField(\"Crm Cd Desc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"Vict Age\", StringType()),\n",
    "    StructField(\"VictSex\", StringType()),\n",
    "    StructField(\"VictDescent\", StringType()),\n",
    "    StructField(\"PremisCd\", StringType()),\n",
    "    StructField(\"PremisDesc\", StringType()),\n",
    "    StructField(\"WeaponUsedCd\", StringType()),\n",
    "    StructField(\"WeaponDesc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"Status Desc\", StringType()),\n",
    "    StructField(\"CrmCd1\", StringType()),\n",
    "    StructField(\"CrmCd2\", StringType()),\n",
    "    StructField(\"CrmCd3\", StringType()),\n",
    "    StructField(\"CrmCd4\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"CrossStreet\", StringType()),\n",
    "    StructField(\"LAT\", DoubleType()),\n",
    "    StructField(\"LON\", DoubleType()),\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1cde0914-5321-4d3d-aa74-234c7abf4339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82f6dacceeaf40f49772554e8113c3f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total correlation: 0.15027668303196612\n",
      "Top 10 richest COMM correlation: -0.3735207290423716\n",
      "Bottom 10 poorest COMM correlation: 0.018338409553787367\n",
      "Elapsed time: 58.88 seconds"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "#Prepare the geojson file\n",
    "blocks_df = sedona.read.format('geojson') \\\n",
    "    .option('multiLine','true').load(fgeo) \\\n",
    "    .selectExpr('explode(features) as features') \\\n",
    "    .select('features.*')\n",
    "\n",
    "flattened_df = blocks_df.select(\n",
    "    [col(f'properties.{col_name}').alias(col_name) for col_name in \\\n",
    "    blocks_df.schema['properties'].dataType.fieldNames()] + ['geometry']) \\\n",
    "    .drop('properties').drop('type')\n",
    "\n",
    "la_comm_df = flattened_df.filter(col(\"CITY\") == \"Los Angeles\") \\\n",
    "    .groupBy(\"COMM\") \\\n",
    "    .agg(\n",
    "        expr(\"ST_Union_Aggr(geometry)\").alias(\"geometry\"),\n",
    "        first(\"POP20\").alias(\"TotalPopulation\")\n",
    "    ) \\\n",
    "    .filter(\n",
    "        (col(\"COMM\").isNotNull()) &\n",
    "        (col(\"COMM\") != \"\") &\n",
    "        (col(\"COMM\") != \"NULL\") &\n",
    "        (col(\"TotalPopulation\") > 0)\n",
    "    )\n",
    "\n",
    "\n",
    "zip_codes_comm = flattened_df.filter(col(\"CITY\") == \"Los Angeles\") \\\n",
    "    .select(\"ZCTA20\", \"COMM\") \\\n",
    "    .filter(\n",
    "        col(\"ZCTA20\").isNotNull() &\n",
    "        (col(\"ZCTA20\") != \"\") &\n",
    "        (col(\"ZCTA20\") != \"NULL\")\n",
    "    ) \\\n",
    "    .dropDuplicates()\n",
    "\n",
    "\n",
    "\n",
    "# \n",
    "income_df = spark.read.csv(\n",
    "    fincome,\n",
    "    header=True,\n",
    "    sep=';',\n",
    "    quote='\"',\n",
    "    escape='\"',\n",
    "    inferSchema=False\n",
    ")\n",
    "\n",
    "# Trim the Zip Code column and rename\n",
    "income_df = income_df.withColumn(\"ZipCode\", trim(col(\"Zip Code\")))\n",
    "\n",
    "# 3️ Keep only rows where Community mentions Los Angeles\n",
    "income_df = income_df.filter(col(\"Community\").rlike(\"(?i)Los Angeles\"))\n",
    "\n",
    "# Clean Income column\n",
    "income_df = income_df.withColumn(\n",
    "    \"Income\",\n",
    "    regexp_replace(col(\"Estimated Median Income\"), r\"[$,]\", \"\").cast(\"float\")\n",
    ")\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "income_df = income_df.filter(\n",
    "    col(\"ZipCode\").isNotNull() &       # ZipCode not NULL\n",
    "    (col(\"ZipCode\") != \"\") &           # ZipCode not empty\n",
    "    (col(\"ZipCode\") != \"NULL\") &       # ZipCode not the string \"NULL\"\n",
    "    col(\"Income\").isNotNull()          # Income not NULL\n",
    ")\n",
    "\n",
    "#  Keep only ZipCode and Income column\n",
    "income_df = income_df.select(\"ZipCode\", \"Income\")\n",
    "\n",
    "income_with_comm = income_df.join(\n",
    "    zip_codes_comm,\n",
    "    income_df.ZipCode == zip_codes_comm.ZCTA20,\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "#in each COMM there are nany ZipCodes with different Income so find average income per comm\n",
    "income_per_comm = income_with_comm.groupBy(\"COMM\") \\\n",
    "    .agg(F.avg(\"Income\").alias(\"avg_income\"))\n",
    "\n",
    "#Crimes\n",
    "crimes_df = spark.read.csv(\n",
    "    fcrime,\n",
    "    header=True,\n",
    "    schema=crimes_schema,\n",
    "    timestampFormat='yyyy MMM dd hh:mm:ss a',\n",
    "    quote='\"',\n",
    "    escape='\"'\n",
    ")\n",
    "\n",
    "#  Cast Lat/Lon to Double  and Filter Zeros\n",
    "crimes_df = crimes_df.filter((col('LAT') != 0.0) & (col('LON') != 0.0))\n",
    "\n",
    "# Extract Year\n",
    "crimes_df = crimes_df.withColumn(\"year\", year(col(\"DATEOCC\")))\n",
    "\n",
    "# Filter for years 2020 and 2021\n",
    "crimes_df = crimes_df.filter(col(\"year\").isin([2020, 2021]))\n",
    "\n",
    "#  Create Geometry (Requires Apache Sedona)\n",
    "#  ST_Point takes (Longitude, Latitude) -> (X, Y)\n",
    "crimes_geo_df = crimes_df.withColumn(\"crime_point\", expr(\"ST_Point(LON, LAT)\"))\n",
    "\n",
    "# Count crimes per community per year,schema with comm,year,pop,crimecounts\n",
    "crime_per_comm_year = (\n",
    "    crimes_geo_df\n",
    "    .join(\n",
    "        la_comm_df.select(\"COMM\", \"geometry\", \"TotalPopulation\"),\n",
    "        expr(\"ST_Within(crime_point, geometry)\"),\n",
    "        \"inner\"\n",
    "    )\n",
    "    .groupBy(\"COMM\", \"year\", \"TotalPopulation\")\n",
    "    .agg(F.count(\"*\").alias(\"annual_crime_count\"))\n",
    ")\n",
    "#to avoid probems with nans set 0 for the comms where no crimes happened\n",
    "crime_per_comm_year = crime_per_comm_year.fillna({\"annual_crime_count\": 0})\n",
    "\n",
    "\n",
    "# Crime rate per person per year\n",
    "crime_rate_ = crime_per_comm_year.withColumn(\n",
    "    \"crime_rate_per_person\",\n",
    "    col(\"annual_crime_count\") / col(\"TotalPopulation\")\n",
    ")\n",
    "\n",
    "# Annual average crime rate per person\n",
    "crime_per_comm = (\n",
    "    crime_rate_\n",
    "    .groupBy(\"COMM\")   #crime rate per person for each community\n",
    "    .agg(\n",
    "        F.avg(\"crime_rate_per_person\").alias(\"annual_avg_crime_rate_per_person\")  #average for all the years 2020,2021\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "final_df = (\n",
    "    crime_per_comm   # COMM, annual_avg_crime_rate_per_person\n",
    "    .join(\n",
    "        income_per_comm,   # COMM, avg_income\n",
    "        on=\"COMM\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    ")\n",
    "# All areas\n",
    "total_corr = final_df.stat.corr(\"avg_income\", \"annual_avg_crime_rate_per_person\")\n",
    "\n",
    "# Top 10 richest areas\n",
    "top10 = final_df.orderBy(col(\"avg_income\").desc()).limit(10)\n",
    "top10_corr = top10.stat.corr(\"avg_income\", \"annual_avg_crime_rate_per_person\")\n",
    "\n",
    "# Bottom 10 poorest areas\n",
    "bottom10 = final_df.orderBy(col(\"avg_income\").asc()).limit(10)\n",
    "bottom10_corr = bottom10.stat.corr(\"avg_income\", \"annual_avg_crime_rate_per_person\")\n",
    "\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "\n",
    "# Show results\n",
    "\n",
    "print(\"Total correlation:\", total_corr)\n",
    "print(\"Top 10 richest COMM correlation:\", top10_corr)\n",
    "print(\"Bottom 10 poorest COMM correlation:\", bottom10_corr)\n",
    "print(f\"Elapsed time: {elapsed:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e02de292-0707-4891-ab20-4f052f3c2854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8289e397c04ccda4ff2ce7ed6a1994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['COMM], ['COMM, avg('Income) AS avg_income#2259]\n",
      "+- Join Inner, (ZipCode#2232 = ZCTA20#2037)\n",
      "   :- Project [ZipCode#2232, Income#2238]\n",
      "   :  +- Filter (((isnotnull(ZipCode#2232) AND NOT (ZipCode#2232 = )) AND NOT (ZipCode#2232 = NULL)) AND isnotnull(Income#2238))\n",
      "   :     +- Project [Zip Code#2226, Community#2227, Estimated Median Income#2228, ZipCode#2232, cast(regexp_replace(Estimated Median Income#2228, [$,], , 1) as float) AS Income#2238]\n",
      "   :        +- Filter RLIKE(Community#2227, (?i)Los Angeles)\n",
      "   :           +- Project [Zip Code#2226, Community#2227, Estimated Median Income#2228, trim(Zip Code#2226, None) AS ZipCode#2232]\n",
      "   :              +- Relation [Zip Code#2226,Community#2227,Estimated Median Income#2228] csv\n",
      "   +- Deduplicate [ZCTA20#2037, COMM#2017]\n",
      "      +- Filter ((isnotnull(ZCTA20#2037) AND NOT (ZCTA20#2037 = )) AND NOT (ZCTA20#2037 = NULL))\n",
      "         +- Project [ZCTA20#2037, COMM#2017]\n",
      "            +- Filter (CITY#2013 = Los Angeles)\n",
      "               +- Project [properties#2004.BG20 AS BG20#2009, properties#2004.BG20FIP_CURRENT AS BG20FIP_CURRENT#2010, properties#2004.BGFIP20 AS BGFIP20#2011, properties#2004.CB20 AS CB20#2012, properties#2004.CITY AS CITY#2013, properties#2004.CITYCOMM AS CITYCOMM#2014, properties#2004.CITYCOMM_CURRENT AS CITYCOMM_CURRENT#2015, properties#2004.CITY_CURRENT AS CITY_CURRENT#2016, properties#2004.COMM AS COMM#2017, properties#2004.COMM_CURRENT AS COMM_CURRENT#2018, properties#2004.COUNTY AS COUNTY#2019, properties#2004.CT20 AS CT20#2020, properties#2004.CTCB20 AS CTCB20#2021, properties#2004.FEAT_TYPE AS FEAT_TYPE#2022, properties#2004.FIP20 AS FIP20#2023, properties#2004.FIP_CURRENT AS FIP_CURRENT#2024, properties#2004.HD22 AS HD22#2025L, properties#2004.HD_NAME AS HD_NAME#2026, properties#2004.HOUSING20 AS HOUSING20#2027L, properties#2004.OBJECTID AS OBJECTID#2028L, properties#2004.POP20 AS POP20#2029L, properties#2004.SPA22 AS SPA22#2030L, properties#2004.SPA_NAME AS SPA_NAME#2031, properties#2004.SUP21 AS SUP21#2032, ... 6 more fields]\n",
      "                  +- Project [features#2000.geometry AS geometry#2003, features#2000.properties AS properties#2004, features#2000.type AS type#2005]\n",
      "                     +- Project [features#2000]\n",
      "                        +- Generate explode(features#1992), false, [features#2000]\n",
      "                           +- Relation [crs#1991,features#1992,name#1993,type#1994] geojson\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "COMM: string, avg_income: double\n",
      "Aggregate [COMM#2017], [COMM#2017, avg(Income#2238) AS avg_income#2259]\n",
      "+- Join Inner, (ZipCode#2232 = ZCTA20#2037)\n",
      "   :- Project [ZipCode#2232, Income#2238]\n",
      "   :  +- Filter (((isnotnull(ZipCode#2232) AND NOT (ZipCode#2232 = )) AND NOT (ZipCode#2232 = NULL)) AND isnotnull(Income#2238))\n",
      "   :     +- Project [Zip Code#2226, Community#2227, Estimated Median Income#2228, ZipCode#2232, cast(regexp_replace(Estimated Median Income#2228, [$,], , 1) as float) AS Income#2238]\n",
      "   :        +- Filter RLIKE(Community#2227, (?i)Los Angeles)\n",
      "   :           +- Project [Zip Code#2226, Community#2227, Estimated Median Income#2228, trim(Zip Code#2226, None) AS ZipCode#2232]\n",
      "   :              +- Relation [Zip Code#2226,Community#2227,Estimated Median Income#2228] csv\n",
      "   +- Deduplicate [ZCTA20#2037, COMM#2017]\n",
      "      +- Filter ((isnotnull(ZCTA20#2037) AND NOT (ZCTA20#2037 = )) AND NOT (ZCTA20#2037 = NULL))\n",
      "         +- Project [ZCTA20#2037, COMM#2017]\n",
      "            +- Filter (CITY#2013 = Los Angeles)\n",
      "               +- Project [properties#2004.BG20 AS BG20#2009, properties#2004.BG20FIP_CURRENT AS BG20FIP_CURRENT#2010, properties#2004.BGFIP20 AS BGFIP20#2011, properties#2004.CB20 AS CB20#2012, properties#2004.CITY AS CITY#2013, properties#2004.CITYCOMM AS CITYCOMM#2014, properties#2004.CITYCOMM_CURRENT AS CITYCOMM_CURRENT#2015, properties#2004.CITY_CURRENT AS CITY_CURRENT#2016, properties#2004.COMM AS COMM#2017, properties#2004.COMM_CURRENT AS COMM_CURRENT#2018, properties#2004.COUNTY AS COUNTY#2019, properties#2004.CT20 AS CT20#2020, properties#2004.CTCB20 AS CTCB20#2021, properties#2004.FEAT_TYPE AS FEAT_TYPE#2022, properties#2004.FIP20 AS FIP20#2023, properties#2004.FIP_CURRENT AS FIP_CURRENT#2024, properties#2004.HD22 AS HD22#2025L, properties#2004.HD_NAME AS HD_NAME#2026, properties#2004.HOUSING20 AS HOUSING20#2027L, properties#2004.OBJECTID AS OBJECTID#2028L, properties#2004.POP20 AS POP20#2029L, properties#2004.SPA22 AS SPA22#2030L, properties#2004.SPA_NAME AS SPA_NAME#2031, properties#2004.SUP21 AS SUP21#2032, ... 6 more fields]\n",
      "                  +- Project [features#2000.geometry AS geometry#2003, features#2000.properties AS properties#2004, features#2000.type AS type#2005]\n",
      "                     +- Project [features#2000]\n",
      "                        +- Generate explode(features#1992), false, [features#2000]\n",
      "                           +- Relation [crs#1991,features#1992,name#1993,type#1994] geojson\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [COMM#2017], [COMM#2017, avg(Income#2238) AS avg_income#2259]\n",
      "+- Project [Income#2238, COMM#2017]\n",
      "   +- Join Inner, (ZipCode#2232 = ZCTA20#2037)\n",
      "      :- Project [trim(Zip Code#2226, None) AS ZipCode#2232, cast(regexp_replace(Estimated Median Income#2228, [$,], , 1) as float) AS Income#2238]\n",
      "      :  +- Filter ((isnotnull(Community#2227) AND isnotnull(Estimated Median Income#2228)) AND (RLIKE(Community#2227, (?i)Los Angeles) AND (((isnotnull(trim(Zip Code#2226, None)) AND NOT (trim(Zip Code#2226, None) = )) AND NOT (trim(Zip Code#2226, None) = NULL)) AND isnotnull(cast(regexp_replace(Estimated Median Income#2228, [$,], , 1) as float)))))\n",
      "      :     +- Relation [Zip Code#2226,Community#2227,Estimated Median Income#2228] csv\n",
      "      +- Aggregate [ZCTA20#2037, COMM#2017], [ZCTA20#2037, COMM#2017]\n",
      "         +- Project [features#2000.properties.ZCTA20 AS ZCTA20#2037, features#2000.properties.COMM AS COMM#2017]\n",
      "            +- Filter (isnotnull(features#2000.properties.CITY) AND ((features#2000.properties.CITY = Los Angeles) AND ((isnotnull(features#2000.properties.ZCTA20) AND NOT (features#2000.properties.ZCTA20 = )) AND NOT (features#2000.properties.ZCTA20 = NULL))))\n",
      "               +- Generate explode(features#1992), [0], false, [features#2000]\n",
      "                  +- Project [features#1992]\n",
      "                     +- Filter ((size(features#1992, true) > 0) AND isnotnull(features#1992))\n",
      "                        +- Relation [crs#1991,features#1992,name#1993,type#1994] geojson\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[COMM#2017], functions=[avg(Income#2238)], output=[COMM#2017, avg_income#2259], schema specialized)\n",
      "   +- Exchange hashpartitioning(COMM#2017, 1000), ENSURE_REQUIREMENTS, [plan_id=7841]\n",
      "      +- HashAggregate(keys=[COMM#2017], functions=[partial_avg(Income#2238)], output=[COMM#2017, sum#2690, count#2691L], schema specialized)\n",
      "         +- Project [Income#2238, COMM#2017]\n",
      "            +- BroadcastHashJoin [ZipCode#2232], [ZCTA20#2037], Inner, BuildLeft, false\n",
      "               :- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=7836]\n",
      "               :  +- Project [trim(Zip Code#2226, None) AS ZipCode#2232, cast(regexp_replace(Estimated Median Income#2228, [$,], , 1) as float) AS Income#2238]\n",
      "               :     +- Filter ((((((isnotnull(Community#2227) AND isnotnull(Estimated Median Income#2228)) AND RLIKE(Community#2227, (?i)Los Angeles)) AND isnotnull(trim(Zip Code#2226, None))) AND NOT (trim(Zip Code#2226, None) = )) AND NOT (trim(Zip Code#2226, None) = NULL)) AND isnotnull(cast(regexp_replace(Estimated Median Income#2228, [$,], , 1) as float)))\n",
      "               :        +- FileScan csv [Zip Code#2226,Community#2227,Estimated Median Income#2228] Batched: false, DataFilters: [isnotnull(Community#2227), isnotnull(Estimated Median Income#2228), RLIKE(Community#2227, (?i)Lo..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_i..., PartitionFilters: [], PushedFilters: [IsNotNull(Community), IsNotNull(Estimated Median Income)], ReadSchema: struct<Zip Code:string,Community:string,Estimated Median Income:string>\n",
      "               +- HashAggregate(keys=[ZCTA20#2037, COMM#2017], functions=[], output=[ZCTA20#2037, COMM#2017], schema specialized)\n",
      "                  +- Exchange hashpartitioning(ZCTA20#2037, COMM#2017, 1000), ENSURE_REQUIREMENTS, [plan_id=7833]\n",
      "                     +- HashAggregate(keys=[ZCTA20#2037, COMM#2017], functions=[], output=[ZCTA20#2037, COMM#2017], schema specialized)\n",
      "                        +- Project [features#2000.properties.ZCTA20 AS ZCTA20#2037, features#2000.properties.COMM AS COMM#2017]\n",
      "                           +- Filter (isnotnull(features#2000.properties.CITY) AND ((features#2000.properties.CITY = Los Angeles) AND ((isnotnull(features#2000.properties.ZCTA20) AND NOT (features#2000.properties.ZCTA20 = )) AND NOT (features#2000.properties.ZCTA20 = NULL))))\n",
      "                              +- Generate explode(features#1992), false, [features#2000]\n",
      "                                 +- Filter ((size(features#1992, true) > 0) AND isnotnull(features#1992))\n",
      "                                    +- FileScan geojson [features#1992] Batched: false, DataFilters: [(size(features#1992, true) > 0), isnotnull(features#1992)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG20:string,BG20FIP_CURRENT:string..."
     ]
    }
   ],
   "source": [
    "income_per_comm.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8373d093-d1f1-4a22-aa18-80a7a8e8ecb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c9bb9982d514f9e926277f85d1bbddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['COMM], ['COMM, avg('crime_rate_per_person) AS annual_avg_crime_rate_per_person#2509]\n",
      "+- Project [COMM#2017, year#2319, TotalPopulation#2189L, annual_crime_count#2492L, (cast(annual_crime_count#2492L as double) / cast(TotalPopulation#2189L as double)) AS crime_rate_per_person#2497]\n",
      "   +- Project [COMM#2017, year#2319, TotalPopulation#2189L, coalesce(annual_crime_count#2483L, cast(0 as bigint)) AS annual_crime_count#2492L]\n",
      "      +- Aggregate [COMM#2017, year#2319, TotalPopulation#2189L], [COMM#2017, year#2319, TotalPopulation#2189L, count(1) AS annual_crime_count#2483L]\n",
      "         +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "            :- Project [DR_NO#2262, DateRptd#2263, DATEOCC#2264, TIMEOCC#2265, AREA#2266, AREANAME#2267, RptDistNo#2268, Part#2269, CrmCd#2270, Crm Cd Desc#2271, Mocodes#2272, Vict Age#2273, VictSex#2274, VictDescent#2275, PremisCd#2276, PremisDesc#2277, WeaponUsedCd#2278, WeaponDesc#2279, Status#2280, Status Desc#2281, CrmCd1#2282, CrmCd2#2283, CrmCd3#2284, CrmCd4#2285, ... 6 more fields]\n",
      "            :  +- Filter year#2319 IN (2020,2021)\n",
      "            :     +- Project [DR_NO#2262, DateRptd#2263, DATEOCC#2264, TIMEOCC#2265, AREA#2266, AREANAME#2267, RptDistNo#2268, Part#2269, CrmCd#2270, Crm Cd Desc#2271, Mocodes#2272, Vict Age#2273, VictSex#2274, VictDescent#2275, PremisCd#2276, PremisDesc#2277, WeaponUsedCd#2278, WeaponDesc#2279, Status#2280, Status Desc#2281, CrmCd1#2282, CrmCd2#2283, CrmCd3#2284, CrmCd4#2285, ... 5 more fields]\n",
      "            :        +- Filter (NOT (LAT#2288 = 0.0) AND NOT (LON#2289 = 0.0))\n",
      "            :           +- Relation [DR_NO#2262,DateRptd#2263,DATEOCC#2264,TIMEOCC#2265,AREA#2266,AREANAME#2267,RptDistNo#2268,Part#2269,CrmCd#2270,Crm Cd Desc#2271,Mocodes#2272,Vict Age#2273,VictSex#2274,VictDescent#2275,PremisCd#2276,PremisDesc#2277,WeaponUsedCd#2278,WeaponDesc#2279,Status#2280,Status Desc#2281,CrmCd1#2282,CrmCd2#2283,CrmCd3#2284,CrmCd4#2285,... 4 more fields] csv\n",
      "            +- Project [COMM#2017, geometry#2187, TotalPopulation#2189L]\n",
      "               +- Filter (((isnotnull(COMM#2017) AND NOT (COMM#2017 = )) AND NOT (COMM#2017 = NULL)) AND (TotalPopulation#2189L > cast(0 as bigint)))\n",
      "                  +- Aggregate [COMM#2017], [COMM#2017, st_union_aggr(geometry#2003, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@64f07e4e, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, Some(ST_Union_Aggr)) AS geometry#2187, first(POP20#2029L, false) AS TotalPopulation#2189L]\n",
      "                     +- Filter (CITY#2013 = Los Angeles)\n",
      "                        +- Project [properties#2004.BG20 AS BG20#2009, properties#2004.BG20FIP_CURRENT AS BG20FIP_CURRENT#2010, properties#2004.BGFIP20 AS BGFIP20#2011, properties#2004.CB20 AS CB20#2012, properties#2004.CITY AS CITY#2013, properties#2004.CITYCOMM AS CITYCOMM#2014, properties#2004.CITYCOMM_CURRENT AS CITYCOMM_CURRENT#2015, properties#2004.CITY_CURRENT AS CITY_CURRENT#2016, properties#2004.COMM AS COMM#2017, properties#2004.COMM_CURRENT AS COMM_CURRENT#2018, properties#2004.COUNTY AS COUNTY#2019, properties#2004.CT20 AS CT20#2020, properties#2004.CTCB20 AS CTCB20#2021, properties#2004.FEAT_TYPE AS FEAT_TYPE#2022, properties#2004.FIP20 AS FIP20#2023, properties#2004.FIP_CURRENT AS FIP_CURRENT#2024, properties#2004.HD22 AS HD22#2025L, properties#2004.HD_NAME AS HD_NAME#2026, properties#2004.HOUSING20 AS HOUSING20#2027L, properties#2004.OBJECTID AS OBJECTID#2028L, properties#2004.POP20 AS POP20#2029L, properties#2004.SPA22 AS SPA22#2030L, properties#2004.SPA_NAME AS SPA_NAME#2031, properties#2004.SUP21 AS SUP21#2032, ... 6 more fields]\n",
      "                           +- Project [features#2000.geometry AS geometry#2003, features#2000.properties AS properties#2004, features#2000.type AS type#2005]\n",
      "                              +- Project [features#2000]\n",
      "                                 +- Generate explode(features#1992), false, [features#2000]\n",
      "                                    +- Relation [crs#1991,features#1992,name#1993,type#1994] geojson\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "COMM: string, annual_avg_crime_rate_per_person: double\n",
      "Aggregate [COMM#2017], [COMM#2017, avg(crime_rate_per_person#2497) AS annual_avg_crime_rate_per_person#2509]\n",
      "+- Project [COMM#2017, year#2319, TotalPopulation#2189L, annual_crime_count#2492L, (cast(annual_crime_count#2492L as double) / cast(TotalPopulation#2189L as double)) AS crime_rate_per_person#2497]\n",
      "   +- Project [COMM#2017, year#2319, TotalPopulation#2189L, coalesce(annual_crime_count#2483L, cast(0 as bigint)) AS annual_crime_count#2492L]\n",
      "      +- Aggregate [COMM#2017, year#2319, TotalPopulation#2189L], [COMM#2017, year#2319, TotalPopulation#2189L, count(1) AS annual_crime_count#2483L]\n",
      "         +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "            :- Project [DR_NO#2262, DateRptd#2263, DATEOCC#2264, TIMEOCC#2265, AREA#2266, AREANAME#2267, RptDistNo#2268, Part#2269, CrmCd#2270, Crm Cd Desc#2271, Mocodes#2272, Vict Age#2273, VictSex#2274, VictDescent#2275, PremisCd#2276, PremisDesc#2277, WeaponUsedCd#2278, WeaponDesc#2279, Status#2280, Status Desc#2281, CrmCd1#2282, CrmCd2#2283, CrmCd3#2284, CrmCd4#2285, ... 6 more fields]\n",
      "            :  +- Filter year#2319 IN (2020,2021)\n",
      "            :     +- Project [DR_NO#2262, DateRptd#2263, DATEOCC#2264, TIMEOCC#2265, AREA#2266, AREANAME#2267, RptDistNo#2268, Part#2269, CrmCd#2270, Crm Cd Desc#2271, Mocodes#2272, Vict Age#2273, VictSex#2274, VictDescent#2275, PremisCd#2276, PremisDesc#2277, WeaponUsedCd#2278, WeaponDesc#2279, Status#2280, Status Desc#2281, CrmCd1#2282, CrmCd2#2283, CrmCd3#2284, CrmCd4#2285, ... 5 more fields]\n",
      "            :        +- Filter (NOT (LAT#2288 = 0.0) AND NOT (LON#2289 = 0.0))\n",
      "            :           +- Relation [DR_NO#2262,DateRptd#2263,DATEOCC#2264,TIMEOCC#2265,AREA#2266,AREANAME#2267,RptDistNo#2268,Part#2269,CrmCd#2270,Crm Cd Desc#2271,Mocodes#2272,Vict Age#2273,VictSex#2274,VictDescent#2275,PremisCd#2276,PremisDesc#2277,WeaponUsedCd#2278,WeaponDesc#2279,Status#2280,Status Desc#2281,CrmCd1#2282,CrmCd2#2283,CrmCd3#2284,CrmCd4#2285,... 4 more fields] csv\n",
      "            +- Project [COMM#2017, geometry#2187, TotalPopulation#2189L]\n",
      "               +- Filter (((isnotnull(COMM#2017) AND NOT (COMM#2017 = )) AND NOT (COMM#2017 = NULL)) AND (TotalPopulation#2189L > cast(0 as bigint)))\n",
      "                  +- Aggregate [COMM#2017], [COMM#2017, st_union_aggr(geometry#2003, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@64f07e4e, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, Some(ST_Union_Aggr)) AS geometry#2187, first(POP20#2029L, false) AS TotalPopulation#2189L]\n",
      "                     +- Filter (CITY#2013 = Los Angeles)\n",
      "                        +- Project [properties#2004.BG20 AS BG20#2009, properties#2004.BG20FIP_CURRENT AS BG20FIP_CURRENT#2010, properties#2004.BGFIP20 AS BGFIP20#2011, properties#2004.CB20 AS CB20#2012, properties#2004.CITY AS CITY#2013, properties#2004.CITYCOMM AS CITYCOMM#2014, properties#2004.CITYCOMM_CURRENT AS CITYCOMM_CURRENT#2015, properties#2004.CITY_CURRENT AS CITY_CURRENT#2016, properties#2004.COMM AS COMM#2017, properties#2004.COMM_CURRENT AS COMM_CURRENT#2018, properties#2004.COUNTY AS COUNTY#2019, properties#2004.CT20 AS CT20#2020, properties#2004.CTCB20 AS CTCB20#2021, properties#2004.FEAT_TYPE AS FEAT_TYPE#2022, properties#2004.FIP20 AS FIP20#2023, properties#2004.FIP_CURRENT AS FIP_CURRENT#2024, properties#2004.HD22 AS HD22#2025L, properties#2004.HD_NAME AS HD_NAME#2026, properties#2004.HOUSING20 AS HOUSING20#2027L, properties#2004.OBJECTID AS OBJECTID#2028L, properties#2004.POP20 AS POP20#2029L, properties#2004.SPA22 AS SPA22#2030L, properties#2004.SPA_NAME AS SPA_NAME#2031, properties#2004.SUP21 AS SUP21#2032, ... 6 more fields]\n",
      "                           +- Project [features#2000.geometry AS geometry#2003, features#2000.properties AS properties#2004, features#2000.type AS type#2005]\n",
      "                              +- Project [features#2000]\n",
      "                                 +- Generate explode(features#1992), false, [features#2000]\n",
      "                                    +- Relation [crs#1991,features#1992,name#1993,type#1994] geojson\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [COMM#2017], [COMM#2017, avg(crime_rate_per_person#2497) AS annual_avg_crime_rate_per_person#2509]\n",
      "+- Aggregate [COMM#2017, year#2319, TotalPopulation#2189L], [COMM#2017, (cast(count(1) as double) / cast(TotalPopulation#2189L as double)) AS crime_rate_per_person#2497]\n",
      "   +- Project [year#2319, COMM#2017, TotalPopulation#2189L]\n",
      "      +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "         :- Project [year(cast(DATEOCC#2264 as date)) AS year#2319,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#2349]\n",
      "         :  +- Filter (((isnotnull(LAT#2288) AND isnotnull(LON#2289)) AND ((NOT (LAT#2288 = 0.0) AND NOT (LON#2289 = 0.0)) AND year(cast(DATEOCC#2264 as date)) IN (2020,2021))) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "         :     +- Relation [DR_NO#2262,DateRptd#2263,DATEOCC#2264,TIMEOCC#2265,AREA#2266,AREANAME#2267,RptDistNo#2268,Part#2269,CrmCd#2270,Crm Cd Desc#2271,Mocodes#2272,Vict Age#2273,VictSex#2274,VictDescent#2275,PremisCd#2276,PremisDesc#2277,WeaponUsedCd#2278,WeaponDesc#2279,Status#2280,Status Desc#2281,CrmCd1#2282,CrmCd2#2283,CrmCd3#2284,CrmCd4#2285,... 4 more fields] csv\n",
      "         +- Filter ((isnotnull(TotalPopulation#2189L) AND (TotalPopulation#2189L > 0)) AND isnotnull(geometry#2187))\n",
      "            +- Aggregate [COMM#2017], [COMM#2017, st_union_aggr(geometry#2003, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@64f07e4e, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, Some(ST_Union_Aggr)) AS geometry#2187, first(POP20#2029L, false) AS TotalPopulation#2189L]\n",
      "               +- Project [features#2000.properties.COMM AS COMM#2017, features#2000.properties.POP20 AS POP20#2029L, features#2000.geometry AS geometry#2003]\n",
      "                  +- Filter (isnotnull(features#2000.properties.CITY) AND ((features#2000.properties.CITY = Los Angeles) AND ((isnotnull(features#2000.properties.COMM) AND NOT (features#2000.properties.COMM = )) AND NOT (features#2000.properties.COMM = NULL))))\n",
      "                     +- Generate explode(features#1992), [0], false, [features#2000]\n",
      "                        +- Project [features#1992]\n",
      "                           +- Filter ((size(features#1992, true) > 0) AND isnotnull(features#1992))\n",
      "                              +- Relation [crs#1991,features#1992,name#1993,type#1994] geojson\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[COMM#2017], functions=[avg(crime_rate_per_person#2497)], output=[COMM#2017, annual_avg_crime_rate_per_person#2509], schema specialized)\n",
      "   +- Exchange hashpartitioning(COMM#2017, 1000), ENSURE_REQUIREMENTS, [plan_id=8240]\n",
      "      +- HashAggregate(keys=[COMM#2017], functions=[partial_avg(crime_rate_per_person#2497)], output=[COMM#2017, sum#2678, count#2679L], schema specialized)\n",
      "         +- HashAggregate(keys=[COMM#2017, year#2319, TotalPopulation#2189L], functions=[count(1)], output=[COMM#2017, crime_rate_per_person#2497], schema specialized)\n",
      "            +- Exchange hashpartitioning(COMM#2017, year#2319, TotalPopulation#2189L, 1000), ENSURE_REQUIREMENTS, [plan_id=8236]\n",
      "               +- HashAggregate(keys=[COMM#2017, year#2319, TotalPopulation#2189L], functions=[partial_count(1)], output=[COMM#2017, year#2319, TotalPopulation#2189L, count#2681L], schema specialized)\n",
      "                  +- Project [year#2319, COMM#2017, TotalPopulation#2189L]\n",
      "                     +- RangeJoin crime_point#2349: geometry, geometry#2187: geometry, WITHIN\n",
      "                        :- Project [year(cast(DATEOCC#2264 as date)) AS year#2319,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#2349]\n",
      "                        :  +- Filter (((((isnotnull(LAT#2288) AND isnotnull(LON#2289)) AND NOT (LAT#2288 = 0.0)) AND NOT (LON#2289 = 0.0)) AND year(cast(DATEOCC#2264 as date)) IN (2020,2021)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "                        :     +- FileScan csv [DATEOCC#2264,LAT#2288,LON#2289] Batched: false, DataFilters: [isnotnull(LAT#2288), isnotnull(LON#2289), NOT (LAT#2288 = 0.0), NOT (LON#2289 = 0.0), year(cast(..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON), Not(EqualTo(LAT,0.0)), Not(EqualTo(LON,0.0))], ReadSchema: struct<DATEOCC:timestamp,LAT:double,LON:double>\n",
      "                        +- Filter ((isnotnull(TotalPopulation#2189L) AND (TotalPopulation#2189L > 0)) AND isnotnull(geometry#2187))\n",
      "                           +- ObjectHashAggregate(keys=[COMM#2017], functions=[st_union_aggr(geometry#2003, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@64f07e4e, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, Some(ST_Union_Aggr)), first(POP20#2029L, false)], output=[COMM#2017, geometry#2187, TotalPopulation#2189L])\n",
      "                              +- Exchange hashpartitioning(COMM#2017, 1000), ENSURE_REQUIREMENTS, [plan_id=8229]\n",
      "                                 +- ObjectHashAggregate(keys=[COMM#2017], functions=[partial_st_union_aggr(geometry#2003, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@64f07e4e, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, Some(ST_Union_Aggr)), partial_first(POP20#2029L, false)], output=[COMM#2017, buf#2683, first#2686L, valueSet#2687])\n",
      "                                    +- Project [features#2000.properties.COMM AS COMM#2017, features#2000.properties.POP20 AS POP20#2029L, features#2000.geometry AS geometry#2003]\n",
      "                                       +- Filter (isnotnull(features#2000.properties.CITY) AND ((features#2000.properties.CITY = Los Angeles) AND ((isnotnull(features#2000.properties.COMM) AND NOT (features#2000.properties.COMM = )) AND NOT (features#2000.properties.COMM = NULL))))\n",
      "                                          +- Generate explode(features#1992), false, [features#2000]\n",
      "                                             +- Filter ((size(features#1992, true) > 0) AND isnotnull(features#1992))\n",
      "                                                +- FileScan geojson [features#1992] Batched: false, DataFilters: [(size(features#1992, true) > 0), isnotnull(features#1992)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG20:string,BG20FIP_CURRENT:string..."
     ]
    }
   ],
   "source": [
    "crime_per_comm.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ae2ccfd-a65d-44b7-aaf2-317e19d9c33c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fd24e8a78664fdc91c8ab6bac61ccb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(Inner, [COMM])\n",
      ":- Aggregate [COMM#2017], [COMM#2017, avg(crime_rate_per_person#2497) AS annual_avg_crime_rate_per_person#2509]\n",
      ":  +- Project [COMM#2017, year#2319, TotalPopulation#2189L, annual_crime_count#2492L, (cast(annual_crime_count#2492L as double) / cast(TotalPopulation#2189L as double)) AS crime_rate_per_person#2497]\n",
      ":     +- Project [COMM#2017, year#2319, TotalPopulation#2189L, coalesce(annual_crime_count#2483L, cast(0 as bigint)) AS annual_crime_count#2492L]\n",
      ":        +- Aggregate [COMM#2017, year#2319, TotalPopulation#2189L], [COMM#2017, year#2319, TotalPopulation#2189L, count(1) AS annual_crime_count#2483L]\n",
      ":           +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      ":              :- Project [DR_NO#2262, DateRptd#2263, DATEOCC#2264, TIMEOCC#2265, AREA#2266, AREANAME#2267, RptDistNo#2268, Part#2269, CrmCd#2270, Crm Cd Desc#2271, Mocodes#2272, Vict Age#2273, VictSex#2274, VictDescent#2275, PremisCd#2276, PremisDesc#2277, WeaponUsedCd#2278, WeaponDesc#2279, Status#2280, Status Desc#2281, CrmCd1#2282, CrmCd2#2283, CrmCd3#2284, CrmCd4#2285, ... 6 more fields]\n",
      ":              :  +- Filter year#2319 IN (2020,2021)\n",
      ":              :     +- Project [DR_NO#2262, DateRptd#2263, DATEOCC#2264, TIMEOCC#2265, AREA#2266, AREANAME#2267, RptDistNo#2268, Part#2269, CrmCd#2270, Crm Cd Desc#2271, Mocodes#2272, Vict Age#2273, VictSex#2274, VictDescent#2275, PremisCd#2276, PremisDesc#2277, WeaponUsedCd#2278, WeaponDesc#2279, Status#2280, Status Desc#2281, CrmCd1#2282, CrmCd2#2283, CrmCd3#2284, CrmCd4#2285, ... 5 more fields]\n",
      ":              :        +- Filter (NOT (LAT#2288 = 0.0) AND NOT (LON#2289 = 0.0))\n",
      ":              :           +- Relation [DR_NO#2262,DateRptd#2263,DATEOCC#2264,TIMEOCC#2265,AREA#2266,AREANAME#2267,RptDistNo#2268,Part#2269,CrmCd#2270,Crm Cd Desc#2271,Mocodes#2272,Vict Age#2273,VictSex#2274,VictDescent#2275,PremisCd#2276,PremisDesc#2277,WeaponUsedCd#2278,WeaponDesc#2279,Status#2280,Status Desc#2281,CrmCd1#2282,CrmCd2#2283,CrmCd3#2284,CrmCd4#2285,... 4 more fields] csv\n",
      ":              +- Project [COMM#2017, geometry#2187, TotalPopulation#2189L]\n",
      ":                 +- Filter (((isnotnull(COMM#2017) AND NOT (COMM#2017 = )) AND NOT (COMM#2017 = NULL)) AND (TotalPopulation#2189L > cast(0 as bigint)))\n",
      ":                    +- Aggregate [COMM#2017], [COMM#2017, st_union_aggr(geometry#2003, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@64f07e4e, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, Some(ST_Union_Aggr)) AS geometry#2187, first(POP20#2029L, false) AS TotalPopulation#2189L]\n",
      ":                       +- Filter (CITY#2013 = Los Angeles)\n",
      ":                          +- Project [properties#2004.BG20 AS BG20#2009, properties#2004.BG20FIP_CURRENT AS BG20FIP_CURRENT#2010, properties#2004.BGFIP20 AS BGFIP20#2011, properties#2004.CB20 AS CB20#2012, properties#2004.CITY AS CITY#2013, properties#2004.CITYCOMM AS CITYCOMM#2014, properties#2004.CITYCOMM_CURRENT AS CITYCOMM_CURRENT#2015, properties#2004.CITY_CURRENT AS CITY_CURRENT#2016, properties#2004.COMM AS COMM#2017, properties#2004.COMM_CURRENT AS COMM_CURRENT#2018, properties#2004.COUNTY AS COUNTY#2019, properties#2004.CT20 AS CT20#2020, properties#2004.CTCB20 AS CTCB20#2021, properties#2004.FEAT_TYPE AS FEAT_TYPE#2022, properties#2004.FIP20 AS FIP20#2023, properties#2004.FIP_CURRENT AS FIP_CURRENT#2024, properties#2004.HD22 AS HD22#2025L, properties#2004.HD_NAME AS HD_NAME#2026, properties#2004.HOUSING20 AS HOUSING20#2027L, properties#2004.OBJECTID AS OBJECTID#2028L, properties#2004.POP20 AS POP20#2029L, properties#2004.SPA22 AS SPA22#2030L, properties#2004.SPA_NAME AS SPA_NAME#2031, properties#2004.SUP21 AS SUP21#2032, ... 6 more fields]\n",
      ":                             +- Project [features#2000.geometry AS geometry#2003, features#2000.properties AS properties#2004, features#2000.type AS type#2005]\n",
      ":                                +- Project [features#2000]\n",
      ":                                   +- Generate explode(features#1992), false, [features#2000]\n",
      ":                                      +- Relation [crs#1991,features#1992,name#1993,type#1994] geojson\n",
      "+- Aggregate [COMM#2524], [COMM#2524, avg(Income#2238) AS avg_income#2259]\n",
      "   +- Join Inner, (ZipCode#2232 = ZCTA20#2544)\n",
      "      :- Project [ZipCode#2232, Income#2238]\n",
      "      :  +- Filter (((isnotnull(ZipCode#2232) AND NOT (ZipCode#2232 = )) AND NOT (ZipCode#2232 = NULL)) AND isnotnull(Income#2238))\n",
      "      :     +- Project [Zip Code#2226, Community#2227, Estimated Median Income#2228, ZipCode#2232, cast(regexp_replace(Estimated Median Income#2228, [$,], , 1) as float) AS Income#2238]\n",
      "      :        +- Filter RLIKE(Community#2227, (?i)Los Angeles)\n",
      "      :           +- Project [Zip Code#2226, Community#2227, Estimated Median Income#2228, trim(Zip Code#2226, None) AS ZipCode#2232]\n",
      "      :              +- Relation [Zip Code#2226,Community#2227,Estimated Median Income#2228] csv\n",
      "      +- Deduplicate [ZCTA20#2544, COMM#2524]\n",
      "         +- Filter ((isnotnull(ZCTA20#2544) AND NOT (ZCTA20#2544 = )) AND NOT (ZCTA20#2544 = NULL))\n",
      "            +- Project [ZCTA20#2544, COMM#2524]\n",
      "               +- Filter (CITY#2520 = Los Angeles)\n",
      "                  +- Project [properties#2004.BG20 AS BG20#2516, properties#2004.BG20FIP_CURRENT AS BG20FIP_CURRENT#2517, properties#2004.BGFIP20 AS BGFIP20#2518, properties#2004.CB20 AS CB20#2519, properties#2004.CITY AS CITY#2520, properties#2004.CITYCOMM AS CITYCOMM#2521, properties#2004.CITYCOMM_CURRENT AS CITYCOMM_CURRENT#2522, properties#2004.CITY_CURRENT AS CITY_CURRENT#2523, properties#2004.COMM AS COMM#2524, properties#2004.COMM_CURRENT AS COMM_CURRENT#2525, properties#2004.COUNTY AS COUNTY#2526, properties#2004.CT20 AS CT20#2527, properties#2004.CTCB20 AS CTCB20#2528, properties#2004.FEAT_TYPE AS FEAT_TYPE#2529, properties#2004.FIP20 AS FIP20#2530, properties#2004.FIP_CURRENT AS FIP_CURRENT#2531, properties#2004.HD22 AS HD22#2532L, properties#2004.HD_NAME AS HD_NAME#2533, properties#2004.HOUSING20 AS HOUSING20#2534L, properties#2004.OBJECTID AS OBJECTID#2535L, properties#2004.POP20 AS POP20#2536L, properties#2004.SPA22 AS SPA22#2537L, properties#2004.SPA_NAME AS SPA_NAME#2538, properties#2004.SUP21 AS SUP21#2539, ... 6 more fields]\n",
      "                     +- Project [features#2000.geometry AS geometry#2003, features#2000.properties AS properties#2004, features#2000.type AS type#2005]\n",
      "                        +- Project [features#2000]\n",
      "                           +- Generate explode(features#2513), false, [features#2000]\n",
      "                              +- Relation [crs#2512,features#2513,name#2514,type#2515] geojson\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "COMM: string, annual_avg_crime_rate_per_person: double, avg_income: double\n",
      "Project [COMM#2017, annual_avg_crime_rate_per_person#2509, avg_income#2259]\n",
      "+- Join Inner, (COMM#2017 = COMM#2524)\n",
      "   :- Aggregate [COMM#2017], [COMM#2017, avg(crime_rate_per_person#2497) AS annual_avg_crime_rate_per_person#2509]\n",
      "   :  +- Project [COMM#2017, year#2319, TotalPopulation#2189L, annual_crime_count#2492L, (cast(annual_crime_count#2492L as double) / cast(TotalPopulation#2189L as double)) AS crime_rate_per_person#2497]\n",
      "   :     +- Project [COMM#2017, year#2319, TotalPopulation#2189L, coalesce(annual_crime_count#2483L, cast(0 as bigint)) AS annual_crime_count#2492L]\n",
      "   :        +- Aggregate [COMM#2017, year#2319, TotalPopulation#2189L], [COMM#2017, year#2319, TotalPopulation#2189L, count(1) AS annual_crime_count#2483L]\n",
      "   :           +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "   :              :- Project [DR_NO#2262, DateRptd#2263, DATEOCC#2264, TIMEOCC#2265, AREA#2266, AREANAME#2267, RptDistNo#2268, Part#2269, CrmCd#2270, Crm Cd Desc#2271, Mocodes#2272, Vict Age#2273, VictSex#2274, VictDescent#2275, PremisCd#2276, PremisDesc#2277, WeaponUsedCd#2278, WeaponDesc#2279, Status#2280, Status Desc#2281, CrmCd1#2282, CrmCd2#2283, CrmCd3#2284, CrmCd4#2285, ... 6 more fields]\n",
      "   :              :  +- Filter year#2319 IN (2020,2021)\n",
      "   :              :     +- Project [DR_NO#2262, DateRptd#2263, DATEOCC#2264, TIMEOCC#2265, AREA#2266, AREANAME#2267, RptDistNo#2268, Part#2269, CrmCd#2270, Crm Cd Desc#2271, Mocodes#2272, Vict Age#2273, VictSex#2274, VictDescent#2275, PremisCd#2276, PremisDesc#2277, WeaponUsedCd#2278, WeaponDesc#2279, Status#2280, Status Desc#2281, CrmCd1#2282, CrmCd2#2283, CrmCd3#2284, CrmCd4#2285, ... 5 more fields]\n",
      "   :              :        +- Filter (NOT (LAT#2288 = 0.0) AND NOT (LON#2289 = 0.0))\n",
      "   :              :           +- Relation [DR_NO#2262,DateRptd#2263,DATEOCC#2264,TIMEOCC#2265,AREA#2266,AREANAME#2267,RptDistNo#2268,Part#2269,CrmCd#2270,Crm Cd Desc#2271,Mocodes#2272,Vict Age#2273,VictSex#2274,VictDescent#2275,PremisCd#2276,PremisDesc#2277,WeaponUsedCd#2278,WeaponDesc#2279,Status#2280,Status Desc#2281,CrmCd1#2282,CrmCd2#2283,CrmCd3#2284,CrmCd4#2285,... 4 more fields] csv\n",
      "   :              +- Project [COMM#2017, geometry#2187, TotalPopulation#2189L]\n",
      "   :                 +- Filter (((isnotnull(COMM#2017) AND NOT (COMM#2017 = )) AND NOT (COMM#2017 = NULL)) AND (TotalPopulation#2189L > cast(0 as bigint)))\n",
      "   :                    +- Aggregate [COMM#2017], [COMM#2017, st_union_aggr(geometry#2003, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@64f07e4e, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, Some(ST_Union_Aggr)) AS geometry#2187, first(POP20#2029L, false) AS TotalPopulation#2189L]\n",
      "   :                       +- Filter (CITY#2013 = Los Angeles)\n",
      "   :                          +- Project [properties#2004.BG20 AS BG20#2009, properties#2004.BG20FIP_CURRENT AS BG20FIP_CURRENT#2010, properties#2004.BGFIP20 AS BGFIP20#2011, properties#2004.CB20 AS CB20#2012, properties#2004.CITY AS CITY#2013, properties#2004.CITYCOMM AS CITYCOMM#2014, properties#2004.CITYCOMM_CURRENT AS CITYCOMM_CURRENT#2015, properties#2004.CITY_CURRENT AS CITY_CURRENT#2016, properties#2004.COMM AS COMM#2017, properties#2004.COMM_CURRENT AS COMM_CURRENT#2018, properties#2004.COUNTY AS COUNTY#2019, properties#2004.CT20 AS CT20#2020, properties#2004.CTCB20 AS CTCB20#2021, properties#2004.FEAT_TYPE AS FEAT_TYPE#2022, properties#2004.FIP20 AS FIP20#2023, properties#2004.FIP_CURRENT AS FIP_CURRENT#2024, properties#2004.HD22 AS HD22#2025L, properties#2004.HD_NAME AS HD_NAME#2026, properties#2004.HOUSING20 AS HOUSING20#2027L, properties#2004.OBJECTID AS OBJECTID#2028L, properties#2004.POP20 AS POP20#2029L, properties#2004.SPA22 AS SPA22#2030L, properties#2004.SPA_NAME AS SPA_NAME#2031, properties#2004.SUP21 AS SUP21#2032, ... 6 more fields]\n",
      "   :                             +- Project [features#2000.geometry AS geometry#2003, features#2000.properties AS properties#2004, features#2000.type AS type#2005]\n",
      "   :                                +- Project [features#2000]\n",
      "   :                                   +- Generate explode(features#1992), false, [features#2000]\n",
      "   :                                      +- Relation [crs#1991,features#1992,name#1993,type#1994] geojson\n",
      "   +- Aggregate [COMM#2524], [COMM#2524, avg(Income#2238) AS avg_income#2259]\n",
      "      +- Join Inner, (ZipCode#2232 = ZCTA20#2544)\n",
      "         :- Project [ZipCode#2232, Income#2238]\n",
      "         :  +- Filter (((isnotnull(ZipCode#2232) AND NOT (ZipCode#2232 = )) AND NOT (ZipCode#2232 = NULL)) AND isnotnull(Income#2238))\n",
      "         :     +- Project [Zip Code#2226, Community#2227, Estimated Median Income#2228, ZipCode#2232, cast(regexp_replace(Estimated Median Income#2228, [$,], , 1) as float) AS Income#2238]\n",
      "         :        +- Filter RLIKE(Community#2227, (?i)Los Angeles)\n",
      "         :           +- Project [Zip Code#2226, Community#2227, Estimated Median Income#2228, trim(Zip Code#2226, None) AS ZipCode#2232]\n",
      "         :              +- Relation [Zip Code#2226,Community#2227,Estimated Median Income#2228] csv\n",
      "         +- Deduplicate [ZCTA20#2544, COMM#2524]\n",
      "            +- Filter ((isnotnull(ZCTA20#2544) AND NOT (ZCTA20#2544 = )) AND NOT (ZCTA20#2544 = NULL))\n",
      "               +- Project [ZCTA20#2544, COMM#2524]\n",
      "                  +- Filter (CITY#2520 = Los Angeles)\n",
      "                     +- Project [properties#2004.BG20 AS BG20#2516, properties#2004.BG20FIP_CURRENT AS BG20FIP_CURRENT#2517, properties#2004.BGFIP20 AS BGFIP20#2518, properties#2004.CB20 AS CB20#2519, properties#2004.CITY AS CITY#2520, properties#2004.CITYCOMM AS CITYCOMM#2521, properties#2004.CITYCOMM_CURRENT AS CITYCOMM_CURRENT#2522, properties#2004.CITY_CURRENT AS CITY_CURRENT#2523, properties#2004.COMM AS COMM#2524, properties#2004.COMM_CURRENT AS COMM_CURRENT#2525, properties#2004.COUNTY AS COUNTY#2526, properties#2004.CT20 AS CT20#2527, properties#2004.CTCB20 AS CTCB20#2528, properties#2004.FEAT_TYPE AS FEAT_TYPE#2529, properties#2004.FIP20 AS FIP20#2530, properties#2004.FIP_CURRENT AS FIP_CURRENT#2531, properties#2004.HD22 AS HD22#2532L, properties#2004.HD_NAME AS HD_NAME#2533, properties#2004.HOUSING20 AS HOUSING20#2534L, properties#2004.OBJECTID AS OBJECTID#2535L, properties#2004.POP20 AS POP20#2536L, properties#2004.SPA22 AS SPA22#2537L, properties#2004.SPA_NAME AS SPA_NAME#2538, properties#2004.SUP21 AS SUP21#2539, ... 6 more fields]\n",
      "                        +- Project [features#2000.geometry AS geometry#2003, features#2000.properties AS properties#2004, features#2000.type AS type#2005]\n",
      "                           +- Project [features#2000]\n",
      "                              +- Generate explode(features#2513), false, [features#2000]\n",
      "                                 +- Relation [crs#2512,features#2513,name#2514,type#2515] geojson\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [COMM#2017, annual_avg_crime_rate_per_person#2509, avg_income#2259]\n",
      "+- Join Inner, (COMM#2017 = COMM#2524)\n",
      "   :- Aggregate [COMM#2017], [COMM#2017, avg(crime_rate_per_person#2497) AS annual_avg_crime_rate_per_person#2509]\n",
      "   :  +- Aggregate [COMM#2017, year#2319, TotalPopulation#2189L], [COMM#2017, (cast(count(1) as double) / cast(TotalPopulation#2189L as double)) AS crime_rate_per_person#2497]\n",
      "   :     +- Project [year#2319, COMM#2017, TotalPopulation#2189L]\n",
      "   :        +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "   :           :- Project [year(cast(DATEOCC#2264 as date)) AS year#2319,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#2349]\n",
      "   :           :  +- Filter (((isnotnull(LAT#2288) AND isnotnull(LON#2289)) AND ((NOT (LAT#2288 = 0.0) AND NOT (LON#2289 = 0.0)) AND year(cast(DATEOCC#2264 as date)) IN (2020,2021))) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "   :           :     +- Relation [DR_NO#2262,DateRptd#2263,DATEOCC#2264,TIMEOCC#2265,AREA#2266,AREANAME#2267,RptDistNo#2268,Part#2269,CrmCd#2270,Crm Cd Desc#2271,Mocodes#2272,Vict Age#2273,VictSex#2274,VictDescent#2275,PremisCd#2276,PremisDesc#2277,WeaponUsedCd#2278,WeaponDesc#2279,Status#2280,Status Desc#2281,CrmCd1#2282,CrmCd2#2283,CrmCd3#2284,CrmCd4#2285,... 4 more fields] csv\n",
      "   :           +- Filter ((isnotnull(TotalPopulation#2189L) AND (TotalPopulation#2189L > 0)) AND isnotnull(geometry#2187))\n",
      "   :              +- Aggregate [COMM#2017], [COMM#2017, st_union_aggr(geometry#2003, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@64f07e4e, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, Some(ST_Union_Aggr)) AS geometry#2187, first(POP20#2029L, false) AS TotalPopulation#2189L]\n",
      "   :                 +- Project [features#2000.properties.COMM AS COMM#2017, features#2000.properties.POP20 AS POP20#2029L, features#2000.geometry AS geometry#2003]\n",
      "   :                    +- Filter (((((isnotnull(features#2000.properties.CITY) AND (features#2000.properties.CITY = Los Angeles)) AND isnotnull(features#2000.properties.COMM)) AND NOT (features#2000.properties.COMM = )) AND NOT (features#2000.properties.COMM = NULL)) AND bloomfilter#3501 of [COMM#2524] filtering [features#2000.properties.COMM])\n",
      "   :                       :  +- Aggregate [ZCTA20#2544, COMM#2524], [ZCTA20#2544, COMM#2524]\n",
      "   :                       :     +- Project [features#2000.properties.ZCTA20 AS ZCTA20#2544, features#2000.properties.COMM AS COMM#2524]\n",
      "   :                       :        +- Filter ((isnotnull(features#2000.properties.CITY) AND ((features#2000.properties.CITY = Los Angeles) AND ((isnotnull(features#2000.properties.ZCTA20) AND NOT (features#2000.properties.ZCTA20 = )) AND NOT (features#2000.properties.ZCTA20 = NULL)))) AND ((NOT (features#2000.properties.COMM = ) AND NOT (features#2000.properties.COMM = NULL)) AND isnotnull(features#2000.properties.COMM)))\n",
      "   :                       :           +- Generate explode(features#2513), [0], false, [features#2000]\n",
      "   :                       :              +- Project [features#2513]\n",
      "   :                       :                 +- Filter ((size(features#2513, true) > 0) AND isnotnull(features#2513))\n",
      "   :                       :                    +- Relation [crs#2512,features#2513,name#2514,type#2515] geojson\n",
      "   :                       +- Generate explode(features#1992), [0], false, [features#2000]\n",
      "   :                          +- Project [features#1992]\n",
      "   :                             +- Filter ((size(features#1992, true) > 0) AND isnotnull(features#1992))\n",
      "   :                                +- Relation [crs#1991,features#1992,name#1993,type#1994] geojson\n",
      "   +- Aggregate [COMM#2524], [COMM#2524, avg(Income#2238) AS avg_income#2259]\n",
      "      +- Project [Income#2238, COMM#2524]\n",
      "         +- Join Inner, (ZipCode#2232 = ZCTA20#2544)\n",
      "            :- Project [trim(Zip Code#2226, None) AS ZipCode#2232, cast(regexp_replace(Estimated Median Income#2228, [$,], , 1) as float) AS Income#2238]\n",
      "            :  +- Filter ((isnotnull(Community#2227) AND isnotnull(Estimated Median Income#2228)) AND (RLIKE(Community#2227, (?i)Los Angeles) AND (((isnotnull(trim(Zip Code#2226, None)) AND NOT (trim(Zip Code#2226, None) = )) AND NOT (trim(Zip Code#2226, None) = NULL)) AND isnotnull(cast(regexp_replace(Estimated Median Income#2228, [$,], , 1) as float)))))\n",
      "            :     +- Relation [Zip Code#2226,Community#2227,Estimated Median Income#2228] csv\n",
      "            +- Aggregate [ZCTA20#2544, COMM#2524], [ZCTA20#2544, COMM#2524]\n",
      "               +- Project [features#2000.properties.ZCTA20 AS ZCTA20#2544, features#2000.properties.COMM AS COMM#2524]\n",
      "                  +- Filter ((isnotnull(features#2000.properties.CITY) AND ((features#2000.properties.CITY = Los Angeles) AND ((isnotnull(features#2000.properties.ZCTA20) AND NOT (features#2000.properties.ZCTA20 = )) AND NOT (features#2000.properties.ZCTA20 = NULL)))) AND ((NOT (features#2000.properties.COMM = ) AND NOT (features#2000.properties.COMM = NULL)) AND isnotnull(features#2000.properties.COMM)))\n",
      "                     +- Generate explode(features#2513), [0], false, [features#2000]\n",
      "                        +- Project [features#2513]\n",
      "                           +- Filter ((size(features#2513, true) > 0) AND isnotnull(features#2513))\n",
      "                              +- Relation [crs#2512,features#2513,name#2514,type#2515] geojson\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [COMM#2017, annual_avg_crime_rate_per_person#2509, avg_income#2259]\n",
      "   +- SortMergeJoin [COMM#2017], [COMM#2524], Inner\n",
      "      :- Sort [COMM#2017 ASC NULLS FIRST], false, 0\n",
      "      :  +- HashAggregate(keys=[COMM#2017], functions=[avg(crime_rate_per_person#2497)], output=[COMM#2017, annual_avg_crime_rate_per_person#2509], schema specialized)\n",
      "      :     +- Exchange hashpartitioning(COMM#2017, 1000), ENSURE_REQUIREMENTS, [plan_id=8557]\n",
      "      :        +- HashAggregate(keys=[COMM#2017], functions=[partial_avg(crime_rate_per_person#2497)], output=[COMM#2017, sum#2678, count#2679L], schema specialized)\n",
      "      :           +- HashAggregate(keys=[COMM#2017, year#2319, TotalPopulation#2189L], functions=[count(1)], output=[COMM#2017, crime_rate_per_person#2497], schema specialized)\n",
      "      :              +- Exchange hashpartitioning(COMM#2017, year#2319, TotalPopulation#2189L, 1000), ENSURE_REQUIREMENTS, [plan_id=8554]\n",
      "      :                 +- HashAggregate(keys=[COMM#2017, year#2319, TotalPopulation#2189L], functions=[partial_count(1)], output=[COMM#2017, year#2319, TotalPopulation#2189L, count#2681L], schema specialized)\n",
      "      :                    +- Project [year#2319, COMM#2017, TotalPopulation#2189L]\n",
      "      :                       +- RangeJoin crime_point#2349: geometry, geometry#2187: geometry, WITHIN\n",
      "      :                          :- Project [year(cast(DATEOCC#2264 as date)) AS year#2319,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#2349]\n",
      "      :                          :  +- Filter (((((isnotnull(LAT#2288) AND isnotnull(LON#2289)) AND NOT (LAT#2288 = 0.0)) AND NOT (LON#2289 = 0.0)) AND year(cast(DATEOCC#2264 as date)) IN (2020,2021)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "      :                          :     +- FileScan csv [DATEOCC#2264,LAT#2288,LON#2289] Batched: false, DataFilters: [isnotnull(LAT#2288), isnotnull(LON#2289), NOT (LAT#2288 = 0.0), NOT (LON#2289 = 0.0), year(cast(..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON), Not(EqualTo(LAT,0.0)), Not(EqualTo(LON,0.0))], ReadSchema: struct<DATEOCC:timestamp,LAT:double,LON:double>\n",
      "      :                          +- Filter ((isnotnull(TotalPopulation#2189L) AND (TotalPopulation#2189L > 0)) AND isnotnull(geometry#2187))\n",
      "      :                             +- ObjectHashAggregate(keys=[COMM#2017], functions=[st_union_aggr(geometry#2003, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@64f07e4e, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, Some(ST_Union_Aggr)), first(POP20#2029L, false)], output=[COMM#2017, geometry#2187, TotalPopulation#2189L])\n",
      "      :                                +- Exchange hashpartitioning(COMM#2017, 1000), ENSURE_REQUIREMENTS, [plan_id=8548]\n",
      "      :                                   +- ObjectHashAggregate(keys=[COMM#2017], functions=[partial_st_union_aggr(geometry#2003, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@64f07e4e, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, Some(ST_Union_Aggr)), partial_first(POP20#2029L, false)], output=[COMM#2017, buf#2683, first#2686L, valueSet#2687])\n",
      "      :                                      +- Project [features#2000.properties.COMM AS COMM#2017, features#2000.properties.POP20 AS POP20#2029L, features#2000.geometry AS geometry#2003]\n",
      "      :                                         +- Filter ((((isnotnull(features#2000.properties.CITY) AND (features#2000.properties.CITY = Los Angeles)) AND isnotnull(features#2000.properties.COMM)) AND NOT (features#2000.properties.COMM = )) AND NOT (features#2000.properties.COMM = NULL))\n",
      "      :                                            +- Generate explode(features#1992), false, [features#2000]\n",
      "      :                                               +- Filter ((size(features#1992, true) > 0) AND isnotnull(features#1992))\n",
      "      :                                                  +- FileScan geojson [features#1992] Batched: false, DataFilters: [(size(features#1992, true) > 0), isnotnull(features#1992)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG20:string,BG20FIP_CURRENT:string...\n",
      "      +- Sort [COMM#2524 ASC NULLS FIRST], false, 0\n",
      "         +- HashAggregate(keys=[COMM#2524], functions=[avg(Income#2238)], output=[COMM#2524, avg_income#2259], schema specialized)\n",
      "            +- Exchange hashpartitioning(COMM#2524, 1000), ENSURE_REQUIREMENTS, [plan_id=8520]\n",
      "               +- HashAggregate(keys=[COMM#2524], functions=[partial_avg(Income#2238)], output=[COMM#2524, sum#2690, count#2691L], schema specialized)\n",
      "                  +- Project [Income#2238, COMM#2524]\n",
      "                     +- BroadcastHashJoin [ZipCode#2232], [ZCTA20#2544], Inner, BuildLeft, false\n",
      "                        :- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=8515]\n",
      "                        :  +- Project [trim(Zip Code#2226, None) AS ZipCode#2232, cast(regexp_replace(Estimated Median Income#2228, [$,], , 1) as float) AS Income#2238]\n",
      "                        :     +- Filter ((((((isnotnull(Community#2227) AND isnotnull(Estimated Median Income#2228)) AND RLIKE(Community#2227, (?i)Los Angeles)) AND isnotnull(trim(Zip Code#2226, None))) AND NOT (trim(Zip Code#2226, None) = )) AND NOT (trim(Zip Code#2226, None) = NULL)) AND isnotnull(cast(regexp_replace(Estimated Median Income#2228, [$,], , 1) as float)))\n",
      "                        :        +- FileScan csv [Zip Code#2226,Community#2227,Estimated Median Income#2228] Batched: false, DataFilters: [isnotnull(Community#2227), isnotnull(Estimated Median Income#2228), RLIKE(Community#2227, (?i)Lo..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_i..., PartitionFilters: [], PushedFilters: [IsNotNull(Community), IsNotNull(Estimated Median Income)], ReadSchema: struct<Zip Code:string,Community:string,Estimated Median Income:string>\n",
      "                        +- HashAggregate(keys=[ZCTA20#2544, COMM#2524], functions=[], output=[ZCTA20#2544, COMM#2524], schema specialized)\n",
      "                           +- Exchange hashpartitioning(ZCTA20#2544, COMM#2524, 1000), ENSURE_REQUIREMENTS, [plan_id=8512]\n",
      "                              +- HashAggregate(keys=[ZCTA20#2544, COMM#2524], functions=[], output=[ZCTA20#2544, COMM#2524], schema specialized)\n",
      "                                 +- Project [features#2000.properties.ZCTA20 AS ZCTA20#2544, features#2000.properties.COMM AS COMM#2524]\n",
      "                                    +- Filter ((isnotnull(features#2000.properties.CITY) AND ((features#2000.properties.CITY = Los Angeles) AND ((isnotnull(features#2000.properties.ZCTA20) AND NOT (features#2000.properties.ZCTA20 = )) AND NOT (features#2000.properties.ZCTA20 = NULL)))) AND ((NOT (features#2000.properties.COMM = ) AND NOT (features#2000.properties.COMM = NULL)) AND isnotnull(features#2000.properties.COMM)))\n",
      "                                       +- Generate explode(features#2513), false, [features#2000]\n",
      "                                          +- Filter ((size(features#2513, true) > 0) AND isnotnull(features#2513))\n",
      "                                             +- FileScan geojson [features#2513] Batched: false, DataFilters: [(size(features#2513, true) > 0), isnotnull(features#2513)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG20:string,BG20FIP_CURRENT:string..."
     ]
    }
   ],
   "source": [
    "final_df.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32ec1334-371e-41e5-a5c7-590cf48f0d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0042b1c243dd45f4b9b88bdc8697e031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(Inner, [COMM])\n",
      ":- Aggregate [COMM#2017], [COMM#2017, avg(crime_rate_per_person#2497) AS annual_avg_crime_rate_per_person#2509]\n",
      ":  +- Project [COMM#2017, year#2319, TotalPopulation#2189L, annual_crime_count#2492L, (cast(annual_crime_count#2492L as double) / cast(TotalPopulation#2189L as double)) AS crime_rate_per_person#2497]\n",
      ":     +- Project [COMM#2017, year#2319, TotalPopulation#2189L, coalesce(annual_crime_count#2483L, cast(0 as bigint)) AS annual_crime_count#2492L]\n",
      ":        +- Aggregate [COMM#2017, year#2319, TotalPopulation#2189L], [COMM#2017, year#2319, TotalPopulation#2189L, count(1) AS annual_crime_count#2483L]\n",
      ":           +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      ":              :- Project [DR_NO#2262, DateRptd#2263, DATEOCC#2264, TIMEOCC#2265, AREA#2266, AREANAME#2267, RptDistNo#2268, Part#2269, CrmCd#2270, Crm Cd Desc#2271, Mocodes#2272, Vict Age#2273, VictSex#2274, VictDescent#2275, PremisCd#2276, PremisDesc#2277, WeaponUsedCd#2278, WeaponDesc#2279, Status#2280, Status Desc#2281, CrmCd1#2282, CrmCd2#2283, CrmCd3#2284, CrmCd4#2285, ... 6 more fields]\n",
      ":              :  +- Filter year#2319 IN (2020,2021)\n",
      ":              :     +- Project [DR_NO#2262, DateRptd#2263, DATEOCC#2264, TIMEOCC#2265, AREA#2266, AREANAME#2267, RptDistNo#2268, Part#2269, CrmCd#2270, Crm Cd Desc#2271, Mocodes#2272, Vict Age#2273, VictSex#2274, VictDescent#2275, PremisCd#2276, PremisDesc#2277, WeaponUsedCd#2278, WeaponDesc#2279, Status#2280, Status Desc#2281, CrmCd1#2282, CrmCd2#2283, CrmCd3#2284, CrmCd4#2285, ... 5 more fields]\n",
      ":              :        +- Filter (NOT (LAT#2288 = 0.0) AND NOT (LON#2289 = 0.0))\n",
      ":              :           +- Relation [DR_NO#2262,DateRptd#2263,DATEOCC#2264,TIMEOCC#2265,AREA#2266,AREANAME#2267,RptDistNo#2268,Part#2269,CrmCd#2270,Crm Cd Desc#2271,Mocodes#2272,Vict Age#2273,VictSex#2274,VictDescent#2275,PremisCd#2276,PremisDesc#2277,WeaponUsedCd#2278,WeaponDesc#2279,Status#2280,Status Desc#2281,CrmCd1#2282,CrmCd2#2283,CrmCd3#2284,CrmCd4#2285,... 4 more fields] csv\n",
      ":              +- Project [COMM#2017, geometry#2187, TotalPopulation#2189L]\n",
      ":                 +- Filter (((isnotnull(COMM#2017) AND NOT (COMM#2017 = )) AND NOT (COMM#2017 = NULL)) AND (TotalPopulation#2189L > cast(0 as bigint)))\n",
      ":                    +- Aggregate [COMM#2017], [COMM#2017, st_union_aggr(geometry#2003, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@64f07e4e, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, Some(ST_Union_Aggr)) AS geometry#2187, first(POP20#2029L, false) AS TotalPopulation#2189L]\n",
      ":                       +- Filter (CITY#2013 = Los Angeles)\n",
      ":                          +- Project [properties#2004.BG20 AS BG20#2009, properties#2004.BG20FIP_CURRENT AS BG20FIP_CURRENT#2010, properties#2004.BGFIP20 AS BGFIP20#2011, properties#2004.CB20 AS CB20#2012, properties#2004.CITY AS CITY#2013, properties#2004.CITYCOMM AS CITYCOMM#2014, properties#2004.CITYCOMM_CURRENT AS CITYCOMM_CURRENT#2015, properties#2004.CITY_CURRENT AS CITY_CURRENT#2016, properties#2004.COMM AS COMM#2017, properties#2004.COMM_CURRENT AS COMM_CURRENT#2018, properties#2004.COUNTY AS COUNTY#2019, properties#2004.CT20 AS CT20#2020, properties#2004.CTCB20 AS CTCB20#2021, properties#2004.FEAT_TYPE AS FEAT_TYPE#2022, properties#2004.FIP20 AS FIP20#2023, properties#2004.FIP_CURRENT AS FIP_CURRENT#2024, properties#2004.HD22 AS HD22#2025L, properties#2004.HD_NAME AS HD_NAME#2026, properties#2004.HOUSING20 AS HOUSING20#2027L, properties#2004.OBJECTID AS OBJECTID#2028L, properties#2004.POP20 AS POP20#2029L, properties#2004.SPA22 AS SPA22#2030L, properties#2004.SPA_NAME AS SPA_NAME#2031, properties#2004.SUP21 AS SUP21#2032, ... 6 more fields]\n",
      ":                             +- Project [features#2000.geometry AS geometry#2003, features#2000.properties AS properties#2004, features#2000.type AS type#2005]\n",
      ":                                +- Project [features#2000]\n",
      ":                                   +- Generate explode(features#1992), false, [features#2000]\n",
      ":                                      +- Relation [crs#1991,features#1992,name#1993,type#1994] geojson\n",
      "+- Aggregate [COMM#2524], [COMM#2524, avg(Income#2238) AS avg_income#2259]\n",
      "   +- Join Inner, (ZipCode#2232 = ZCTA20#2544)\n",
      "      :- Project [ZipCode#2232, Income#2238]\n",
      "      :  +- Filter (((isnotnull(ZipCode#2232) AND NOT (ZipCode#2232 = )) AND NOT (ZipCode#2232 = NULL)) AND isnotnull(Income#2238))\n",
      "      :     +- Project [Zip Code#2226, Community#2227, Estimated Median Income#2228, ZipCode#2232, cast(regexp_replace(Estimated Median Income#2228, [$,], , 1) as float) AS Income#2238]\n",
      "      :        +- Filter RLIKE(Community#2227, (?i)Los Angeles)\n",
      "      :           +- Project [Zip Code#2226, Community#2227, Estimated Median Income#2228, trim(Zip Code#2226, None) AS ZipCode#2232]\n",
      "      :              +- Relation [Zip Code#2226,Community#2227,Estimated Median Income#2228] csv\n",
      "      +- Deduplicate [ZCTA20#2544, COMM#2524]\n",
      "         +- Filter ((isnotnull(ZCTA20#2544) AND NOT (ZCTA20#2544 = )) AND NOT (ZCTA20#2544 = NULL))\n",
      "            +- Project [ZCTA20#2544, COMM#2524]\n",
      "               +- Filter (CITY#2520 = Los Angeles)\n",
      "                  +- Project [properties#2004.BG20 AS BG20#2516, properties#2004.BG20FIP_CURRENT AS BG20FIP_CURRENT#2517, properties#2004.BGFIP20 AS BGFIP20#2518, properties#2004.CB20 AS CB20#2519, properties#2004.CITY AS CITY#2520, properties#2004.CITYCOMM AS CITYCOMM#2521, properties#2004.CITYCOMM_CURRENT AS CITYCOMM_CURRENT#2522, properties#2004.CITY_CURRENT AS CITY_CURRENT#2523, properties#2004.COMM AS COMM#2524, properties#2004.COMM_CURRENT AS COMM_CURRENT#2525, properties#2004.COUNTY AS COUNTY#2526, properties#2004.CT20 AS CT20#2527, properties#2004.CTCB20 AS CTCB20#2528, properties#2004.FEAT_TYPE AS FEAT_TYPE#2529, properties#2004.FIP20 AS FIP20#2530, properties#2004.FIP_CURRENT AS FIP_CURRENT#2531, properties#2004.HD22 AS HD22#2532L, properties#2004.HD_NAME AS HD_NAME#2533, properties#2004.HOUSING20 AS HOUSING20#2534L, properties#2004.OBJECTID AS OBJECTID#2535L, properties#2004.POP20 AS POP20#2536L, properties#2004.SPA22 AS SPA22#2537L, properties#2004.SPA_NAME AS SPA_NAME#2538, properties#2004.SUP21 AS SUP21#2539, ... 6 more fields]\n",
      "                     +- Project [features#2000.geometry AS geometry#2003, features#2000.properties AS properties#2004, features#2000.type AS type#2005]\n",
      "                        +- Project [features#2000]\n",
      "                           +- Generate explode(features#2513), false, [features#2000]\n",
      "                              +- Relation [crs#2512,features#2513,name#2514,type#2515] geojson\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "COMM: string, annual_avg_crime_rate_per_person: double, avg_income: double\n",
      "Project [COMM#2017, annual_avg_crime_rate_per_person#2509, avg_income#2259]\n",
      "+- Join Inner, (COMM#2017 = COMM#2524)\n",
      "   :- Aggregate [COMM#2017], [COMM#2017, avg(crime_rate_per_person#2497) AS annual_avg_crime_rate_per_person#2509]\n",
      "   :  +- Project [COMM#2017, year#2319, TotalPopulation#2189L, annual_crime_count#2492L, (cast(annual_crime_count#2492L as double) / cast(TotalPopulation#2189L as double)) AS crime_rate_per_person#2497]\n",
      "   :     +- Project [COMM#2017, year#2319, TotalPopulation#2189L, coalesce(annual_crime_count#2483L, cast(0 as bigint)) AS annual_crime_count#2492L]\n",
      "   :        +- Aggregate [COMM#2017, year#2319, TotalPopulation#2189L], [COMM#2017, year#2319, TotalPopulation#2189L, count(1) AS annual_crime_count#2483L]\n",
      "   :           +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "   :              :- Project [DR_NO#2262, DateRptd#2263, DATEOCC#2264, TIMEOCC#2265, AREA#2266, AREANAME#2267, RptDistNo#2268, Part#2269, CrmCd#2270, Crm Cd Desc#2271, Mocodes#2272, Vict Age#2273, VictSex#2274, VictDescent#2275, PremisCd#2276, PremisDesc#2277, WeaponUsedCd#2278, WeaponDesc#2279, Status#2280, Status Desc#2281, CrmCd1#2282, CrmCd2#2283, CrmCd3#2284, CrmCd4#2285, ... 6 more fields]\n",
      "   :              :  +- Filter year#2319 IN (2020,2021)\n",
      "   :              :     +- Project [DR_NO#2262, DateRptd#2263, DATEOCC#2264, TIMEOCC#2265, AREA#2266, AREANAME#2267, RptDistNo#2268, Part#2269, CrmCd#2270, Crm Cd Desc#2271, Mocodes#2272, Vict Age#2273, VictSex#2274, VictDescent#2275, PremisCd#2276, PremisDesc#2277, WeaponUsedCd#2278, WeaponDesc#2279, Status#2280, Status Desc#2281, CrmCd1#2282, CrmCd2#2283, CrmCd3#2284, CrmCd4#2285, ... 5 more fields]\n",
      "   :              :        +- Filter (NOT (LAT#2288 = 0.0) AND NOT (LON#2289 = 0.0))\n",
      "   :              :           +- Relation [DR_NO#2262,DateRptd#2263,DATEOCC#2264,TIMEOCC#2265,AREA#2266,AREANAME#2267,RptDistNo#2268,Part#2269,CrmCd#2270,Crm Cd Desc#2271,Mocodes#2272,Vict Age#2273,VictSex#2274,VictDescent#2275,PremisCd#2276,PremisDesc#2277,WeaponUsedCd#2278,WeaponDesc#2279,Status#2280,Status Desc#2281,CrmCd1#2282,CrmCd2#2283,CrmCd3#2284,CrmCd4#2285,... 4 more fields] csv\n",
      "   :              +- Project [COMM#2017, geometry#2187, TotalPopulation#2189L]\n",
      "   :                 +- Filter (((isnotnull(COMM#2017) AND NOT (COMM#2017 = )) AND NOT (COMM#2017 = NULL)) AND (TotalPopulation#2189L > cast(0 as bigint)))\n",
      "   :                    +- Aggregate [COMM#2017], [COMM#2017, st_union_aggr(geometry#2003, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@64f07e4e, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, Some(ST_Union_Aggr)) AS geometry#2187, first(POP20#2029L, false) AS TotalPopulation#2189L]\n",
      "   :                       +- Filter (CITY#2013 = Los Angeles)\n",
      "   :                          +- Project [properties#2004.BG20 AS BG20#2009, properties#2004.BG20FIP_CURRENT AS BG20FIP_CURRENT#2010, properties#2004.BGFIP20 AS BGFIP20#2011, properties#2004.CB20 AS CB20#2012, properties#2004.CITY AS CITY#2013, properties#2004.CITYCOMM AS CITYCOMM#2014, properties#2004.CITYCOMM_CURRENT AS CITYCOMM_CURRENT#2015, properties#2004.CITY_CURRENT AS CITY_CURRENT#2016, properties#2004.COMM AS COMM#2017, properties#2004.COMM_CURRENT AS COMM_CURRENT#2018, properties#2004.COUNTY AS COUNTY#2019, properties#2004.CT20 AS CT20#2020, properties#2004.CTCB20 AS CTCB20#2021, properties#2004.FEAT_TYPE AS FEAT_TYPE#2022, properties#2004.FIP20 AS FIP20#2023, properties#2004.FIP_CURRENT AS FIP_CURRENT#2024, properties#2004.HD22 AS HD22#2025L, properties#2004.HD_NAME AS HD_NAME#2026, properties#2004.HOUSING20 AS HOUSING20#2027L, properties#2004.OBJECTID AS OBJECTID#2028L, properties#2004.POP20 AS POP20#2029L, properties#2004.SPA22 AS SPA22#2030L, properties#2004.SPA_NAME AS SPA_NAME#2031, properties#2004.SUP21 AS SUP21#2032, ... 6 more fields]\n",
      "   :                             +- Project [features#2000.geometry AS geometry#2003, features#2000.properties AS properties#2004, features#2000.type AS type#2005]\n",
      "   :                                +- Project [features#2000]\n",
      "   :                                   +- Generate explode(features#1992), false, [features#2000]\n",
      "   :                                      +- Relation [crs#1991,features#1992,name#1993,type#1994] geojson\n",
      "   +- Aggregate [COMM#2524], [COMM#2524, avg(Income#2238) AS avg_income#2259]\n",
      "      +- Join Inner, (ZipCode#2232 = ZCTA20#2544)\n",
      "         :- Project [ZipCode#2232, Income#2238]\n",
      "         :  +- Filter (((isnotnull(ZipCode#2232) AND NOT (ZipCode#2232 = )) AND NOT (ZipCode#2232 = NULL)) AND isnotnull(Income#2238))\n",
      "         :     +- Project [Zip Code#2226, Community#2227, Estimated Median Income#2228, ZipCode#2232, cast(regexp_replace(Estimated Median Income#2228, [$,], , 1) as float) AS Income#2238]\n",
      "         :        +- Filter RLIKE(Community#2227, (?i)Los Angeles)\n",
      "         :           +- Project [Zip Code#2226, Community#2227, Estimated Median Income#2228, trim(Zip Code#2226, None) AS ZipCode#2232]\n",
      "         :              +- Relation [Zip Code#2226,Community#2227,Estimated Median Income#2228] csv\n",
      "         +- Deduplicate [ZCTA20#2544, COMM#2524]\n",
      "            +- Filter ((isnotnull(ZCTA20#2544) AND NOT (ZCTA20#2544 = )) AND NOT (ZCTA20#2544 = NULL))\n",
      "               +- Project [ZCTA20#2544, COMM#2524]\n",
      "                  +- Filter (CITY#2520 = Los Angeles)\n",
      "                     +- Project [properties#2004.BG20 AS BG20#2516, properties#2004.BG20FIP_CURRENT AS BG20FIP_CURRENT#2517, properties#2004.BGFIP20 AS BGFIP20#2518, properties#2004.CB20 AS CB20#2519, properties#2004.CITY AS CITY#2520, properties#2004.CITYCOMM AS CITYCOMM#2521, properties#2004.CITYCOMM_CURRENT AS CITYCOMM_CURRENT#2522, properties#2004.CITY_CURRENT AS CITY_CURRENT#2523, properties#2004.COMM AS COMM#2524, properties#2004.COMM_CURRENT AS COMM_CURRENT#2525, properties#2004.COUNTY AS COUNTY#2526, properties#2004.CT20 AS CT20#2527, properties#2004.CTCB20 AS CTCB20#2528, properties#2004.FEAT_TYPE AS FEAT_TYPE#2529, properties#2004.FIP20 AS FIP20#2530, properties#2004.FIP_CURRENT AS FIP_CURRENT#2531, properties#2004.HD22 AS HD22#2532L, properties#2004.HD_NAME AS HD_NAME#2533, properties#2004.HOUSING20 AS HOUSING20#2534L, properties#2004.OBJECTID AS OBJECTID#2535L, properties#2004.POP20 AS POP20#2536L, properties#2004.SPA22 AS SPA22#2537L, properties#2004.SPA_NAME AS SPA_NAME#2538, properties#2004.SUP21 AS SUP21#2539, ... 6 more fields]\n",
      "                        +- Project [features#2000.geometry AS geometry#2003, features#2000.properties AS properties#2004, features#2000.type AS type#2005]\n",
      "                           +- Project [features#2000]\n",
      "                              +- Generate explode(features#2513), false, [features#2000]\n",
      "                                 +- Relation [crs#2512,features#2513,name#2514,type#2515] geojson\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [COMM#2017, annual_avg_crime_rate_per_person#2509, avg_income#2259]\n",
      "+- Join Inner, (COMM#2017 = COMM#2524)\n",
      "   :- Aggregate [COMM#2017], [COMM#2017, avg(crime_rate_per_person#2497) AS annual_avg_crime_rate_per_person#2509]\n",
      "   :  +- Aggregate [COMM#2017, year#2319, TotalPopulation#2189L], [COMM#2017, (cast(count(1) as double) / cast(TotalPopulation#2189L as double)) AS crime_rate_per_person#2497]\n",
      "   :     +- Project [year#2319, COMM#2017, TotalPopulation#2189L]\n",
      "   :        +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "   :           :- Project [year(cast(DATEOCC#2264 as date)) AS year#2319,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#2349]\n",
      "   :           :  +- Filter (((isnotnull(LAT#2288) AND isnotnull(LON#2289)) AND ((NOT (LAT#2288 = 0.0) AND NOT (LON#2289 = 0.0)) AND year(cast(DATEOCC#2264 as date)) IN (2020,2021))) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "   :           :     +- Relation [DR_NO#2262,DateRptd#2263,DATEOCC#2264,TIMEOCC#2265,AREA#2266,AREANAME#2267,RptDistNo#2268,Part#2269,CrmCd#2270,Crm Cd Desc#2271,Mocodes#2272,Vict Age#2273,VictSex#2274,VictDescent#2275,PremisCd#2276,PremisDesc#2277,WeaponUsedCd#2278,WeaponDesc#2279,Status#2280,Status Desc#2281,CrmCd1#2282,CrmCd2#2283,CrmCd3#2284,CrmCd4#2285,... 4 more fields] csv\n",
      "   :           +- Filter ((isnotnull(TotalPopulation#2189L) AND (TotalPopulation#2189L > 0)) AND isnotnull(geometry#2187))\n",
      "   :              +- Aggregate [COMM#2017], [COMM#2017, st_union_aggr(geometry#2003, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@64f07e4e, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, Some(ST_Union_Aggr)) AS geometry#2187, first(POP20#2029L, false) AS TotalPopulation#2189L]\n",
      "   :                 +- Project [features#2000.properties.COMM AS COMM#2017, features#2000.properties.POP20 AS POP20#2029L, features#2000.geometry AS geometry#2003]\n",
      "   :                    +- Filter (((((isnotnull(features#2000.properties.CITY) AND (features#2000.properties.CITY = Los Angeles)) AND isnotnull(features#2000.properties.COMM)) AND NOT (features#2000.properties.COMM = )) AND NOT (features#2000.properties.COMM = NULL)) AND bloomfilter#3501 of [COMM#2524] filtering [features#2000.properties.COMM])\n",
      "   :                       :  +- Aggregate [ZCTA20#2544, COMM#2524], [ZCTA20#2544, COMM#2524]\n",
      "   :                       :     +- Project [features#2000.properties.ZCTA20 AS ZCTA20#2544, features#2000.properties.COMM AS COMM#2524]\n",
      "   :                       :        +- Filter ((isnotnull(features#2000.properties.CITY) AND ((features#2000.properties.CITY = Los Angeles) AND ((isnotnull(features#2000.properties.ZCTA20) AND NOT (features#2000.properties.ZCTA20 = )) AND NOT (features#2000.properties.ZCTA20 = NULL)))) AND ((NOT (features#2000.properties.COMM = ) AND NOT (features#2000.properties.COMM = NULL)) AND isnotnull(features#2000.properties.COMM)))\n",
      "   :                       :           +- Generate explode(features#2513), [0], false, [features#2000]\n",
      "   :                       :              +- Project [features#2513]\n",
      "   :                       :                 +- Filter ((size(features#2513, true) > 0) AND isnotnull(features#2513))\n",
      "   :                       :                    +- Relation [crs#2512,features#2513,name#2514,type#2515] geojson\n",
      "   :                       +- Generate explode(features#1992), [0], false, [features#2000]\n",
      "   :                          +- Project [features#1992]\n",
      "   :                             +- Filter ((size(features#1992, true) > 0) AND isnotnull(features#1992))\n",
      "   :                                +- Relation [crs#1991,features#1992,name#1993,type#1994] geojson\n",
      "   +- Aggregate [COMM#2524], [COMM#2524, avg(Income#2238) AS avg_income#2259]\n",
      "      +- Project [Income#2238, COMM#2524]\n",
      "         +- Join Inner, (ZipCode#2232 = ZCTA20#2544)\n",
      "            :- Project [trim(Zip Code#2226, None) AS ZipCode#2232, cast(regexp_replace(Estimated Median Income#2228, [$,], , 1) as float) AS Income#2238]\n",
      "            :  +- Filter ((isnotnull(Community#2227) AND isnotnull(Estimated Median Income#2228)) AND (RLIKE(Community#2227, (?i)Los Angeles) AND (((isnotnull(trim(Zip Code#2226, None)) AND NOT (trim(Zip Code#2226, None) = )) AND NOT (trim(Zip Code#2226, None) = NULL)) AND isnotnull(cast(regexp_replace(Estimated Median Income#2228, [$,], , 1) as float)))))\n",
      "            :     +- Relation [Zip Code#2226,Community#2227,Estimated Median Income#2228] csv\n",
      "            +- Aggregate [ZCTA20#2544, COMM#2524], [ZCTA20#2544, COMM#2524]\n",
      "               +- Project [features#2000.properties.ZCTA20 AS ZCTA20#2544, features#2000.properties.COMM AS COMM#2524]\n",
      "                  +- Filter ((isnotnull(features#2000.properties.CITY) AND ((features#2000.properties.CITY = Los Angeles) AND ((isnotnull(features#2000.properties.ZCTA20) AND NOT (features#2000.properties.ZCTA20 = )) AND NOT (features#2000.properties.ZCTA20 = NULL)))) AND ((NOT (features#2000.properties.COMM = ) AND NOT (features#2000.properties.COMM = NULL)) AND isnotnull(features#2000.properties.COMM)))\n",
      "                     +- Generate explode(features#2513), [0], false, [features#2000]\n",
      "                        +- Project [features#2513]\n",
      "                           +- Filter ((size(features#2513, true) > 0) AND isnotnull(features#2513))\n",
      "                              +- Relation [crs#2512,features#2513,name#2514,type#2515] geojson\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [COMM#2017, annual_avg_crime_rate_per_person#2509, avg_income#2259]\n",
      "   +- SortMergeJoin [COMM#2017], [COMM#2524], Inner\n",
      "      :- Sort [COMM#2017 ASC NULLS FIRST], false, 0\n",
      "      :  +- HashAggregate(keys=[COMM#2017], functions=[avg(crime_rate_per_person#2497)], output=[COMM#2017, annual_avg_crime_rate_per_person#2509], schema specialized)\n",
      "      :     +- Exchange hashpartitioning(COMM#2017, 1000), ENSURE_REQUIREMENTS, [plan_id=8557]\n",
      "      :        +- HashAggregate(keys=[COMM#2017], functions=[partial_avg(crime_rate_per_person#2497)], output=[COMM#2017, sum#2678, count#2679L], schema specialized)\n",
      "      :           +- HashAggregate(keys=[COMM#2017, year#2319, TotalPopulation#2189L], functions=[count(1)], output=[COMM#2017, crime_rate_per_person#2497], schema specialized)\n",
      "      :              +- Exchange hashpartitioning(COMM#2017, year#2319, TotalPopulation#2189L, 1000), ENSURE_REQUIREMENTS, [plan_id=8554]\n",
      "      :                 +- HashAggregate(keys=[COMM#2017, year#2319, TotalPopulation#2189L], functions=[partial_count(1)], output=[COMM#2017, year#2319, TotalPopulation#2189L, count#2681L], schema specialized)\n",
      "      :                    +- Project [year#2319, COMM#2017, TotalPopulation#2189L]\n",
      "      :                       +- RangeJoin crime_point#2349: geometry, geometry#2187: geometry, WITHIN\n",
      "      :                          :- Project [year(cast(DATEOCC#2264 as date)) AS year#2319,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#2349]\n",
      "      :                          :  +- Filter (((((isnotnull(LAT#2288) AND isnotnull(LON#2289)) AND NOT (LAT#2288 = 0.0)) AND NOT (LON#2289 = 0.0)) AND year(cast(DATEOCC#2264 as date)) IN (2020,2021)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "      :                          :     +- FileScan csv [DATEOCC#2264,LAT#2288,LON#2289] Batched: false, DataFilters: [isnotnull(LAT#2288), isnotnull(LON#2289), NOT (LAT#2288 = 0.0), NOT (LON#2289 = 0.0), year(cast(..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON), Not(EqualTo(LAT,0.0)), Not(EqualTo(LON,0.0))], ReadSchema: struct<DATEOCC:timestamp,LAT:double,LON:double>\n",
      "      :                          +- Filter ((isnotnull(TotalPopulation#2189L) AND (TotalPopulation#2189L > 0)) AND isnotnull(geometry#2187))\n",
      "      :                             +- ObjectHashAggregate(keys=[COMM#2017], functions=[st_union_aggr(geometry#2003, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@64f07e4e, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, Some(ST_Union_Aggr)), first(POP20#2029L, false)], output=[COMM#2017, geometry#2187, TotalPopulation#2189L])\n",
      "      :                                +- Exchange hashpartitioning(COMM#2017, 1000), ENSURE_REQUIREMENTS, [plan_id=8548]\n",
      "      :                                   +- ObjectHashAggregate(keys=[COMM#2017], functions=[partial_st_union_aggr(geometry#2003, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@64f07e4e, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, Some(ST_Union_Aggr)), partial_first(POP20#2029L, false)], output=[COMM#2017, buf#2683, first#2686L, valueSet#2687])\n",
      "      :                                      +- Project [features#2000.properties.COMM AS COMM#2017, features#2000.properties.POP20 AS POP20#2029L, features#2000.geometry AS geometry#2003]\n",
      "      :                                         +- Filter ((((isnotnull(features#2000.properties.CITY) AND (features#2000.properties.CITY = Los Angeles)) AND isnotnull(features#2000.properties.COMM)) AND NOT (features#2000.properties.COMM = )) AND NOT (features#2000.properties.COMM = NULL))\n",
      "      :                                            +- Generate explode(features#1992), false, [features#2000]\n",
      "      :                                               +- Filter ((size(features#1992, true) > 0) AND isnotnull(features#1992))\n",
      "      :                                                  +- FileScan geojson [features#1992] Batched: false, DataFilters: [(size(features#1992, true) > 0), isnotnull(features#1992)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG20:string,BG20FIP_CURRENT:string...\n",
      "      +- Sort [COMM#2524 ASC NULLS FIRST], false, 0\n",
      "         +- HashAggregate(keys=[COMM#2524], functions=[avg(Income#2238)], output=[COMM#2524, avg_income#2259], schema specialized)\n",
      "            +- Exchange hashpartitioning(COMM#2524, 1000), ENSURE_REQUIREMENTS, [plan_id=8520]\n",
      "               +- HashAggregate(keys=[COMM#2524], functions=[partial_avg(Income#2238)], output=[COMM#2524, sum#2690, count#2691L], schema specialized)\n",
      "                  +- Project [Income#2238, COMM#2524]\n",
      "                     +- BroadcastHashJoin [ZipCode#2232], [ZCTA20#2544], Inner, BuildLeft, false\n",
      "                        :- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=8515]\n",
      "                        :  +- Project [trim(Zip Code#2226, None) AS ZipCode#2232, cast(regexp_replace(Estimated Median Income#2228, [$,], , 1) as float) AS Income#2238]\n",
      "                        :     +- Filter ((((((isnotnull(Community#2227) AND isnotnull(Estimated Median Income#2228)) AND RLIKE(Community#2227, (?i)Los Angeles)) AND isnotnull(trim(Zip Code#2226, None))) AND NOT (trim(Zip Code#2226, None) = )) AND NOT (trim(Zip Code#2226, None) = NULL)) AND isnotnull(cast(regexp_replace(Estimated Median Income#2228, [$,], , 1) as float)))\n",
      "                        :        +- FileScan csv [Zip Code#2226,Community#2227,Estimated Median Income#2228] Batched: false, DataFilters: [isnotnull(Community#2227), isnotnull(Estimated Median Income#2228), RLIKE(Community#2227, (?i)Lo..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_i..., PartitionFilters: [], PushedFilters: [IsNotNull(Community), IsNotNull(Estimated Median Income)], ReadSchema: struct<Zip Code:string,Community:string,Estimated Median Income:string>\n",
      "                        +- HashAggregate(keys=[ZCTA20#2544, COMM#2524], functions=[], output=[ZCTA20#2544, COMM#2524], schema specialized)\n",
      "                           +- Exchange hashpartitioning(ZCTA20#2544, COMM#2524, 1000), ENSURE_REQUIREMENTS, [plan_id=8512]\n",
      "                              +- HashAggregate(keys=[ZCTA20#2544, COMM#2524], functions=[], output=[ZCTA20#2544, COMM#2524], schema specialized)\n",
      "                                 +- Project [features#2000.properties.ZCTA20 AS ZCTA20#2544, features#2000.properties.COMM AS COMM#2524]\n",
      "                                    +- Filter ((isnotnull(features#2000.properties.CITY) AND ((features#2000.properties.CITY = Los Angeles) AND ((isnotnull(features#2000.properties.ZCTA20) AND NOT (features#2000.properties.ZCTA20 = )) AND NOT (features#2000.properties.ZCTA20 = NULL)))) AND ((NOT (features#2000.properties.COMM = ) AND NOT (features#2000.properties.COMM = NULL)) AND isnotnull(features#2000.properties.COMM)))\n",
      "                                       +- Generate explode(features#2513), false, [features#2000]\n",
      "                                          +- Filter ((size(features#2513, true) > 0) AND isnotnull(features#2513))\n",
      "                                             +- FileScan geojson [features#2513] Batched: false, DataFilters: [(size(features#2513, true) > 0), isnotnull(features#2513)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG20:string,BG20FIP_CURRENT:string...\n",
      "\n",
      "+------------------+--------------------------------+------------------+\n",
      "|COMM              |annual_avg_crime_rate_per_person|avg_income        |\n",
      "+------------------+--------------------------------+------------------+\n",
      "|Adams-Normandie   |4.005263157894737               |44248.5           |\n",
      "|Angelino Heights  |1.8863636363636362              |61857.0           |\n",
      "|Arleta            |20.684210526315788              |61787.0           |\n",
      "|Atwater Village   |1.0385232744783308              |89806.0           |\n",
      "|Baldwin Hills     |3.4104609929078014              |51519.0           |\n",
      "|Bel Air           |6.439024390243903               |171812.5          |\n",
      "|Beverly Crest     |4.463768115942029               |138893.5          |\n",
      "|Beverlywood       |8.652542372881356               |95030.0           |\n",
      "|Boyle Heights     |9.65035799522673                |51684.333333333336|\n",
      "|Brookside         |0.3943661971830986              |64799.0           |\n",
      "|Cadillac-Corning  |0.7142857142857142              |95030.0           |\n",
      "|Carthay           |15.470588235294118              |102567.0          |\n",
      "|Central           |7.443478260869565               |39655.5           |\n",
      "|Century City      |19.11111111111111               |106504.66666666667|\n",
      "|Century Palms/Cove|17.313793103448276              |47077.8           |\n",
      "|Chatsworth        |16.913793103448278              |106649.33333333333|\n",
      "|Cheviot Hills     |9.294117647058822               |99805.66666666667 |\n",
      "|Cloverdale/Cochran|5.723684210526315               |57637.5           |\n",
      "|Country Club Park |7.721052631578948               |61025.333333333336|\n",
      "|Crenshaw District |4.813291139240507               |52771.0           |\n",
      "+------------------+--------------------------------+------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "# Explain the final join\n",
    "final_df.explain(True)\n",
    "# Show top 20 rows\n",
    "final_df.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4bf767d2-c379-4b23-b16d-b637c18ee042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfdcd0757bc94d79b2664d3381e2a54b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- geometry: geometry (nullable = true)\n",
      " |-- properties: struct (nullable = true)\n",
      " |    |-- BG20: string (nullable = true)\n",
      " |    |-- BG20FIP_CURRENT: string (nullable = true)\n",
      " |    |-- BGFIP20: string (nullable = true)\n",
      " |    |-- CB20: string (nullable = true)\n",
      " |    |-- CITY: string (nullable = true)\n",
      " |    |-- CITYCOMM: string (nullable = true)\n",
      " |    |-- CITYCOMM_CURRENT: string (nullable = true)\n",
      " |    |-- CITY_CURRENT: string (nullable = true)\n",
      " |    |-- COMM: string (nullable = true)\n",
      " |    |-- COMM_CURRENT: string (nullable = true)\n",
      " |    |-- COUNTY: string (nullable = true)\n",
      " |    |-- CT20: string (nullable = true)\n",
      " |    |-- CTCB20: string (nullable = true)\n",
      " |    |-- FEAT_TYPE: string (nullable = true)\n",
      " |    |-- FIP20: string (nullable = true)\n",
      " |    |-- FIP_CURRENT: string (nullable = true)\n",
      " |    |-- HD22: long (nullable = true)\n",
      " |    |-- HD_NAME: string (nullable = true)\n",
      " |    |-- HOUSING20: long (nullable = true)\n",
      " |    |-- OBJECTID: long (nullable = true)\n",
      " |    |-- POP20: long (nullable = true)\n",
      " |    |-- SPA22: long (nullable = true)\n",
      " |    |-- SPA_NAME: string (nullable = true)\n",
      " |    |-- SUP21: string (nullable = true)\n",
      " |    |-- SUP_LABEL: string (nullable = true)\n",
      " |    |-- ShapeSTArea: double (nullable = true)\n",
      " |    |-- ShapeSTLength: double (nullable = true)\n",
      " |    |-- State: string (nullable = true)\n",
      " |    |-- ZCTA20: string (nullable = true)\n",
      " |-- type: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "blocks_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8a330eac-472e-420d-8b93-82bfef40fccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb4887b04da7490c97c54a5a503f465e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- BG20: string (nullable = true)\n",
      " |-- BG20FIP_CURRENT: string (nullable = true)\n",
      " |-- BGFIP20: string (nullable = true)\n",
      " |-- CB20: string (nullable = true)\n",
      " |-- CITY: string (nullable = true)\n",
      " |-- CITYCOMM: string (nullable = true)\n",
      " |-- CITYCOMM_CURRENT: string (nullable = true)\n",
      " |-- CITY_CURRENT: string (nullable = true)\n",
      " |-- COMM: string (nullable = true)\n",
      " |-- COMM_CURRENT: string (nullable = true)\n",
      " |-- COUNTY: string (nullable = true)\n",
      " |-- CT20: string (nullable = true)\n",
      " |-- CTCB20: string (nullable = true)\n",
      " |-- FEAT_TYPE: string (nullable = true)\n",
      " |-- FIP20: string (nullable = true)\n",
      " |-- FIP_CURRENT: string (nullable = true)\n",
      " |-- HD22: long (nullable = true)\n",
      " |-- HD_NAME: string (nullable = true)\n",
      " |-- HOUSING20: long (nullable = true)\n",
      " |-- OBJECTID: long (nullable = true)\n",
      " |-- POP20: long (nullable = true)\n",
      " |-- SPA22: long (nullable = true)\n",
      " |-- SPA_NAME: string (nullable = true)\n",
      " |-- SUP21: string (nullable = true)\n",
      " |-- SUP_LABEL: string (nullable = true)\n",
      " |-- ShapeSTArea: double (nullable = true)\n",
      " |-- ShapeSTLength: double (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- ZCTA20: string (nullable = true)\n",
      " |-- geometry: geometry (nullable = true)"
     ]
    }
   ],
   "source": [
    "flattened_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e7ec3328-58cf-4b4b-8521-c1b1940b33f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abd8397026b24fd1b4e4dc29f2e42565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- BG20: string (nullable = true)\n",
      " |-- BG20FIP_CURRENT: string (nullable = true)\n",
      " |-- BGFIP20: string (nullable = true)\n",
      " |-- CB20: string (nullable = true)\n",
      " |-- CITY: string (nullable = true)\n",
      " |-- CITYCOMM: string (nullable = true)\n",
      " |-- CITYCOMM_CURRENT: string (nullable = true)\n",
      " |-- CITY_CURRENT: string (nullable = true)\n",
      " |-- COMM: string (nullable = true)\n",
      " |-- COMM_CURRENT: string (nullable = true)\n",
      " |-- COUNTY: string (nullable = true)\n",
      " |-- CT20: string (nullable = true)\n",
      " |-- CTCB20: string (nullable = true)\n",
      " |-- FEAT_TYPE: string (nullable = true)\n",
      " |-- FIP20: string (nullable = true)\n",
      " |-- FIP_CURRENT: string (nullable = true)\n",
      " |-- HD22: long (nullable = true)\n",
      " |-- HD_NAME: string (nullable = true)\n",
      " |-- HOUSING20: long (nullable = true)\n",
      " |-- OBJECTID: long (nullable = true)\n",
      " |-- POP20: long (nullable = true)\n",
      " |-- SPA22: long (nullable = true)\n",
      " |-- SPA_NAME: string (nullable = true)\n",
      " |-- SUP21: string (nullable = true)\n",
      " |-- SUP_LABEL: string (nullable = true)\n",
      " |-- ShapeSTArea: double (nullable = true)\n",
      " |-- ShapeSTLength: double (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- ZCTA20: string (nullable = true)\n",
      " |-- geometry: geometry (nullable = true)"
     ]
    }
   ],
   "source": [
    "la_flattened_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6dbfb098-ea4c-4b08-b7de-78553e54b205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29e6d794c4ba492faf8ddb9ea4cd32b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ZipCode: string (nullable = true)\n",
      " |-- Income: float (nullable = true)"
     ]
    }
   ],
   "source": [
    "income_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c3e1bcd9-1b0e-437d-a809-93e9c28fde13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca69068fee914d7e830ec9cacb72def1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DR_NO: string (nullable = true)\n",
      " |-- DateRptd: timestamp (nullable = true)\n",
      " |-- DATEOCC: timestamp (nullable = true)\n",
      " |-- TIMEOCC: string (nullable = true)\n",
      " |-- AREA: string (nullable = true)\n",
      " |-- AREANAME: string (nullable = true)\n",
      " |-- RptDistNo: string (nullable = true)\n",
      " |-- Part: integer (nullable = true)\n",
      " |-- CrmCd: string (nullable = true)\n",
      " |-- Crm Cd Desc: string (nullable = true)\n",
      " |-- Mocodes: string (nullable = true)\n",
      " |-- Vict Age: string (nullable = true)\n",
      " |-- VictSex: string (nullable = true)\n",
      " |-- VictDescent: string (nullable = true)\n",
      " |-- PremisCd: string (nullable = true)\n",
      " |-- PremisDesc: string (nullable = true)\n",
      " |-- WeaponUsedCd: string (nullable = true)\n",
      " |-- WeaponDesc: string (nullable = true)\n",
      " |-- Status: string (nullable = true)\n",
      " |-- Status Desc: string (nullable = true)\n",
      " |-- CrmCd1: string (nullable = true)\n",
      " |-- CrmCd2: string (nullable = true)\n",
      " |-- CrmCd3: string (nullable = true)\n",
      " |-- CrmCd4: string (nullable = true)\n",
      " |-- LOCATION: string (nullable = true)\n",
      " |-- CrossStreet: string (nullable = true)\n",
      " |-- LAT: double (nullable = true)\n",
      " |-- LON: double (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- DR_NO: string (nullable = true)\n",
      " |-- DateRptd: timestamp (nullable = true)\n",
      " |-- DATEOCC: timestamp (nullable = true)\n",
      " |-- TIMEOCC: string (nullable = true)\n",
      " |-- AREA: string (nullable = true)\n",
      " |-- AREANAME: string (nullable = true)\n",
      " |-- RptDistNo: string (nullable = true)\n",
      " |-- Part: integer (nullable = true)\n",
      " |-- CrmCd: string (nullable = true)\n",
      " |-- Crm Cd Desc: string (nullable = true)\n",
      " |-- Mocodes: string (nullable = true)\n",
      " |-- Vict Age: string (nullable = true)\n",
      " |-- VictSex: string (nullable = true)\n",
      " |-- VictDescent: string (nullable = true)\n",
      " |-- PremisCd: string (nullable = true)\n",
      " |-- PremisDesc: string (nullable = true)\n",
      " |-- WeaponUsedCd: string (nullable = true)\n",
      " |-- WeaponDesc: string (nullable = true)\n",
      " |-- Status: string (nullable = true)\n",
      " |-- Status Desc: string (nullable = true)\n",
      " |-- CrmCd1: string (nullable = true)\n",
      " |-- CrmCd2: string (nullable = true)\n",
      " |-- CrmCd3: string (nullable = true)\n",
      " |-- CrmCd4: string (nullable = true)\n",
      " |-- LOCATION: string (nullable = true)\n",
      " |-- CrossStreet: string (nullable = true)\n",
      " |-- LAT: double (nullable = true)\n",
      " |-- LON: double (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- crime_point: geometry (nullable = true)"
     ]
    }
   ],
   "source": [
    "crimes_df.printSchema()\n",
    "crimes_geo_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35e44313-ca45-4507-9e6a-0073df833fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '4', 'spark.executor.cores': '2', 'spark.executor.memory': '4g'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1562</td><td>application_1765289937462_1548</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1548/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-131.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1548_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1564</td><td>application_1765289937462_1550</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1550/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-149.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1550_01_000001/livy\">Link</a></td><td>None</td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.executor.instances\": \"4\",\n",
    "        \"spark.executor.cores\": \"2\",\n",
    "        \"spark.executor.memory\": \"4g\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6506d44a-c601-488c-9e68-4ca90e526cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1570</td><td>application_1765289937462_1556</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1556/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-251.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1556_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6c03ddd38e47429bef6df07ce0456d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "005e83b68e0747bca6d4949b51b3d3d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from sedona.spark import SedonaContext\n",
    "\n",
    "# Create a new Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LA Crime Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#  Initialize Sedona (if using spatial operations)\n",
    "sedona = SedonaContext.create(spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "047c191d-76bb-4e04-a179-b97e4090e22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1569</td><td>application_1765289937462_1555</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1555/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-55.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1555_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "611a36389d7b405c817d4b79dfef5108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e43743725cf4212837904b9d6f86b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    StructField, StructType, StringType, IntegerType, \n",
    "    FloatType, DoubleType, DateType, TimestampType  \n",
    ")\n",
    "from pyspark.sql.functions import (\n",
    "    year, when, count, sum, col, row_number, \n",
    "    to_timestamp, regexp_replace, to_date, expr,avg,broadcast,first    \n",
    ")\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, sum, count, expr, coalesce, lit, when,explode,collect_set\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import  to_date, year\n",
    "import time\n",
    "from pyspark.sql.functions import col, regexp_replace, trim\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, expr, year, trim, regexp_replace, first, broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aabd1e1-c273-47d9-8287-81f12f329e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5355684b57a432bb3aa35be329ab275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fgeo=\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Census_Blocks_2020.geojson\"\n",
    "fincome=\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_income_2021.csv\"\n",
    "fcrime=\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6a3fe08-0b32-4477-bf04-b2f3c60c6565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab3bed56b9714793acabab485768515a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "crimes_schema = StructType([\n",
    "    StructField(\"DR_NO\", StringType()),\n",
    "    StructField(\"DateRptd\", TimestampType()),\n",
    "    StructField(\"DATEOCC\", TimestampType()),\n",
    "    StructField(\"TIMEOCC\", StringType()),\n",
    "    StructField(\"AREA\", StringType()),\n",
    "    StructField(\"AREANAME\", StringType()),\n",
    "    StructField(\"RptDistNo\", StringType()),\n",
    "    StructField(\"Part\", IntegerType()),\n",
    "    StructField(\"CrmCd\", StringType()),\n",
    "    StructField(\"Crm Cd Desc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"Vict Age\", StringType()),\n",
    "    StructField(\"VictSex\", StringType()),\n",
    "    StructField(\"VictDescent\", StringType()),\n",
    "    StructField(\"PremisCd\", StringType()),\n",
    "    StructField(\"PremisDesc\", StringType()),\n",
    "    StructField(\"WeaponUsedCd\", StringType()),\n",
    "    StructField(\"WeaponDesc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"Status Desc\", StringType()),\n",
    "    StructField(\"CrmCd1\", StringType()),\n",
    "    StructField(\"CrmCd2\", StringType()),\n",
    "    StructField(\"CrmCd3\", StringType()),\n",
    "    StructField(\"CrmCd4\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"CrossStreet\", StringType()),\n",
    "    StructField(\"LAT\", DoubleType()),\n",
    "    StructField(\"LON\", DoubleType()),\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6408aa97-0ef2-4aaa-850c-f19a49594502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8032975726440e7a2e62dd994af3a77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, expr, year, trim, regexp_replace, first, broadcast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d296ea6-6b9f-4172-92ed-8ae76341876a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5234349a55d41ab84a40510ff586129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total correlation: 0.1502766830319662\n",
      "Top 10 richest COMM correlation: -0.3735207290423716\n",
      "Bottom 10 poorest COMM correlation: 0.018338409553787367\n",
      "Elapsed time: 77.83 seconds"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "#Prepare the geojson file\n",
    "blocks_df = sedona.read.format('geojson') \\\n",
    "    .option('multiLine','true').load(fgeo) \\\n",
    "    .selectExpr('explode(features) as features') \\\n",
    "    .select('features.*')\n",
    "\n",
    "flattened_df = blocks_df.select(\n",
    "    [col(f'properties.{col_name}').alias(col_name) for col_name in \\\n",
    "    blocks_df.schema['properties'].dataType.fieldNames()] + ['geometry']) \\\n",
    "    .drop('properties').drop('type')\n",
    "\n",
    "la_comm_df = flattened_df.filter(col(\"CITY\") == \"Los Angeles\") \\\n",
    "    .groupBy(\"COMM\") \\\n",
    "    .agg(\n",
    "        expr(\"ST_Union_Aggr(geometry)\").alias(\"geometry\"),\n",
    "        first(\"POP20\").alias(\"TotalPopulation\")\n",
    "    ) \\\n",
    "    .filter(\n",
    "        (col(\"COMM\").isNotNull()) &\n",
    "        (col(\"COMM\") != \"\") &\n",
    "        (col(\"COMM\") != \"NULL\") &\n",
    "        (col(\"TotalPopulation\") > 0)\n",
    "    )\n",
    "\n",
    "\n",
    "zip_codes_comm = flattened_df.filter(col(\"CITY\") == \"Los Angeles\") \\\n",
    "    .select(\"ZCTA20\", \"COMM\") \\\n",
    "    .filter(\n",
    "        col(\"ZCTA20\").isNotNull() &\n",
    "        (col(\"ZCTA20\") != \"\") &\n",
    "        (col(\"ZCTA20\") != \"NULL\")\n",
    "    ) \\\n",
    "    .dropDuplicates()\n",
    "\n",
    "\n",
    "\n",
    "#  Read CSV with correct delimiter\n",
    "income_df = spark.read.csv(\n",
    "    fincome,\n",
    "    header=True,\n",
    "    sep=';',\n",
    "    quote='\"',\n",
    "    escape='\"',\n",
    "    inferSchema=False\n",
    ")\n",
    "\n",
    "# 2️Trim the Zip Code column and rename\n",
    "income_df = income_df.withColumn(\"ZipCode\", trim(col(\"Zip Code\")))\n",
    "\n",
    "# 3️ Keep only rows where Community mentions Los Angeles\n",
    "income_df = income_df.filter(col(\"Community\").rlike(\"(?i)Los Angeles\"))\n",
    "\n",
    "# Clean Income column\n",
    "income_df = income_df.withColumn(\n",
    "    \"Income\",\n",
    "    regexp_replace(col(\"Estimated Median Income\"), r\"[$,]\", \"\").cast(\"float\")\n",
    ")\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "income_df = income_df.filter(\n",
    "    col(\"ZipCode\").isNotNull() &       # ZipCode not NULL\n",
    "    (col(\"ZipCode\") != \"\") &           # ZipCode not empty\n",
    "    (col(\"ZipCode\") != \"NULL\") &       # ZipCode not the string \"NULL\"\n",
    "    col(\"Income\").isNotNull()          # Income not NULL\n",
    ")\n",
    "\n",
    "#  Keep only ZipCode and Income column\n",
    "income_df = income_df.select(\"ZipCode\", \"Income\")\n",
    "\n",
    "income_with_comm = income_df.join(\n",
    "    zip_codes_comm,\n",
    "    income_df.ZipCode == zip_codes_comm.ZCTA20,\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "income_per_comm = income_with_comm.groupBy(\"COMM\") \\\n",
    "    .agg(F.avg(\"Income\").alias(\"avg_income\"))\n",
    "\n",
    "#Crimes\n",
    "crimes_df = spark.read.csv(\n",
    "    fcrime,\n",
    "    header=True,\n",
    "    schema=crimes_schema,\n",
    "    timestampFormat='yyyy MMM dd hh:mm:ss a',\n",
    "    quote='\"',\n",
    "    escape='\"'\n",
    ")\n",
    "\n",
    "#  Cast Lat/Lon to Double  and Filter Zeros\n",
    "crimes_df = crimes_df.filter((col('LAT') != 0.0) & (col('LON') != 0.0))\n",
    "\n",
    "# Extract Year\n",
    "crimes_df = crimes_df.withColumn(\"year\", year(col(\"DATEOCC\")))\n",
    "\n",
    "# Filter for years 2020 and 2021\n",
    "crimes_df = crimes_df.filter(col(\"year\").isin([2020, 2021]))\n",
    "\n",
    "#  Create Geometry (Requires Apache Sedona)\n",
    "#  ST_Point takes (Longitude, Latitude) -> (X, Y)\n",
    "crimes_geo_df = crimes_df.withColumn(\"crime_point\", expr(\"ST_Point(LON, LAT)\"))\n",
    "\n",
    "# Count crimes per community per year\n",
    "crime_per_comm_year = (\n",
    "    crimes_geo_df\n",
    "    .join(\n",
    "        la_comm_df.select(\"COMM\", \"geometry\", \"TotalPopulation\"),\n",
    "        expr(\"ST_Within(crime_point, geometry)\"),\n",
    "        \"inner\"\n",
    "    )\n",
    "    .groupBy(\"COMM\", \"year\", \"TotalPopulation\")\n",
    "    .agg(F.count(\"*\").alias(\"annual_crime_count\"))\n",
    ")\n",
    "crime_per_comm_year = crime_per_comm_year.fillna({\"annual_crime_count\": 0})\n",
    "\n",
    "\n",
    "# Crime rate per person per year\n",
    "crime_rate_ = crime_per_comm_year.withColumn(\n",
    "    \"crime_rate_per_person\",\n",
    "    col(\"annual_crime_count\") / col(\"TotalPopulation\")\n",
    ")\n",
    "\n",
    "# Annual average crime rate per person\n",
    "crime_per_comm = (\n",
    "    crime_rate_\n",
    "    .groupBy(\"COMM\")\n",
    "    .agg(\n",
    "        F.avg(\"crime_rate_per_person\").alias(\"annual_avg_crime_rate_per_person\")\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "final_df = (\n",
    "    crime_per_comm   # COMM, annual_avg_crime_rate_per_person\n",
    "    .join(\n",
    "        income_per_comm,   # COMM, avg_income\n",
    "        on=\"COMM\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    ")\n",
    "# All areas\n",
    "total_corr = final_df.stat.corr(\"avg_income\", \"annual_avg_crime_rate_per_person\")\n",
    "\n",
    "# Top 10 richest areas\n",
    "top10 = final_df.orderBy(col(\"avg_income\").desc()).limit(10)\n",
    "top10_corr = top10.stat.corr(\"avg_income\", \"annual_avg_crime_rate_per_person\")\n",
    "\n",
    "# Bottom 10 poorest areas\n",
    "bottom10 = final_df.orderBy(col(\"avg_income\").asc()).limit(10)\n",
    "bottom10_corr = bottom10.stat.corr(\"avg_income\", \"annual_avg_crime_rate_per_person\")\n",
    "\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "\n",
    "# Show results\n",
    "\n",
    "print(\"Total correlation:\", total_corr)\n",
    "print(\"Top 10 richest COMM correlation:\", top10_corr)\n",
    "print(\"Bottom 10 poorest COMM correlation:\", bottom10_corr)\n",
    "print(f\"Elapsed time: {elapsed:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "187cfe66-ba16-4365-ae9e-9402a9d92a1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e52ca687f64029b99c74ba64fcb3c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['COMM], ['COMM, avg('Income) AS avg_income#756]\n",
      "+- Join Inner, (ZipCode#729 = ZCTA20#534)\n",
      "   :- Project [ZipCode#729, Income#735]\n",
      "   :  +- Filter (((isnotnull(ZipCode#729) AND NOT (ZipCode#729 = )) AND NOT (ZipCode#729 = NULL)) AND isnotnull(Income#735))\n",
      "   :     +- Project [Zip Code#723, Community#724, Estimated Median Income#725, ZipCode#729, cast(regexp_replace(Estimated Median Income#725, [$,], , 1) as float) AS Income#735]\n",
      "   :        +- Filter RLIKE(Community#724, (?i)Los Angeles)\n",
      "   :           +- Project [Zip Code#723, Community#724, Estimated Median Income#725, trim(Zip Code#723, None) AS ZipCode#729]\n",
      "   :              +- Relation [Zip Code#723,Community#724,Estimated Median Income#725] csv\n",
      "   +- Deduplicate [ZCTA20#534, COMM#514]\n",
      "      +- Filter ((isnotnull(ZCTA20#534) AND NOT (ZCTA20#534 = )) AND NOT (ZCTA20#534 = NULL))\n",
      "         +- Project [ZCTA20#534, COMM#514]\n",
      "            +- Filter (CITY#510 = Los Angeles)\n",
      "               +- Project [properties#501.BG20 AS BG20#506, properties#501.BG20FIP_CURRENT AS BG20FIP_CURRENT#507, properties#501.BGFIP20 AS BGFIP20#508, properties#501.CB20 AS CB20#509, properties#501.CITY AS CITY#510, properties#501.CITYCOMM AS CITYCOMM#511, properties#501.CITYCOMM_CURRENT AS CITYCOMM_CURRENT#512, properties#501.CITY_CURRENT AS CITY_CURRENT#513, properties#501.COMM AS COMM#514, properties#501.COMM_CURRENT AS COMM_CURRENT#515, properties#501.COUNTY AS COUNTY#516, properties#501.CT20 AS CT20#517, properties#501.CTCB20 AS CTCB20#518, properties#501.FEAT_TYPE AS FEAT_TYPE#519, properties#501.FIP20 AS FIP20#520, properties#501.FIP_CURRENT AS FIP_CURRENT#521, properties#501.HD22 AS HD22#522L, properties#501.HD_NAME AS HD_NAME#523, properties#501.HOUSING20 AS HOUSING20#524L, properties#501.OBJECTID AS OBJECTID#525L, properties#501.POP20 AS POP20#526L, properties#501.SPA22 AS SPA22#527L, properties#501.SPA_NAME AS SPA_NAME#528, properties#501.SUP21 AS SUP21#529, ... 6 more fields]\n",
      "                  +- Project [features#497.geometry AS geometry#500, features#497.properties AS properties#501, features#497.type AS type#502]\n",
      "                     +- Project [features#497]\n",
      "                        +- Generate explode(features#489), false, [features#497]\n",
      "                           +- Relation [crs#488,features#489,name#490,type#491] geojson\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "COMM: string, avg_income: double\n",
      "Aggregate [COMM#514], [COMM#514, avg(Income#735) AS avg_income#756]\n",
      "+- Join Inner, (ZipCode#729 = ZCTA20#534)\n",
      "   :- Project [ZipCode#729, Income#735]\n",
      "   :  +- Filter (((isnotnull(ZipCode#729) AND NOT (ZipCode#729 = )) AND NOT (ZipCode#729 = NULL)) AND isnotnull(Income#735))\n",
      "   :     +- Project [Zip Code#723, Community#724, Estimated Median Income#725, ZipCode#729, cast(regexp_replace(Estimated Median Income#725, [$,], , 1) as float) AS Income#735]\n",
      "   :        +- Filter RLIKE(Community#724, (?i)Los Angeles)\n",
      "   :           +- Project [Zip Code#723, Community#724, Estimated Median Income#725, trim(Zip Code#723, None) AS ZipCode#729]\n",
      "   :              +- Relation [Zip Code#723,Community#724,Estimated Median Income#725] csv\n",
      "   +- Deduplicate [ZCTA20#534, COMM#514]\n",
      "      +- Filter ((isnotnull(ZCTA20#534) AND NOT (ZCTA20#534 = )) AND NOT (ZCTA20#534 = NULL))\n",
      "         +- Project [ZCTA20#534, COMM#514]\n",
      "            +- Filter (CITY#510 = Los Angeles)\n",
      "               +- Project [properties#501.BG20 AS BG20#506, properties#501.BG20FIP_CURRENT AS BG20FIP_CURRENT#507, properties#501.BGFIP20 AS BGFIP20#508, properties#501.CB20 AS CB20#509, properties#501.CITY AS CITY#510, properties#501.CITYCOMM AS CITYCOMM#511, properties#501.CITYCOMM_CURRENT AS CITYCOMM_CURRENT#512, properties#501.CITY_CURRENT AS CITY_CURRENT#513, properties#501.COMM AS COMM#514, properties#501.COMM_CURRENT AS COMM_CURRENT#515, properties#501.COUNTY AS COUNTY#516, properties#501.CT20 AS CT20#517, properties#501.CTCB20 AS CTCB20#518, properties#501.FEAT_TYPE AS FEAT_TYPE#519, properties#501.FIP20 AS FIP20#520, properties#501.FIP_CURRENT AS FIP_CURRENT#521, properties#501.HD22 AS HD22#522L, properties#501.HD_NAME AS HD_NAME#523, properties#501.HOUSING20 AS HOUSING20#524L, properties#501.OBJECTID AS OBJECTID#525L, properties#501.POP20 AS POP20#526L, properties#501.SPA22 AS SPA22#527L, properties#501.SPA_NAME AS SPA_NAME#528, properties#501.SUP21 AS SUP21#529, ... 6 more fields]\n",
      "                  +- Project [features#497.geometry AS geometry#500, features#497.properties AS properties#501, features#497.type AS type#502]\n",
      "                     +- Project [features#497]\n",
      "                        +- Generate explode(features#489), false, [features#497]\n",
      "                           +- Relation [crs#488,features#489,name#490,type#491] geojson\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [COMM#514], [COMM#514, avg(Income#735) AS avg_income#756]\n",
      "+- Project [Income#735, COMM#514]\n",
      "   +- Join Inner, (ZipCode#729 = ZCTA20#534)\n",
      "      :- Project [trim(Zip Code#723, None) AS ZipCode#729, cast(regexp_replace(Estimated Median Income#725, [$,], , 1) as float) AS Income#735]\n",
      "      :  +- Filter ((isnotnull(Community#724) AND isnotnull(Estimated Median Income#725)) AND (RLIKE(Community#724, (?i)Los Angeles) AND (((isnotnull(trim(Zip Code#723, None)) AND NOT (trim(Zip Code#723, None) = )) AND NOT (trim(Zip Code#723, None) = NULL)) AND isnotnull(cast(regexp_replace(Estimated Median Income#725, [$,], , 1) as float)))))\n",
      "      :     +- Relation [Zip Code#723,Community#724,Estimated Median Income#725] csv\n",
      "      +- Aggregate [ZCTA20#534, COMM#514], [ZCTA20#534, COMM#514]\n",
      "         +- Project [features#497.properties.ZCTA20 AS ZCTA20#534, features#497.properties.COMM AS COMM#514]\n",
      "            +- Filter (isnotnull(features#497.properties.CITY) AND ((features#497.properties.CITY = Los Angeles) AND ((isnotnull(features#497.properties.ZCTA20) AND NOT (features#497.properties.ZCTA20 = )) AND NOT (features#497.properties.ZCTA20 = NULL))))\n",
      "               +- Generate explode(features#489), [0], false, [features#497]\n",
      "                  +- Project [features#489]\n",
      "                     +- Filter ((size(features#489, true) > 0) AND isnotnull(features#489))\n",
      "                        +- Relation [crs#488,features#489,name#490,type#491] geojson\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[COMM#514], functions=[avg(Income#735)], output=[COMM#514, avg_income#756], schema specialized)\n",
      "   +- Exchange hashpartitioning(COMM#514, 1000), ENSURE_REQUIREMENTS, [plan_id=3439]\n",
      "      +- HashAggregate(keys=[COMM#514], functions=[partial_avg(Income#735)], output=[COMM#514, sum#1187, count#1188L], schema specialized)\n",
      "         +- Project [Income#735, COMM#514]\n",
      "            +- BroadcastHashJoin [ZipCode#729], [ZCTA20#534], Inner, BuildLeft, false\n",
      "               :- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=3434]\n",
      "               :  +- Project [trim(Zip Code#723, None) AS ZipCode#729, cast(regexp_replace(Estimated Median Income#725, [$,], , 1) as float) AS Income#735]\n",
      "               :     +- Filter ((((((isnotnull(Community#724) AND isnotnull(Estimated Median Income#725)) AND RLIKE(Community#724, (?i)Los Angeles)) AND isnotnull(trim(Zip Code#723, None))) AND NOT (trim(Zip Code#723, None) = )) AND NOT (trim(Zip Code#723, None) = NULL)) AND isnotnull(cast(regexp_replace(Estimated Median Income#725, [$,], , 1) as float)))\n",
      "               :        +- FileScan csv [Zip Code#723,Community#724,Estimated Median Income#725] Batched: false, DataFilters: [isnotnull(Community#724), isnotnull(Estimated Median Income#725), RLIKE(Community#724, (?i)Los A..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_i..., PartitionFilters: [], PushedFilters: [IsNotNull(Community), IsNotNull(Estimated Median Income)], ReadSchema: struct<Zip Code:string,Community:string,Estimated Median Income:string>\n",
      "               +- HashAggregate(keys=[ZCTA20#534, COMM#514], functions=[], output=[ZCTA20#534, COMM#514], schema specialized)\n",
      "                  +- Exchange hashpartitioning(ZCTA20#534, COMM#514, 1000), ENSURE_REQUIREMENTS, [plan_id=3431]\n",
      "                     +- HashAggregate(keys=[ZCTA20#534, COMM#514], functions=[], output=[ZCTA20#534, COMM#514], schema specialized)\n",
      "                        +- Project [features#497.properties.ZCTA20 AS ZCTA20#534, features#497.properties.COMM AS COMM#514]\n",
      "                           +- Filter (isnotnull(features#497.properties.CITY) AND ((features#497.properties.CITY = Los Angeles) AND ((isnotnull(features#497.properties.ZCTA20) AND NOT (features#497.properties.ZCTA20 = )) AND NOT (features#497.properties.ZCTA20 = NULL))))\n",
      "                              +- Generate explode(features#489), false, [features#497]\n",
      "                                 +- Filter ((size(features#489, true) > 0) AND isnotnull(features#489))\n",
      "                                    +- FileScan geojson [features#489] Batched: false, DataFilters: [(size(features#489, true) > 0), isnotnull(features#489)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG20:string,BG20FIP_CURRENT:string..."
     ]
    }
   ],
   "source": [
    "income_per_comm.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bdad42e-48fe-4030-9139-913e56528313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88b27b5e5145480884cd7aca22c9566c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['COMM], ['COMM, avg('crime_rate_per_person) AS annual_avg_crime_rate_per_person#1006]\n",
      "+- Project [COMM#514, year#816, TotalPopulation#686L, annual_crime_count#989L, (cast(annual_crime_count#989L as double) / cast(TotalPopulation#686L as double)) AS crime_rate_per_person#994]\n",
      "   +- Project [COMM#514, year#816, TotalPopulation#686L, coalesce(annual_crime_count#980L, cast(0 as bigint)) AS annual_crime_count#989L]\n",
      "      +- Aggregate [COMM#514, year#816, TotalPopulation#686L], [COMM#514, year#816, TotalPopulation#686L, count(1) AS annual_crime_count#980L]\n",
      "         +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "            :- Project [DR_NO#759, DateRptd#760, DATEOCC#761, TIMEOCC#762, AREA#763, AREANAME#764, RptDistNo#765, Part#766, CrmCd#767, Crm Cd Desc#768, Mocodes#769, Vict Age#770, VictSex#771, VictDescent#772, PremisCd#773, PremisDesc#774, WeaponUsedCd#775, WeaponDesc#776, Status#777, Status Desc#778, CrmCd1#779, CrmCd2#780, CrmCd3#781, CrmCd4#782, ... 6 more fields]\n",
      "            :  +- Filter year#816 IN (2020,2021)\n",
      "            :     +- Project [DR_NO#759, DateRptd#760, DATEOCC#761, TIMEOCC#762, AREA#763, AREANAME#764, RptDistNo#765, Part#766, CrmCd#767, Crm Cd Desc#768, Mocodes#769, Vict Age#770, VictSex#771, VictDescent#772, PremisCd#773, PremisDesc#774, WeaponUsedCd#775, WeaponDesc#776, Status#777, Status Desc#778, CrmCd1#779, CrmCd2#780, CrmCd3#781, CrmCd4#782, ... 5 more fields]\n",
      "            :        +- Filter (NOT (LAT#785 = 0.0) AND NOT (LON#786 = 0.0))\n",
      "            :           +- Relation [DR_NO#759,DateRptd#760,DATEOCC#761,TIMEOCC#762,AREA#763,AREANAME#764,RptDistNo#765,Part#766,CrmCd#767,Crm Cd Desc#768,Mocodes#769,Vict Age#770,VictSex#771,VictDescent#772,PremisCd#773,PremisDesc#774,WeaponUsedCd#775,WeaponDesc#776,Status#777,Status Desc#778,CrmCd1#779,CrmCd2#780,CrmCd3#781,CrmCd4#782,... 4 more fields] csv\n",
      "            +- Project [COMM#514, geometry#684, TotalPopulation#686L]\n",
      "               +- Filter (((isnotnull(COMM#514) AND NOT (COMM#514 = )) AND NOT (COMM#514 = NULL)) AND (TotalPopulation#686L > cast(0 as bigint)))\n",
      "                  +- Aggregate [COMM#514], [COMM#514, st_union_aggr(geometry#500, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@176c6e3, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, Some(ST_Union_Aggr)) AS geometry#684, first(POP20#526L, false) AS TotalPopulation#686L]\n",
      "                     +- Filter (CITY#510 = Los Angeles)\n",
      "                        +- Project [properties#501.BG20 AS BG20#506, properties#501.BG20FIP_CURRENT AS BG20FIP_CURRENT#507, properties#501.BGFIP20 AS BGFIP20#508, properties#501.CB20 AS CB20#509, properties#501.CITY AS CITY#510, properties#501.CITYCOMM AS CITYCOMM#511, properties#501.CITYCOMM_CURRENT AS CITYCOMM_CURRENT#512, properties#501.CITY_CURRENT AS CITY_CURRENT#513, properties#501.COMM AS COMM#514, properties#501.COMM_CURRENT AS COMM_CURRENT#515, properties#501.COUNTY AS COUNTY#516, properties#501.CT20 AS CT20#517, properties#501.CTCB20 AS CTCB20#518, properties#501.FEAT_TYPE AS FEAT_TYPE#519, properties#501.FIP20 AS FIP20#520, properties#501.FIP_CURRENT AS FIP_CURRENT#521, properties#501.HD22 AS HD22#522L, properties#501.HD_NAME AS HD_NAME#523, properties#501.HOUSING20 AS HOUSING20#524L, properties#501.OBJECTID AS OBJECTID#525L, properties#501.POP20 AS POP20#526L, properties#501.SPA22 AS SPA22#527L, properties#501.SPA_NAME AS SPA_NAME#528, properties#501.SUP21 AS SUP21#529, ... 6 more fields]\n",
      "                           +- Project [features#497.geometry AS geometry#500, features#497.properties AS properties#501, features#497.type AS type#502]\n",
      "                              +- Project [features#497]\n",
      "                                 +- Generate explode(features#489), false, [features#497]\n",
      "                                    +- Relation [crs#488,features#489,name#490,type#491] geojson\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "COMM: string, annual_avg_crime_rate_per_person: double\n",
      "Aggregate [COMM#514], [COMM#514, avg(crime_rate_per_person#994) AS annual_avg_crime_rate_per_person#1006]\n",
      "+- Project [COMM#514, year#816, TotalPopulation#686L, annual_crime_count#989L, (cast(annual_crime_count#989L as double) / cast(TotalPopulation#686L as double)) AS crime_rate_per_person#994]\n",
      "   +- Project [COMM#514, year#816, TotalPopulation#686L, coalesce(annual_crime_count#980L, cast(0 as bigint)) AS annual_crime_count#989L]\n",
      "      +- Aggregate [COMM#514, year#816, TotalPopulation#686L], [COMM#514, year#816, TotalPopulation#686L, count(1) AS annual_crime_count#980L]\n",
      "         +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "            :- Project [DR_NO#759, DateRptd#760, DATEOCC#761, TIMEOCC#762, AREA#763, AREANAME#764, RptDistNo#765, Part#766, CrmCd#767, Crm Cd Desc#768, Mocodes#769, Vict Age#770, VictSex#771, VictDescent#772, PremisCd#773, PremisDesc#774, WeaponUsedCd#775, WeaponDesc#776, Status#777, Status Desc#778, CrmCd1#779, CrmCd2#780, CrmCd3#781, CrmCd4#782, ... 6 more fields]\n",
      "            :  +- Filter year#816 IN (2020,2021)\n",
      "            :     +- Project [DR_NO#759, DateRptd#760, DATEOCC#761, TIMEOCC#762, AREA#763, AREANAME#764, RptDistNo#765, Part#766, CrmCd#767, Crm Cd Desc#768, Mocodes#769, Vict Age#770, VictSex#771, VictDescent#772, PremisCd#773, PremisDesc#774, WeaponUsedCd#775, WeaponDesc#776, Status#777, Status Desc#778, CrmCd1#779, CrmCd2#780, CrmCd3#781, CrmCd4#782, ... 5 more fields]\n",
      "            :        +- Filter (NOT (LAT#785 = 0.0) AND NOT (LON#786 = 0.0))\n",
      "            :           +- Relation [DR_NO#759,DateRptd#760,DATEOCC#761,TIMEOCC#762,AREA#763,AREANAME#764,RptDistNo#765,Part#766,CrmCd#767,Crm Cd Desc#768,Mocodes#769,Vict Age#770,VictSex#771,VictDescent#772,PremisCd#773,PremisDesc#774,WeaponUsedCd#775,WeaponDesc#776,Status#777,Status Desc#778,CrmCd1#779,CrmCd2#780,CrmCd3#781,CrmCd4#782,... 4 more fields] csv\n",
      "            +- Project [COMM#514, geometry#684, TotalPopulation#686L]\n",
      "               +- Filter (((isnotnull(COMM#514) AND NOT (COMM#514 = )) AND NOT (COMM#514 = NULL)) AND (TotalPopulation#686L > cast(0 as bigint)))\n",
      "                  +- Aggregate [COMM#514], [COMM#514, st_union_aggr(geometry#500, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@176c6e3, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, Some(ST_Union_Aggr)) AS geometry#684, first(POP20#526L, false) AS TotalPopulation#686L]\n",
      "                     +- Filter (CITY#510 = Los Angeles)\n",
      "                        +- Project [properties#501.BG20 AS BG20#506, properties#501.BG20FIP_CURRENT AS BG20FIP_CURRENT#507, properties#501.BGFIP20 AS BGFIP20#508, properties#501.CB20 AS CB20#509, properties#501.CITY AS CITY#510, properties#501.CITYCOMM AS CITYCOMM#511, properties#501.CITYCOMM_CURRENT AS CITYCOMM_CURRENT#512, properties#501.CITY_CURRENT AS CITY_CURRENT#513, properties#501.COMM AS COMM#514, properties#501.COMM_CURRENT AS COMM_CURRENT#515, properties#501.COUNTY AS COUNTY#516, properties#501.CT20 AS CT20#517, properties#501.CTCB20 AS CTCB20#518, properties#501.FEAT_TYPE AS FEAT_TYPE#519, properties#501.FIP20 AS FIP20#520, properties#501.FIP_CURRENT AS FIP_CURRENT#521, properties#501.HD22 AS HD22#522L, properties#501.HD_NAME AS HD_NAME#523, properties#501.HOUSING20 AS HOUSING20#524L, properties#501.OBJECTID AS OBJECTID#525L, properties#501.POP20 AS POP20#526L, properties#501.SPA22 AS SPA22#527L, properties#501.SPA_NAME AS SPA_NAME#528, properties#501.SUP21 AS SUP21#529, ... 6 more fields]\n",
      "                           +- Project [features#497.geometry AS geometry#500, features#497.properties AS properties#501, features#497.type AS type#502]\n",
      "                              +- Project [features#497]\n",
      "                                 +- Generate explode(features#489), false, [features#497]\n",
      "                                    +- Relation [crs#488,features#489,name#490,type#491] geojson\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [COMM#514], [COMM#514, avg(crime_rate_per_person#994) AS annual_avg_crime_rate_per_person#1006]\n",
      "+- Aggregate [COMM#514, year#816, TotalPopulation#686L], [COMM#514, (cast(count(1) as double) / cast(TotalPopulation#686L as double)) AS crime_rate_per_person#994]\n",
      "   +- Project [year#816, COMM#514, TotalPopulation#686L]\n",
      "      +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "         :- Project [year(cast(DATEOCC#761 as date)) AS year#816,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#846]\n",
      "         :  +- Filter (((isnotnull(LAT#785) AND isnotnull(LON#786)) AND ((NOT (LAT#785 = 0.0) AND NOT (LON#786 = 0.0)) AND year(cast(DATEOCC#761 as date)) IN (2020,2021))) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "         :     +- Relation [DR_NO#759,DateRptd#760,DATEOCC#761,TIMEOCC#762,AREA#763,AREANAME#764,RptDistNo#765,Part#766,CrmCd#767,Crm Cd Desc#768,Mocodes#769,Vict Age#770,VictSex#771,VictDescent#772,PremisCd#773,PremisDesc#774,WeaponUsedCd#775,WeaponDesc#776,Status#777,Status Desc#778,CrmCd1#779,CrmCd2#780,CrmCd3#781,CrmCd4#782,... 4 more fields] csv\n",
      "         +- Filter ((isnotnull(TotalPopulation#686L) AND (TotalPopulation#686L > 0)) AND isnotnull(geometry#684))\n",
      "            +- Aggregate [COMM#514], [COMM#514, st_union_aggr(geometry#500, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@176c6e3, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, Some(ST_Union_Aggr)) AS geometry#684, first(POP20#526L, false) AS TotalPopulation#686L]\n",
      "               +- Project [features#497.properties.COMM AS COMM#514, features#497.properties.POP20 AS POP20#526L, features#497.geometry AS geometry#500]\n",
      "                  +- Filter (isnotnull(features#497.properties.CITY) AND ((features#497.properties.CITY = Los Angeles) AND ((isnotnull(features#497.properties.COMM) AND NOT (features#497.properties.COMM = )) AND NOT (features#497.properties.COMM = NULL))))\n",
      "                     +- Generate explode(features#489), [0], false, [features#497]\n",
      "                        +- Project [features#489]\n",
      "                           +- Filter ((size(features#489, true) > 0) AND isnotnull(features#489))\n",
      "                              +- Relation [crs#488,features#489,name#490,type#491] geojson\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[COMM#514], functions=[avg(crime_rate_per_person#994)], output=[COMM#514, annual_avg_crime_rate_per_person#1006], schema specialized)\n",
      "   +- Exchange hashpartitioning(COMM#514, 1000), ENSURE_REQUIREMENTS, [plan_id=3838]\n",
      "      +- HashAggregate(keys=[COMM#514], functions=[partial_avg(crime_rate_per_person#994)], output=[COMM#514, sum#1175, count#1176L], schema specialized)\n",
      "         +- HashAggregate(keys=[COMM#514, year#816, TotalPopulation#686L], functions=[count(1)], output=[COMM#514, crime_rate_per_person#994], schema specialized)\n",
      "            +- Exchange hashpartitioning(COMM#514, year#816, TotalPopulation#686L, 1000), ENSURE_REQUIREMENTS, [plan_id=3834]\n",
      "               +- HashAggregate(keys=[COMM#514, year#816, TotalPopulation#686L], functions=[partial_count(1)], output=[COMM#514, year#816, TotalPopulation#686L, count#1178L], schema specialized)\n",
      "                  +- Project [year#816, COMM#514, TotalPopulation#686L]\n",
      "                     +- RangeJoin crime_point#846: geometry, geometry#684: geometry, WITHIN\n",
      "                        :- Project [year(cast(DATEOCC#761 as date)) AS year#816,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#846]\n",
      "                        :  +- Filter (((((isnotnull(LAT#785) AND isnotnull(LON#786)) AND NOT (LAT#785 = 0.0)) AND NOT (LON#786 = 0.0)) AND year(cast(DATEOCC#761 as date)) IN (2020,2021)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "                        :     +- FileScan csv [DATEOCC#761,LAT#785,LON#786] Batched: false, DataFilters: [isnotnull(LAT#785), isnotnull(LON#786), NOT (LAT#785 = 0.0), NOT (LON#786 = 0.0), year(cast(DATE..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON), Not(EqualTo(LAT,0.0)), Not(EqualTo(LON,0.0))], ReadSchema: struct<DATEOCC:timestamp,LAT:double,LON:double>\n",
      "                        +- Filter ((isnotnull(TotalPopulation#686L) AND (TotalPopulation#686L > 0)) AND isnotnull(geometry#684))\n",
      "                           +- ObjectHashAggregate(keys=[COMM#514], functions=[st_union_aggr(geometry#500, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@176c6e3, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, Some(ST_Union_Aggr)), first(POP20#526L, false)], output=[COMM#514, geometry#684, TotalPopulation#686L])\n",
      "                              +- Exchange hashpartitioning(COMM#514, 1000), ENSURE_REQUIREMENTS, [plan_id=3827]\n",
      "                                 +- ObjectHashAggregate(keys=[COMM#514], functions=[partial_st_union_aggr(geometry#500, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@176c6e3, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, Some(ST_Union_Aggr)), partial_first(POP20#526L, false)], output=[COMM#514, buf#1180, first#1183L, valueSet#1184])\n",
      "                                    +- Project [features#497.properties.COMM AS COMM#514, features#497.properties.POP20 AS POP20#526L, features#497.geometry AS geometry#500]\n",
      "                                       +- Filter (isnotnull(features#497.properties.CITY) AND ((features#497.properties.CITY = Los Angeles) AND ((isnotnull(features#497.properties.COMM) AND NOT (features#497.properties.COMM = )) AND NOT (features#497.properties.COMM = NULL))))\n",
      "                                          +- Generate explode(features#489), false, [features#497]\n",
      "                                             +- Filter ((size(features#489, true) > 0) AND isnotnull(features#489))\n",
      "                                                +- FileScan geojson [features#489] Batched: false, DataFilters: [(size(features#489, true) > 0), isnotnull(features#489)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG20:string,BG20FIP_CURRENT:string..."
     ]
    }
   ],
   "source": [
    "crime_per_comm.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97a2b6f0-3f74-4394-b0e6-d9dcf67497df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c346babfe1e469db7538fe51e58ebb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(Inner, [COMM])\n",
      ":- Aggregate [COMM#514], [COMM#514, avg(crime_rate_per_person#994) AS annual_avg_crime_rate_per_person#1006]\n",
      ":  +- Project [COMM#514, year#816, TotalPopulation#686L, annual_crime_count#989L, (cast(annual_crime_count#989L as double) / cast(TotalPopulation#686L as double)) AS crime_rate_per_person#994]\n",
      ":     +- Project [COMM#514, year#816, TotalPopulation#686L, coalesce(annual_crime_count#980L, cast(0 as bigint)) AS annual_crime_count#989L]\n",
      ":        +- Aggregate [COMM#514, year#816, TotalPopulation#686L], [COMM#514, year#816, TotalPopulation#686L, count(1) AS annual_crime_count#980L]\n",
      ":           +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      ":              :- Project [DR_NO#759, DateRptd#760, DATEOCC#761, TIMEOCC#762, AREA#763, AREANAME#764, RptDistNo#765, Part#766, CrmCd#767, Crm Cd Desc#768, Mocodes#769, Vict Age#770, VictSex#771, VictDescent#772, PremisCd#773, PremisDesc#774, WeaponUsedCd#775, WeaponDesc#776, Status#777, Status Desc#778, CrmCd1#779, CrmCd2#780, CrmCd3#781, CrmCd4#782, ... 6 more fields]\n",
      ":              :  +- Filter year#816 IN (2020,2021)\n",
      ":              :     +- Project [DR_NO#759, DateRptd#760, DATEOCC#761, TIMEOCC#762, AREA#763, AREANAME#764, RptDistNo#765, Part#766, CrmCd#767, Crm Cd Desc#768, Mocodes#769, Vict Age#770, VictSex#771, VictDescent#772, PremisCd#773, PremisDesc#774, WeaponUsedCd#775, WeaponDesc#776, Status#777, Status Desc#778, CrmCd1#779, CrmCd2#780, CrmCd3#781, CrmCd4#782, ... 5 more fields]\n",
      ":              :        +- Filter (NOT (LAT#785 = 0.0) AND NOT (LON#786 = 0.0))\n",
      ":              :           +- Relation [DR_NO#759,DateRptd#760,DATEOCC#761,TIMEOCC#762,AREA#763,AREANAME#764,RptDistNo#765,Part#766,CrmCd#767,Crm Cd Desc#768,Mocodes#769,Vict Age#770,VictSex#771,VictDescent#772,PremisCd#773,PremisDesc#774,WeaponUsedCd#775,WeaponDesc#776,Status#777,Status Desc#778,CrmCd1#779,CrmCd2#780,CrmCd3#781,CrmCd4#782,... 4 more fields] csv\n",
      ":              +- Project [COMM#514, geometry#684, TotalPopulation#686L]\n",
      ":                 +- Filter (((isnotnull(COMM#514) AND NOT (COMM#514 = )) AND NOT (COMM#514 = NULL)) AND (TotalPopulation#686L > cast(0 as bigint)))\n",
      ":                    +- Aggregate [COMM#514], [COMM#514, st_union_aggr(geometry#500, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@176c6e3, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, Some(ST_Union_Aggr)) AS geometry#684, first(POP20#526L, false) AS TotalPopulation#686L]\n",
      ":                       +- Filter (CITY#510 = Los Angeles)\n",
      ":                          +- Project [properties#501.BG20 AS BG20#506, properties#501.BG20FIP_CURRENT AS BG20FIP_CURRENT#507, properties#501.BGFIP20 AS BGFIP20#508, properties#501.CB20 AS CB20#509, properties#501.CITY AS CITY#510, properties#501.CITYCOMM AS CITYCOMM#511, properties#501.CITYCOMM_CURRENT AS CITYCOMM_CURRENT#512, properties#501.CITY_CURRENT AS CITY_CURRENT#513, properties#501.COMM AS COMM#514, properties#501.COMM_CURRENT AS COMM_CURRENT#515, properties#501.COUNTY AS COUNTY#516, properties#501.CT20 AS CT20#517, properties#501.CTCB20 AS CTCB20#518, properties#501.FEAT_TYPE AS FEAT_TYPE#519, properties#501.FIP20 AS FIP20#520, properties#501.FIP_CURRENT AS FIP_CURRENT#521, properties#501.HD22 AS HD22#522L, properties#501.HD_NAME AS HD_NAME#523, properties#501.HOUSING20 AS HOUSING20#524L, properties#501.OBJECTID AS OBJECTID#525L, properties#501.POP20 AS POP20#526L, properties#501.SPA22 AS SPA22#527L, properties#501.SPA_NAME AS SPA_NAME#528, properties#501.SUP21 AS SUP21#529, ... 6 more fields]\n",
      ":                             +- Project [features#497.geometry AS geometry#500, features#497.properties AS properties#501, features#497.type AS type#502]\n",
      ":                                +- Project [features#497]\n",
      ":                                   +- Generate explode(features#489), false, [features#497]\n",
      ":                                      +- Relation [crs#488,features#489,name#490,type#491] geojson\n",
      "+- Aggregate [COMM#1021], [COMM#1021, avg(Income#735) AS avg_income#756]\n",
      "   +- Join Inner, (ZipCode#729 = ZCTA20#1041)\n",
      "      :- Project [ZipCode#729, Income#735]\n",
      "      :  +- Filter (((isnotnull(ZipCode#729) AND NOT (ZipCode#729 = )) AND NOT (ZipCode#729 = NULL)) AND isnotnull(Income#735))\n",
      "      :     +- Project [Zip Code#723, Community#724, Estimated Median Income#725, ZipCode#729, cast(regexp_replace(Estimated Median Income#725, [$,], , 1) as float) AS Income#735]\n",
      "      :        +- Filter RLIKE(Community#724, (?i)Los Angeles)\n",
      "      :           +- Project [Zip Code#723, Community#724, Estimated Median Income#725, trim(Zip Code#723, None) AS ZipCode#729]\n",
      "      :              +- Relation [Zip Code#723,Community#724,Estimated Median Income#725] csv\n",
      "      +- Deduplicate [ZCTA20#1041, COMM#1021]\n",
      "         +- Filter ((isnotnull(ZCTA20#1041) AND NOT (ZCTA20#1041 = )) AND NOT (ZCTA20#1041 = NULL))\n",
      "            +- Project [ZCTA20#1041, COMM#1021]\n",
      "               +- Filter (CITY#1017 = Los Angeles)\n",
      "                  +- Project [properties#501.BG20 AS BG20#1013, properties#501.BG20FIP_CURRENT AS BG20FIP_CURRENT#1014, properties#501.BGFIP20 AS BGFIP20#1015, properties#501.CB20 AS CB20#1016, properties#501.CITY AS CITY#1017, properties#501.CITYCOMM AS CITYCOMM#1018, properties#501.CITYCOMM_CURRENT AS CITYCOMM_CURRENT#1019, properties#501.CITY_CURRENT AS CITY_CURRENT#1020, properties#501.COMM AS COMM#1021, properties#501.COMM_CURRENT AS COMM_CURRENT#1022, properties#501.COUNTY AS COUNTY#1023, properties#501.CT20 AS CT20#1024, properties#501.CTCB20 AS CTCB20#1025, properties#501.FEAT_TYPE AS FEAT_TYPE#1026, properties#501.FIP20 AS FIP20#1027, properties#501.FIP_CURRENT AS FIP_CURRENT#1028, properties#501.HD22 AS HD22#1029L, properties#501.HD_NAME AS HD_NAME#1030, properties#501.HOUSING20 AS HOUSING20#1031L, properties#501.OBJECTID AS OBJECTID#1032L, properties#501.POP20 AS POP20#1033L, properties#501.SPA22 AS SPA22#1034L, properties#501.SPA_NAME AS SPA_NAME#1035, properties#501.SUP21 AS SUP21#1036, ... 6 more fields]\n",
      "                     +- Project [features#497.geometry AS geometry#500, features#497.properties AS properties#501, features#497.type AS type#502]\n",
      "                        +- Project [features#497]\n",
      "                           +- Generate explode(features#1010), false, [features#497]\n",
      "                              +- Relation [crs#1009,features#1010,name#1011,type#1012] geojson\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "COMM: string, annual_avg_crime_rate_per_person: double, avg_income: double\n",
      "Project [COMM#514, annual_avg_crime_rate_per_person#1006, avg_income#756]\n",
      "+- Join Inner, (COMM#514 = COMM#1021)\n",
      "   :- Aggregate [COMM#514], [COMM#514, avg(crime_rate_per_person#994) AS annual_avg_crime_rate_per_person#1006]\n",
      "   :  +- Project [COMM#514, year#816, TotalPopulation#686L, annual_crime_count#989L, (cast(annual_crime_count#989L as double) / cast(TotalPopulation#686L as double)) AS crime_rate_per_person#994]\n",
      "   :     +- Project [COMM#514, year#816, TotalPopulation#686L, coalesce(annual_crime_count#980L, cast(0 as bigint)) AS annual_crime_count#989L]\n",
      "   :        +- Aggregate [COMM#514, year#816, TotalPopulation#686L], [COMM#514, year#816, TotalPopulation#686L, count(1) AS annual_crime_count#980L]\n",
      "   :           +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "   :              :- Project [DR_NO#759, DateRptd#760, DATEOCC#761, TIMEOCC#762, AREA#763, AREANAME#764, RptDistNo#765, Part#766, CrmCd#767, Crm Cd Desc#768, Mocodes#769, Vict Age#770, VictSex#771, VictDescent#772, PremisCd#773, PremisDesc#774, WeaponUsedCd#775, WeaponDesc#776, Status#777, Status Desc#778, CrmCd1#779, CrmCd2#780, CrmCd3#781, CrmCd4#782, ... 6 more fields]\n",
      "   :              :  +- Filter year#816 IN (2020,2021)\n",
      "   :              :     +- Project [DR_NO#759, DateRptd#760, DATEOCC#761, TIMEOCC#762, AREA#763, AREANAME#764, RptDistNo#765, Part#766, CrmCd#767, Crm Cd Desc#768, Mocodes#769, Vict Age#770, VictSex#771, VictDescent#772, PremisCd#773, PremisDesc#774, WeaponUsedCd#775, WeaponDesc#776, Status#777, Status Desc#778, CrmCd1#779, CrmCd2#780, CrmCd3#781, CrmCd4#782, ... 5 more fields]\n",
      "   :              :        +- Filter (NOT (LAT#785 = 0.0) AND NOT (LON#786 = 0.0))\n",
      "   :              :           +- Relation [DR_NO#759,DateRptd#760,DATEOCC#761,TIMEOCC#762,AREA#763,AREANAME#764,RptDistNo#765,Part#766,CrmCd#767,Crm Cd Desc#768,Mocodes#769,Vict Age#770,VictSex#771,VictDescent#772,PremisCd#773,PremisDesc#774,WeaponUsedCd#775,WeaponDesc#776,Status#777,Status Desc#778,CrmCd1#779,CrmCd2#780,CrmCd3#781,CrmCd4#782,... 4 more fields] csv\n",
      "   :              +- Project [COMM#514, geometry#684, TotalPopulation#686L]\n",
      "   :                 +- Filter (((isnotnull(COMM#514) AND NOT (COMM#514 = )) AND NOT (COMM#514 = NULL)) AND (TotalPopulation#686L > cast(0 as bigint)))\n",
      "   :                    +- Aggregate [COMM#514], [COMM#514, st_union_aggr(geometry#500, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@176c6e3, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, Some(ST_Union_Aggr)) AS geometry#684, first(POP20#526L, false) AS TotalPopulation#686L]\n",
      "   :                       +- Filter (CITY#510 = Los Angeles)\n",
      "   :                          +- Project [properties#501.BG20 AS BG20#506, properties#501.BG20FIP_CURRENT AS BG20FIP_CURRENT#507, properties#501.BGFIP20 AS BGFIP20#508, properties#501.CB20 AS CB20#509, properties#501.CITY AS CITY#510, properties#501.CITYCOMM AS CITYCOMM#511, properties#501.CITYCOMM_CURRENT AS CITYCOMM_CURRENT#512, properties#501.CITY_CURRENT AS CITY_CURRENT#513, properties#501.COMM AS COMM#514, properties#501.COMM_CURRENT AS COMM_CURRENT#515, properties#501.COUNTY AS COUNTY#516, properties#501.CT20 AS CT20#517, properties#501.CTCB20 AS CTCB20#518, properties#501.FEAT_TYPE AS FEAT_TYPE#519, properties#501.FIP20 AS FIP20#520, properties#501.FIP_CURRENT AS FIP_CURRENT#521, properties#501.HD22 AS HD22#522L, properties#501.HD_NAME AS HD_NAME#523, properties#501.HOUSING20 AS HOUSING20#524L, properties#501.OBJECTID AS OBJECTID#525L, properties#501.POP20 AS POP20#526L, properties#501.SPA22 AS SPA22#527L, properties#501.SPA_NAME AS SPA_NAME#528, properties#501.SUP21 AS SUP21#529, ... 6 more fields]\n",
      "   :                             +- Project [features#497.geometry AS geometry#500, features#497.properties AS properties#501, features#497.type AS type#502]\n",
      "   :                                +- Project [features#497]\n",
      "   :                                   +- Generate explode(features#489), false, [features#497]\n",
      "   :                                      +- Relation [crs#488,features#489,name#490,type#491] geojson\n",
      "   +- Aggregate [COMM#1021], [COMM#1021, avg(Income#735) AS avg_income#756]\n",
      "      +- Join Inner, (ZipCode#729 = ZCTA20#1041)\n",
      "         :- Project [ZipCode#729, Income#735]\n",
      "         :  +- Filter (((isnotnull(ZipCode#729) AND NOT (ZipCode#729 = )) AND NOT (ZipCode#729 = NULL)) AND isnotnull(Income#735))\n",
      "         :     +- Project [Zip Code#723, Community#724, Estimated Median Income#725, ZipCode#729, cast(regexp_replace(Estimated Median Income#725, [$,], , 1) as float) AS Income#735]\n",
      "         :        +- Filter RLIKE(Community#724, (?i)Los Angeles)\n",
      "         :           +- Project [Zip Code#723, Community#724, Estimated Median Income#725, trim(Zip Code#723, None) AS ZipCode#729]\n",
      "         :              +- Relation [Zip Code#723,Community#724,Estimated Median Income#725] csv\n",
      "         +- Deduplicate [ZCTA20#1041, COMM#1021]\n",
      "            +- Filter ((isnotnull(ZCTA20#1041) AND NOT (ZCTA20#1041 = )) AND NOT (ZCTA20#1041 = NULL))\n",
      "               +- Project [ZCTA20#1041, COMM#1021]\n",
      "                  +- Filter (CITY#1017 = Los Angeles)\n",
      "                     +- Project [properties#501.BG20 AS BG20#1013, properties#501.BG20FIP_CURRENT AS BG20FIP_CURRENT#1014, properties#501.BGFIP20 AS BGFIP20#1015, properties#501.CB20 AS CB20#1016, properties#501.CITY AS CITY#1017, properties#501.CITYCOMM AS CITYCOMM#1018, properties#501.CITYCOMM_CURRENT AS CITYCOMM_CURRENT#1019, properties#501.CITY_CURRENT AS CITY_CURRENT#1020, properties#501.COMM AS COMM#1021, properties#501.COMM_CURRENT AS COMM_CURRENT#1022, properties#501.COUNTY AS COUNTY#1023, properties#501.CT20 AS CT20#1024, properties#501.CTCB20 AS CTCB20#1025, properties#501.FEAT_TYPE AS FEAT_TYPE#1026, properties#501.FIP20 AS FIP20#1027, properties#501.FIP_CURRENT AS FIP_CURRENT#1028, properties#501.HD22 AS HD22#1029L, properties#501.HD_NAME AS HD_NAME#1030, properties#501.HOUSING20 AS HOUSING20#1031L, properties#501.OBJECTID AS OBJECTID#1032L, properties#501.POP20 AS POP20#1033L, properties#501.SPA22 AS SPA22#1034L, properties#501.SPA_NAME AS SPA_NAME#1035, properties#501.SUP21 AS SUP21#1036, ... 6 more fields]\n",
      "                        +- Project [features#497.geometry AS geometry#500, features#497.properties AS properties#501, features#497.type AS type#502]\n",
      "                           +- Project [features#497]\n",
      "                              +- Generate explode(features#1010), false, [features#497]\n",
      "                                 +- Relation [crs#1009,features#1010,name#1011,type#1012] geojson\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [COMM#514, annual_avg_crime_rate_per_person#1006, avg_income#756]\n",
      "+- Join Inner, (COMM#514 = COMM#1021)\n",
      "   :- Aggregate [COMM#514], [COMM#514, avg(crime_rate_per_person#994) AS annual_avg_crime_rate_per_person#1006]\n",
      "   :  +- Aggregate [COMM#514, year#816, TotalPopulation#686L], [COMM#514, (cast(count(1) as double) / cast(TotalPopulation#686L as double)) AS crime_rate_per_person#994]\n",
      "   :     +- Project [year#816, COMM#514, TotalPopulation#686L]\n",
      "   :        +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "   :           :- Project [year(cast(DATEOCC#761 as date)) AS year#816,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#846]\n",
      "   :           :  +- Filter (((isnotnull(LAT#785) AND isnotnull(LON#786)) AND ((NOT (LAT#785 = 0.0) AND NOT (LON#786 = 0.0)) AND year(cast(DATEOCC#761 as date)) IN (2020,2021))) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "   :           :     +- Relation [DR_NO#759,DateRptd#760,DATEOCC#761,TIMEOCC#762,AREA#763,AREANAME#764,RptDistNo#765,Part#766,CrmCd#767,Crm Cd Desc#768,Mocodes#769,Vict Age#770,VictSex#771,VictDescent#772,PremisCd#773,PremisDesc#774,WeaponUsedCd#775,WeaponDesc#776,Status#777,Status Desc#778,CrmCd1#779,CrmCd2#780,CrmCd3#781,CrmCd4#782,... 4 more fields] csv\n",
      "   :           +- Filter ((isnotnull(TotalPopulation#686L) AND (TotalPopulation#686L > 0)) AND isnotnull(geometry#684))\n",
      "   :              +- Aggregate [COMM#514], [COMM#514, st_union_aggr(geometry#500, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@176c6e3, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, Some(ST_Union_Aggr)) AS geometry#684, first(POP20#526L, false) AS TotalPopulation#686L]\n",
      "   :                 +- Project [features#497.properties.COMM AS COMM#514, features#497.properties.POP20 AS POP20#526L, features#497.geometry AS geometry#500]\n",
      "   :                    +- Filter (((((isnotnull(features#497.properties.CITY) AND (features#497.properties.CITY = Los Angeles)) AND isnotnull(features#497.properties.COMM)) AND NOT (features#497.properties.COMM = )) AND NOT (features#497.properties.COMM = NULL)) AND bloomfilter#1976 of [COMM#1021] filtering [features#497.properties.COMM])\n",
      "   :                       :  +- Aggregate [ZCTA20#1041, COMM#1021], [ZCTA20#1041, COMM#1021]\n",
      "   :                       :     +- Project [features#497.properties.ZCTA20 AS ZCTA20#1041, features#497.properties.COMM AS COMM#1021]\n",
      "   :                       :        +- Filter ((isnotnull(features#497.properties.CITY) AND ((features#497.properties.CITY = Los Angeles) AND ((isnotnull(features#497.properties.ZCTA20) AND NOT (features#497.properties.ZCTA20 = )) AND NOT (features#497.properties.ZCTA20 = NULL)))) AND ((NOT (features#497.properties.COMM = ) AND NOT (features#497.properties.COMM = NULL)) AND isnotnull(features#497.properties.COMM)))\n",
      "   :                       :           +- Generate explode(features#1010), [0], false, [features#497]\n",
      "   :                       :              +- Project [features#1010]\n",
      "   :                       :                 +- Filter ((size(features#1010, true) > 0) AND isnotnull(features#1010))\n",
      "   :                       :                    +- Relation [crs#1009,features#1010,name#1011,type#1012] geojson\n",
      "   :                       +- Generate explode(features#489), [0], false, [features#497]\n",
      "   :                          +- Project [features#489]\n",
      "   :                             +- Filter ((size(features#489, true) > 0) AND isnotnull(features#489))\n",
      "   :                                +- Relation [crs#488,features#489,name#490,type#491] geojson\n",
      "   +- Aggregate [COMM#1021], [COMM#1021, avg(Income#735) AS avg_income#756]\n",
      "      +- Project [Income#735, COMM#1021]\n",
      "         +- Join Inner, (ZipCode#729 = ZCTA20#1041)\n",
      "            :- Project [trim(Zip Code#723, None) AS ZipCode#729, cast(regexp_replace(Estimated Median Income#725, [$,], , 1) as float) AS Income#735]\n",
      "            :  +- Filter ((isnotnull(Community#724) AND isnotnull(Estimated Median Income#725)) AND (RLIKE(Community#724, (?i)Los Angeles) AND (((isnotnull(trim(Zip Code#723, None)) AND NOT (trim(Zip Code#723, None) = )) AND NOT (trim(Zip Code#723, None) = NULL)) AND isnotnull(cast(regexp_replace(Estimated Median Income#725, [$,], , 1) as float)))))\n",
      "            :     +- Relation [Zip Code#723,Community#724,Estimated Median Income#725] csv\n",
      "            +- Aggregate [ZCTA20#1041, COMM#1021], [ZCTA20#1041, COMM#1021]\n",
      "               +- Project [features#497.properties.ZCTA20 AS ZCTA20#1041, features#497.properties.COMM AS COMM#1021]\n",
      "                  +- Filter ((isnotnull(features#497.properties.CITY) AND ((features#497.properties.CITY = Los Angeles) AND ((isnotnull(features#497.properties.ZCTA20) AND NOT (features#497.properties.ZCTA20 = )) AND NOT (features#497.properties.ZCTA20 = NULL)))) AND ((NOT (features#497.properties.COMM = ) AND NOT (features#497.properties.COMM = NULL)) AND isnotnull(features#497.properties.COMM)))\n",
      "                     +- Generate explode(features#1010), [0], false, [features#497]\n",
      "                        +- Project [features#1010]\n",
      "                           +- Filter ((size(features#1010, true) > 0) AND isnotnull(features#1010))\n",
      "                              +- Relation [crs#1009,features#1010,name#1011,type#1012] geojson\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [COMM#514, annual_avg_crime_rate_per_person#1006, avg_income#756]\n",
      "   +- SortMergeJoin [COMM#514], [COMM#1021], Inner\n",
      "      :- Sort [COMM#514 ASC NULLS FIRST], false, 0\n",
      "      :  +- HashAggregate(keys=[COMM#514], functions=[avg(crime_rate_per_person#994)], output=[COMM#514, annual_avg_crime_rate_per_person#1006], schema specialized)\n",
      "      :     +- Exchange hashpartitioning(COMM#514, 1000), ENSURE_REQUIREMENTS, [plan_id=4155]\n",
      "      :        +- HashAggregate(keys=[COMM#514], functions=[partial_avg(crime_rate_per_person#994)], output=[COMM#514, sum#1175, count#1176L], schema specialized)\n",
      "      :           +- HashAggregate(keys=[COMM#514, year#816, TotalPopulation#686L], functions=[count(1)], output=[COMM#514, crime_rate_per_person#994], schema specialized)\n",
      "      :              +- Exchange hashpartitioning(COMM#514, year#816, TotalPopulation#686L, 1000), ENSURE_REQUIREMENTS, [plan_id=4152]\n",
      "      :                 +- HashAggregate(keys=[COMM#514, year#816, TotalPopulation#686L], functions=[partial_count(1)], output=[COMM#514, year#816, TotalPopulation#686L, count#1178L], schema specialized)\n",
      "      :                    +- Project [year#816, COMM#514, TotalPopulation#686L]\n",
      "      :                       +- RangeJoin crime_point#846: geometry, geometry#684: geometry, WITHIN\n",
      "      :                          :- Project [year(cast(DATEOCC#761 as date)) AS year#816,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#846]\n",
      "      :                          :  +- Filter (((((isnotnull(LAT#785) AND isnotnull(LON#786)) AND NOT (LAT#785 = 0.0)) AND NOT (LON#786 = 0.0)) AND year(cast(DATEOCC#761 as date)) IN (2020,2021)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "      :                          :     +- FileScan csv [DATEOCC#761,LAT#785,LON#786] Batched: false, DataFilters: [isnotnull(LAT#785), isnotnull(LON#786), NOT (LAT#785 = 0.0), NOT (LON#786 = 0.0), year(cast(DATE..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON), Not(EqualTo(LAT,0.0)), Not(EqualTo(LON,0.0))], ReadSchema: struct<DATEOCC:timestamp,LAT:double,LON:double>\n",
      "      :                          +- Filter ((isnotnull(TotalPopulation#686L) AND (TotalPopulation#686L > 0)) AND isnotnull(geometry#684))\n",
      "      :                             +- ObjectHashAggregate(keys=[COMM#514], functions=[st_union_aggr(geometry#500, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@176c6e3, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, Some(ST_Union_Aggr)), first(POP20#526L, false)], output=[COMM#514, geometry#684, TotalPopulation#686L])\n",
      "      :                                +- Exchange hashpartitioning(COMM#514, 1000), ENSURE_REQUIREMENTS, [plan_id=4146]\n",
      "      :                                   +- ObjectHashAggregate(keys=[COMM#514], functions=[partial_st_union_aggr(geometry#500, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@176c6e3, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, Some(ST_Union_Aggr)), partial_first(POP20#526L, false)], output=[COMM#514, buf#1180, first#1183L, valueSet#1184])\n",
      "      :                                      +- Project [features#497.properties.COMM AS COMM#514, features#497.properties.POP20 AS POP20#526L, features#497.geometry AS geometry#500]\n",
      "      :                                         +- Filter ((((isnotnull(features#497.properties.CITY) AND (features#497.properties.CITY = Los Angeles)) AND isnotnull(features#497.properties.COMM)) AND NOT (features#497.properties.COMM = )) AND NOT (features#497.properties.COMM = NULL))\n",
      "      :                                            +- Generate explode(features#489), false, [features#497]\n",
      "      :                                               +- Filter ((size(features#489, true) > 0) AND isnotnull(features#489))\n",
      "      :                                                  +- FileScan geojson [features#489] Batched: false, DataFilters: [(size(features#489, true) > 0), isnotnull(features#489)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG20:string,BG20FIP_CURRENT:string...\n",
      "      +- Sort [COMM#1021 ASC NULLS FIRST], false, 0\n",
      "         +- HashAggregate(keys=[COMM#1021], functions=[avg(Income#735)], output=[COMM#1021, avg_income#756], schema specialized)\n",
      "            +- Exchange hashpartitioning(COMM#1021, 1000), ENSURE_REQUIREMENTS, [plan_id=4118]\n",
      "               +- HashAggregate(keys=[COMM#1021], functions=[partial_avg(Income#735)], output=[COMM#1021, sum#1187, count#1188L], schema specialized)\n",
      "                  +- Project [Income#735, COMM#1021]\n",
      "                     +- BroadcastHashJoin [ZipCode#729], [ZCTA20#1041], Inner, BuildLeft, false\n",
      "                        :- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=4113]\n",
      "                        :  +- Project [trim(Zip Code#723, None) AS ZipCode#729, cast(regexp_replace(Estimated Median Income#725, [$,], , 1) as float) AS Income#735]\n",
      "                        :     +- Filter ((((((isnotnull(Community#724) AND isnotnull(Estimated Median Income#725)) AND RLIKE(Community#724, (?i)Los Angeles)) AND isnotnull(trim(Zip Code#723, None))) AND NOT (trim(Zip Code#723, None) = )) AND NOT (trim(Zip Code#723, None) = NULL)) AND isnotnull(cast(regexp_replace(Estimated Median Income#725, [$,], , 1) as float)))\n",
      "                        :        +- FileScan csv [Zip Code#723,Community#724,Estimated Median Income#725] Batched: false, DataFilters: [isnotnull(Community#724), isnotnull(Estimated Median Income#725), RLIKE(Community#724, (?i)Los A..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_i..., PartitionFilters: [], PushedFilters: [IsNotNull(Community), IsNotNull(Estimated Median Income)], ReadSchema: struct<Zip Code:string,Community:string,Estimated Median Income:string>\n",
      "                        +- HashAggregate(keys=[ZCTA20#1041, COMM#1021], functions=[], output=[ZCTA20#1041, COMM#1021], schema specialized)\n",
      "                           +- Exchange hashpartitioning(ZCTA20#1041, COMM#1021, 1000), ENSURE_REQUIREMENTS, [plan_id=4110]\n",
      "                              +- HashAggregate(keys=[ZCTA20#1041, COMM#1021], functions=[], output=[ZCTA20#1041, COMM#1021], schema specialized)\n",
      "                                 +- Project [features#497.properties.ZCTA20 AS ZCTA20#1041, features#497.properties.COMM AS COMM#1021]\n",
      "                                    +- Filter ((isnotnull(features#497.properties.CITY) AND ((features#497.properties.CITY = Los Angeles) AND ((isnotnull(features#497.properties.ZCTA20) AND NOT (features#497.properties.ZCTA20 = )) AND NOT (features#497.properties.ZCTA20 = NULL)))) AND ((NOT (features#497.properties.COMM = ) AND NOT (features#497.properties.COMM = NULL)) AND isnotnull(features#497.properties.COMM)))\n",
      "                                       +- Generate explode(features#1010), false, [features#497]\n",
      "                                          +- Filter ((size(features#1010, true) > 0) AND isnotnull(features#1010))\n",
      "                                             +- FileScan geojson [features#1010] Batched: false, DataFilters: [(size(features#1010, true) > 0), isnotnull(features#1010)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG20:string,BG20FIP_CURRENT:string..."
     ]
    }
   ],
   "source": [
    "final_df.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ba62709-e636-42a1-923b-0abd782e2537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.sql.catalog.spark_catalog.type': 'hive', 'spark.executor.instances': '8', 'spark.executor.cores': '1', 'spark.executor.memory': '2g'}, 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1564</td><td>application_1765289937462_1550</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1550/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-149.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1550_01_000001/livy\">Link</a></td><td>None</td><td></td></tr><tr><td>1571</td><td>application_1765289937462_1557</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1557/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-154.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1557_01_000002/livy\">Link</a></td><td>None</td><td></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.executor.instances\": \"8\",\n",
    "        \"spark.executor.cores\": \"1\",\n",
    "        \"spark.executor.memory\": \"2g\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a280c41-a61e-4799-af49-0f3f06232794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1572</td><td>application_1765289937462_1558</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-63.eu-central-1.compute.internal:20888/proxy/application_1765289937462_1558/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-250.eu-central-1.compute.internal:8042/node/containerlogs/container_1765289937462_1558_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a0be534224548de881e58634adf57a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce3d18123d2e4ddf8583f83ffdbf2a76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from sedona.spark import SedonaContext\n",
    "\n",
    "# Create a new Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LA Crime Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#  Initialize Sedona (if using spatial operations)\n",
    "sedona = SedonaContext.create(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "601e9f24-f371-417d-a041-6a32a6ff920e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b40d76c702dc42059a345c475c49168f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    StructField, StructType, StringType, IntegerType, \n",
    "    FloatType, DoubleType, DateType, TimestampType  \n",
    ")\n",
    "from pyspark.sql.functions import (\n",
    "    year, when, count, sum, col, row_number, \n",
    "    to_timestamp, regexp_replace, to_date, expr,avg,broadcast,first    \n",
    ")\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, sum, count, expr, coalesce, lit, when,explode,collect_set\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import  to_date, year\n",
    "import time\n",
    "from pyspark.sql.functions import col, regexp_replace, trim\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, expr, year, trim, regexp_replace, first, broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90dbb4d6-3a6f-4fb3-ab47-3e122bd52ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6ad58eeb86546109b221e6b572c435e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fgeo=\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Census_Blocks_2020.geojson\"\n",
    "fincome=\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_income_2021.csv\"\n",
    "fcrime=\"s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_Crime_Data/LA_Crime_Data_2020_2025.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34a6f008-0819-474f-8e8b-fe2ffcae970c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eff104b998f43febc0aa53fb8dd9dc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "crimes_schema = StructType([\n",
    "    StructField(\"DR_NO\", StringType()),\n",
    "    StructField(\"DateRptd\", TimestampType()),\n",
    "    StructField(\"DATEOCC\", TimestampType()),\n",
    "    StructField(\"TIMEOCC\", StringType()),\n",
    "    StructField(\"AREA\", StringType()),\n",
    "    StructField(\"AREANAME\", StringType()),\n",
    "    StructField(\"RptDistNo\", StringType()),\n",
    "    StructField(\"Part\", IntegerType()),\n",
    "    StructField(\"CrmCd\", StringType()),\n",
    "    StructField(\"Crm Cd Desc\", StringType()),\n",
    "    StructField(\"Mocodes\", StringType()),\n",
    "    StructField(\"Vict Age\", StringType()),\n",
    "    StructField(\"VictSex\", StringType()),\n",
    "    StructField(\"VictDescent\", StringType()),\n",
    "    StructField(\"PremisCd\", StringType()),\n",
    "    StructField(\"PremisDesc\", StringType()),\n",
    "    StructField(\"WeaponUsedCd\", StringType()),\n",
    "    StructField(\"WeaponDesc\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"Status Desc\", StringType()),\n",
    "    StructField(\"CrmCd1\", StringType()),\n",
    "    StructField(\"CrmCd2\", StringType()),\n",
    "    StructField(\"CrmCd3\", StringType()),\n",
    "    StructField(\"CrmCd4\", StringType()),\n",
    "    StructField(\"LOCATION\", StringType()),\n",
    "    StructField(\"CrossStreet\", StringType()),\n",
    "    StructField(\"LAT\", DoubleType()),\n",
    "    StructField(\"LON\", DoubleType()),\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e2c3d5d-eb3b-40c9-b532-40f218429715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79ef8c2475a14090bcdc69c101eed709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total correlation: 0.1502766830319662\n",
      "Top 10 richest COMM correlation: -0.3735207290423716\n",
      "Bottom 10 poorest COMM correlation: 0.018338409553787367\n",
      "Elapsed time: 84.02 seconds"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "#Prepare the geojson file\n",
    "blocks_df = sedona.read.format('geojson') \\\n",
    "    .option('multiLine','true').load(fgeo) \\\n",
    "    .selectExpr('explode(features) as features') \\\n",
    "    .select('features.*')\n",
    "\n",
    "flattened_df = blocks_df.select(\n",
    "    [col(f'properties.{col_name}').alias(col_name) for col_name in \\\n",
    "    blocks_df.schema['properties'].dataType.fieldNames()] + ['geometry']) \\\n",
    "    .drop('properties').drop('type')\n",
    "\n",
    "la_comm_df = flattened_df.filter(col(\"CITY\") == \"Los Angeles\") \\\n",
    "    .groupBy(\"COMM\") \\\n",
    "    .agg(\n",
    "        expr(\"ST_Union_Aggr(geometry)\").alias(\"geometry\"),\n",
    "        first(\"POP20\").alias(\"TotalPopulation\")\n",
    "    ) \\\n",
    "    .filter(\n",
    "        (col(\"COMM\").isNotNull()) &\n",
    "        (col(\"COMM\") != \"\") &\n",
    "        (col(\"COMM\") != \"NULL\") &\n",
    "        (col(\"TotalPopulation\") > 0)\n",
    "    )\n",
    "\n",
    "\n",
    "zip_codes_comm = flattened_df.filter(col(\"CITY\") == \"Los Angeles\") \\\n",
    "    .select(\"ZCTA20\", \"COMM\") \\\n",
    "    .filter(\n",
    "        col(\"ZCTA20\").isNotNull() &\n",
    "        (col(\"ZCTA20\") != \"\") &\n",
    "        (col(\"ZCTA20\") != \"NULL\")\n",
    "    ) \\\n",
    "    .dropDuplicates()\n",
    "\n",
    "\n",
    "\n",
    "#  Read CSV with correct delimiter\n",
    "income_df = spark.read.csv(\n",
    "    fincome,\n",
    "    header=True,\n",
    "    sep=';',\n",
    "    quote='\"',\n",
    "    escape='\"',\n",
    "    inferSchema=False\n",
    ")\n",
    "\n",
    "# 2️Trim the Zip Code column and rename\n",
    "income_df = income_df.withColumn(\"ZipCode\", trim(col(\"Zip Code\")))\n",
    "\n",
    "# 3️ Keep only rows where Community mentions Los Angeles\n",
    "income_df = income_df.filter(col(\"Community\").rlike(\"(?i)Los Angeles\"))\n",
    "\n",
    "# Clean Income column\n",
    "income_df = income_df.withColumn(\n",
    "    \"Income\",\n",
    "    regexp_replace(col(\"Estimated Median Income\"), r\"[$,]\", \"\").cast(\"float\")\n",
    ")\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "income_df = income_df.filter(\n",
    "    col(\"ZipCode\").isNotNull() &       # ZipCode not NULL\n",
    "    (col(\"ZipCode\") != \"\") &           # ZipCode not empty\n",
    "    (col(\"ZipCode\") != \"NULL\") &       # ZipCode not the string \"NULL\"\n",
    "    col(\"Income\").isNotNull()          # Income not NULL\n",
    ")\n",
    "\n",
    "#  Keep only ZipCode and Income column\n",
    "income_df = income_df.select(\"ZipCode\", \"Income\")\n",
    "\n",
    "income_with_comm = income_df.join(\n",
    "    zip_codes_comm,\n",
    "    income_df.ZipCode == zip_codes_comm.ZCTA20,\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "income_per_comm = income_with_comm.groupBy(\"COMM\") \\\n",
    "    .agg(F.avg(\"Income\").alias(\"avg_income\"))\n",
    "\n",
    "#Crimes\n",
    "crimes_df = spark.read.csv(\n",
    "    fcrime,\n",
    "    header=True,\n",
    "    schema=crimes_schema,\n",
    "    timestampFormat='yyyy MMM dd hh:mm:ss a',\n",
    "    quote='\"',\n",
    "    escape='\"'\n",
    ")\n",
    "\n",
    "#  Cast Lat/Lon to Double  and Filter Zeros\n",
    "crimes_df = crimes_df.filter((col('LAT') != 0.0) & (col('LON') != 0.0))\n",
    "\n",
    "# Extract Year\n",
    "crimes_df = crimes_df.withColumn(\"year\", year(col(\"DATEOCC\")))\n",
    "\n",
    "# Filter for years 2020 and 2021\n",
    "crimes_df = crimes_df.filter(col(\"year\").isin([2020, 2021]))\n",
    "\n",
    "#  Create Geometry (Requires Apache Sedona)\n",
    "#  ST_Point takes (Longitude, Latitude) -> (X, Y)\n",
    "crimes_geo_df = crimes_df.withColumn(\"crime_point\", expr(\"ST_Point(LON, LAT)\"))\n",
    "\n",
    "# Count crimes per community per year\n",
    "crime_per_comm_year = (\n",
    "    crimes_geo_df\n",
    "    .join(\n",
    "        la_comm_df.select(\"COMM\", \"geometry\", \"TotalPopulation\"),\n",
    "        expr(\"ST_Within(crime_point, geometry)\"),\n",
    "        \"inner\"\n",
    "    )\n",
    "    .groupBy(\"COMM\", \"year\", \"TotalPopulation\")\n",
    "    .agg(F.count(\"*\").alias(\"annual_crime_count\"))\n",
    ")\n",
    "crime_per_comm_year = crime_per_comm_year.fillna({\"annual_crime_count\": 0})\n",
    "\n",
    "\n",
    "# Crime rate per person per year\n",
    "crime_rate_ = crime_per_comm_year.withColumn(\n",
    "    \"crime_rate_per_person\",\n",
    "    col(\"annual_crime_count\") / col(\"TotalPopulation\")\n",
    ")\n",
    "\n",
    "# Annual average crime rate per person\n",
    "crime_per_comm = (\n",
    "    crime_rate_\n",
    "    .groupBy(\"COMM\")\n",
    "    .agg(\n",
    "        F.avg(\"crime_rate_per_person\").alias(\"annual_avg_crime_rate_per_person\")\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "final_df = (\n",
    "    crime_per_comm   # COMM, annual_avg_crime_rate_per_person\n",
    "    .join(\n",
    "        income_per_comm,   # COMM, avg_income\n",
    "        on=\"COMM\",\n",
    "        how=\"inner\"\n",
    "    )\n",
    ")\n",
    "# All areas\n",
    "total_corr = final_df.stat.corr(\"avg_income\", \"annual_avg_crime_rate_per_person\")\n",
    "\n",
    "# Top 10 richest areas\n",
    "top10 = final_df.orderBy(col(\"avg_income\").desc()).limit(10)\n",
    "top10_corr = top10.stat.corr(\"avg_income\", \"annual_avg_crime_rate_per_person\")\n",
    "\n",
    "# Bottom 10 poorest areas\n",
    "bottom10 = final_df.orderBy(col(\"avg_income\").asc()).limit(10)\n",
    "bottom10_corr = bottom10.stat.corr(\"avg_income\", \"annual_avg_crime_rate_per_person\")\n",
    "\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "\n",
    "# Show results\n",
    "\n",
    "print(\"Total correlation:\", total_corr)\n",
    "print(\"Top 10 richest COMM correlation:\", top10_corr)\n",
    "print(\"Bottom 10 poorest COMM correlation:\", bottom10_corr)\n",
    "print(f\"Elapsed time: {elapsed:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "414595fd-f640-4f9f-bfba-ded04148f835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21354795758848929ea44a219c4a2fa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['COMM], ['COMM, avg('Income) AS avg_income#292]\n",
      "+- Join Inner, (ZipCode#265 = ZCTA20#70)\n",
      "   :- Project [ZipCode#265, Income#271]\n",
      "   :  +- Filter (((isnotnull(ZipCode#265) AND NOT (ZipCode#265 = )) AND NOT (ZipCode#265 = NULL)) AND isnotnull(Income#271))\n",
      "   :     +- Project [Zip Code#259, Community#260, Estimated Median Income#261, ZipCode#265, cast(regexp_replace(Estimated Median Income#261, [$,], , 1) as float) AS Income#271]\n",
      "   :        +- Filter RLIKE(Community#260, (?i)Los Angeles)\n",
      "   :           +- Project [Zip Code#259, Community#260, Estimated Median Income#261, trim(Zip Code#259, None) AS ZipCode#265]\n",
      "   :              +- Relation [Zip Code#259,Community#260,Estimated Median Income#261] csv\n",
      "   +- Deduplicate [ZCTA20#70, COMM#50]\n",
      "      +- Filter ((isnotnull(ZCTA20#70) AND NOT (ZCTA20#70 = )) AND NOT (ZCTA20#70 = NULL))\n",
      "         +- Project [ZCTA20#70, COMM#50]\n",
      "            +- Filter (CITY#46 = Los Angeles)\n",
      "               +- Project [properties#37.BG20 AS BG20#42, properties#37.BG20FIP_CURRENT AS BG20FIP_CURRENT#43, properties#37.BGFIP20 AS BGFIP20#44, properties#37.CB20 AS CB20#45, properties#37.CITY AS CITY#46, properties#37.CITYCOMM AS CITYCOMM#47, properties#37.CITYCOMM_CURRENT AS CITYCOMM_CURRENT#48, properties#37.CITY_CURRENT AS CITY_CURRENT#49, properties#37.COMM AS COMM#50, properties#37.COMM_CURRENT AS COMM_CURRENT#51, properties#37.COUNTY AS COUNTY#52, properties#37.CT20 AS CT20#53, properties#37.CTCB20 AS CTCB20#54, properties#37.FEAT_TYPE AS FEAT_TYPE#55, properties#37.FIP20 AS FIP20#56, properties#37.FIP_CURRENT AS FIP_CURRENT#57, properties#37.HD22 AS HD22#58L, properties#37.HD_NAME AS HD_NAME#59, properties#37.HOUSING20 AS HOUSING20#60L, properties#37.OBJECTID AS OBJECTID#61L, properties#37.POP20 AS POP20#62L, properties#37.SPA22 AS SPA22#63L, properties#37.SPA_NAME AS SPA_NAME#64, properties#37.SUP21 AS SUP21#65, ... 6 more fields]\n",
      "                  +- Project [features#33.geometry AS geometry#36, features#33.properties AS properties#37, features#33.type AS type#38]\n",
      "                     +- Project [features#33]\n",
      "                        +- Generate explode(features#25), false, [features#33]\n",
      "                           +- Relation [crs#24,features#25,name#26,type#27] geojson\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "COMM: string, avg_income: double\n",
      "Aggregate [COMM#50], [COMM#50, avg(Income#271) AS avg_income#292]\n",
      "+- Join Inner, (ZipCode#265 = ZCTA20#70)\n",
      "   :- Project [ZipCode#265, Income#271]\n",
      "   :  +- Filter (((isnotnull(ZipCode#265) AND NOT (ZipCode#265 = )) AND NOT (ZipCode#265 = NULL)) AND isnotnull(Income#271))\n",
      "   :     +- Project [Zip Code#259, Community#260, Estimated Median Income#261, ZipCode#265, cast(regexp_replace(Estimated Median Income#261, [$,], , 1) as float) AS Income#271]\n",
      "   :        +- Filter RLIKE(Community#260, (?i)Los Angeles)\n",
      "   :           +- Project [Zip Code#259, Community#260, Estimated Median Income#261, trim(Zip Code#259, None) AS ZipCode#265]\n",
      "   :              +- Relation [Zip Code#259,Community#260,Estimated Median Income#261] csv\n",
      "   +- Deduplicate [ZCTA20#70, COMM#50]\n",
      "      +- Filter ((isnotnull(ZCTA20#70) AND NOT (ZCTA20#70 = )) AND NOT (ZCTA20#70 = NULL))\n",
      "         +- Project [ZCTA20#70, COMM#50]\n",
      "            +- Filter (CITY#46 = Los Angeles)\n",
      "               +- Project [properties#37.BG20 AS BG20#42, properties#37.BG20FIP_CURRENT AS BG20FIP_CURRENT#43, properties#37.BGFIP20 AS BGFIP20#44, properties#37.CB20 AS CB20#45, properties#37.CITY AS CITY#46, properties#37.CITYCOMM AS CITYCOMM#47, properties#37.CITYCOMM_CURRENT AS CITYCOMM_CURRENT#48, properties#37.CITY_CURRENT AS CITY_CURRENT#49, properties#37.COMM AS COMM#50, properties#37.COMM_CURRENT AS COMM_CURRENT#51, properties#37.COUNTY AS COUNTY#52, properties#37.CT20 AS CT20#53, properties#37.CTCB20 AS CTCB20#54, properties#37.FEAT_TYPE AS FEAT_TYPE#55, properties#37.FIP20 AS FIP20#56, properties#37.FIP_CURRENT AS FIP_CURRENT#57, properties#37.HD22 AS HD22#58L, properties#37.HD_NAME AS HD_NAME#59, properties#37.HOUSING20 AS HOUSING20#60L, properties#37.OBJECTID AS OBJECTID#61L, properties#37.POP20 AS POP20#62L, properties#37.SPA22 AS SPA22#63L, properties#37.SPA_NAME AS SPA_NAME#64, properties#37.SUP21 AS SUP21#65, ... 6 more fields]\n",
      "                  +- Project [features#33.geometry AS geometry#36, features#33.properties AS properties#37, features#33.type AS type#38]\n",
      "                     +- Project [features#33]\n",
      "                        +- Generate explode(features#25), false, [features#33]\n",
      "                           +- Relation [crs#24,features#25,name#26,type#27] geojson\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [COMM#50], [COMM#50, avg(Income#271) AS avg_income#292]\n",
      "+- Project [Income#271, COMM#50]\n",
      "   +- Join Inner, (ZipCode#265 = ZCTA20#70)\n",
      "      :- Project [trim(Zip Code#259, None) AS ZipCode#265, cast(regexp_replace(Estimated Median Income#261, [$,], , 1) as float) AS Income#271]\n",
      "      :  +- Filter ((isnotnull(Community#260) AND isnotnull(Estimated Median Income#261)) AND (RLIKE(Community#260, (?i)Los Angeles) AND (((isnotnull(trim(Zip Code#259, None)) AND NOT (trim(Zip Code#259, None) = )) AND NOT (trim(Zip Code#259, None) = NULL)) AND isnotnull(cast(regexp_replace(Estimated Median Income#261, [$,], , 1) as float)))))\n",
      "      :     +- Relation [Zip Code#259,Community#260,Estimated Median Income#261] csv\n",
      "      +- Aggregate [ZCTA20#70, COMM#50], [ZCTA20#70, COMM#50]\n",
      "         +- Project [features#33.properties.ZCTA20 AS ZCTA20#70, features#33.properties.COMM AS COMM#50]\n",
      "            +- Filter (isnotnull(features#33.properties.CITY) AND ((features#33.properties.CITY = Los Angeles) AND ((isnotnull(features#33.properties.ZCTA20) AND NOT (features#33.properties.ZCTA20 = )) AND NOT (features#33.properties.ZCTA20 = NULL))))\n",
      "               +- Generate explode(features#25), [0], false, [features#33]\n",
      "                  +- Project [features#25]\n",
      "                     +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "                        +- Relation [crs#24,features#25,name#26,type#27] geojson\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[COMM#50], functions=[avg(Income#271)], output=[COMM#50, avg_income#292], schema specialized)\n",
      "   +- Exchange hashpartitioning(COMM#50, 1000), ENSURE_REQUIREMENTS, [plan_id=3408]\n",
      "      +- HashAggregate(keys=[COMM#50], functions=[partial_avg(Income#271)], output=[COMM#50, sum#723, count#724L], schema specialized)\n",
      "         +- Project [Income#271, COMM#50]\n",
      "            +- BroadcastHashJoin [ZipCode#265], [ZCTA20#70], Inner, BuildLeft, false\n",
      "               :- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=3403]\n",
      "               :  +- Project [trim(Zip Code#259, None) AS ZipCode#265, cast(regexp_replace(Estimated Median Income#261, [$,], , 1) as float) AS Income#271]\n",
      "               :     +- Filter ((((((isnotnull(Community#260) AND isnotnull(Estimated Median Income#261)) AND RLIKE(Community#260, (?i)Los Angeles)) AND isnotnull(trim(Zip Code#259, None))) AND NOT (trim(Zip Code#259, None) = )) AND NOT (trim(Zip Code#259, None) = NULL)) AND isnotnull(cast(regexp_replace(Estimated Median Income#261, [$,], , 1) as float)))\n",
      "               :        +- FileScan csv [Zip Code#259,Community#260,Estimated Median Income#261] Batched: false, DataFilters: [isnotnull(Community#260), isnotnull(Estimated Median Income#261), RLIKE(Community#260, (?i)Los A..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_i..., PartitionFilters: [], PushedFilters: [IsNotNull(Community), IsNotNull(Estimated Median Income)], ReadSchema: struct<Zip Code:string,Community:string,Estimated Median Income:string>\n",
      "               +- HashAggregate(keys=[ZCTA20#70, COMM#50], functions=[], output=[ZCTA20#70, COMM#50], schema specialized)\n",
      "                  +- Exchange hashpartitioning(ZCTA20#70, COMM#50, 1000), ENSURE_REQUIREMENTS, [plan_id=3400]\n",
      "                     +- HashAggregate(keys=[ZCTA20#70, COMM#50], functions=[], output=[ZCTA20#70, COMM#50], schema specialized)\n",
      "                        +- Project [features#33.properties.ZCTA20 AS ZCTA20#70, features#33.properties.COMM AS COMM#50]\n",
      "                           +- Filter (isnotnull(features#33.properties.CITY) AND ((features#33.properties.CITY = Los Angeles) AND ((isnotnull(features#33.properties.ZCTA20) AND NOT (features#33.properties.ZCTA20 = )) AND NOT (features#33.properties.ZCTA20 = NULL))))\n",
      "                              +- Generate explode(features#25), false, [features#33]\n",
      "                                 +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "                                    +- FileScan geojson [features#25] Batched: false, DataFilters: [(size(features#25, true) > 0), isnotnull(features#25)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG20:string,BG20FIP_CURRENT:string..."
     ]
    }
   ],
   "source": [
    "income_per_comm.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84ae6ea7-73ae-4e1e-a371-5131acf7512f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba9450783c9f48b5a88acf6308fd6e6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Project [COMM#50, year#352, TotalPopulation#222L, coalesce(annual_crime_count#516L, cast(0 as bigint)) AS annual_crime_count#525L]\n",
      "+- Aggregate [COMM#50, year#352, TotalPopulation#222L], [COMM#50, year#352, TotalPopulation#222L, count(1) AS annual_crime_count#516L]\n",
      "   +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "      :- Project [DR_NO#295, DateRptd#296, DATEOCC#297, TIMEOCC#298, AREA#299, AREANAME#300, RptDistNo#301, Part#302, CrmCd#303, Crm Cd Desc#304, Mocodes#305, Vict Age#306, VictSex#307, VictDescent#308, PremisCd#309, PremisDesc#310, WeaponUsedCd#311, WeaponDesc#312, Status#313, Status Desc#314, CrmCd1#315, CrmCd2#316, CrmCd3#317, CrmCd4#318, ... 6 more fields]\n",
      "      :  +- Filter year#352 IN (2020,2021)\n",
      "      :     +- Project [DR_NO#295, DateRptd#296, DATEOCC#297, TIMEOCC#298, AREA#299, AREANAME#300, RptDistNo#301, Part#302, CrmCd#303, Crm Cd Desc#304, Mocodes#305, Vict Age#306, VictSex#307, VictDescent#308, PremisCd#309, PremisDesc#310, WeaponUsedCd#311, WeaponDesc#312, Status#313, Status Desc#314, CrmCd1#315, CrmCd2#316, CrmCd3#317, CrmCd4#318, ... 5 more fields]\n",
      "      :        +- Filter (NOT (LAT#321 = 0.0) AND NOT (LON#322 = 0.0))\n",
      "      :           +- Relation [DR_NO#295,DateRptd#296,DATEOCC#297,TIMEOCC#298,AREA#299,AREANAME#300,RptDistNo#301,Part#302,CrmCd#303,Crm Cd Desc#304,Mocodes#305,Vict Age#306,VictSex#307,VictDescent#308,PremisCd#309,PremisDesc#310,WeaponUsedCd#311,WeaponDesc#312,Status#313,Status Desc#314,CrmCd1#315,CrmCd2#316,CrmCd3#317,CrmCd4#318,... 4 more fields] csv\n",
      "      +- Project [COMM#50, geometry#220, TotalPopulation#222L]\n",
      "         +- Filter (((isnotnull(COMM#50) AND NOT (COMM#50 = )) AND NOT (COMM#50 = NULL)) AND (TotalPopulation#222L > cast(0 as bigint)))\n",
      "            +- Aggregate [COMM#50], [COMM#50, st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@6de89f59, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, Some(ST_Union_Aggr)) AS geometry#220, first(POP20#62L, false) AS TotalPopulation#222L]\n",
      "               +- Filter (CITY#46 = Los Angeles)\n",
      "                  +- Project [properties#37.BG20 AS BG20#42, properties#37.BG20FIP_CURRENT AS BG20FIP_CURRENT#43, properties#37.BGFIP20 AS BGFIP20#44, properties#37.CB20 AS CB20#45, properties#37.CITY AS CITY#46, properties#37.CITYCOMM AS CITYCOMM#47, properties#37.CITYCOMM_CURRENT AS CITYCOMM_CURRENT#48, properties#37.CITY_CURRENT AS CITY_CURRENT#49, properties#37.COMM AS COMM#50, properties#37.COMM_CURRENT AS COMM_CURRENT#51, properties#37.COUNTY AS COUNTY#52, properties#37.CT20 AS CT20#53, properties#37.CTCB20 AS CTCB20#54, properties#37.FEAT_TYPE AS FEAT_TYPE#55, properties#37.FIP20 AS FIP20#56, properties#37.FIP_CURRENT AS FIP_CURRENT#57, properties#37.HD22 AS HD22#58L, properties#37.HD_NAME AS HD_NAME#59, properties#37.HOUSING20 AS HOUSING20#60L, properties#37.OBJECTID AS OBJECTID#61L, properties#37.POP20 AS POP20#62L, properties#37.SPA22 AS SPA22#63L, properties#37.SPA_NAME AS SPA_NAME#64, properties#37.SUP21 AS SUP21#65, ... 6 more fields]\n",
      "                     +- Project [features#33.geometry AS geometry#36, features#33.properties AS properties#37, features#33.type AS type#38]\n",
      "                        +- Project [features#33]\n",
      "                           +- Generate explode(features#25), false, [features#33]\n",
      "                              +- Relation [crs#24,features#25,name#26,type#27] geojson\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "COMM: string, year: int, TotalPopulation: bigint, annual_crime_count: bigint\n",
      "Project [COMM#50, year#352, TotalPopulation#222L, coalesce(annual_crime_count#516L, cast(0 as bigint)) AS annual_crime_count#525L]\n",
      "+- Aggregate [COMM#50, year#352, TotalPopulation#222L], [COMM#50, year#352, TotalPopulation#222L, count(1) AS annual_crime_count#516L]\n",
      "   +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "      :- Project [DR_NO#295, DateRptd#296, DATEOCC#297, TIMEOCC#298, AREA#299, AREANAME#300, RptDistNo#301, Part#302, CrmCd#303, Crm Cd Desc#304, Mocodes#305, Vict Age#306, VictSex#307, VictDescent#308, PremisCd#309, PremisDesc#310, WeaponUsedCd#311, WeaponDesc#312, Status#313, Status Desc#314, CrmCd1#315, CrmCd2#316, CrmCd3#317, CrmCd4#318, ... 6 more fields]\n",
      "      :  +- Filter year#352 IN (2020,2021)\n",
      "      :     +- Project [DR_NO#295, DateRptd#296, DATEOCC#297, TIMEOCC#298, AREA#299, AREANAME#300, RptDistNo#301, Part#302, CrmCd#303, Crm Cd Desc#304, Mocodes#305, Vict Age#306, VictSex#307, VictDescent#308, PremisCd#309, PremisDesc#310, WeaponUsedCd#311, WeaponDesc#312, Status#313, Status Desc#314, CrmCd1#315, CrmCd2#316, CrmCd3#317, CrmCd4#318, ... 5 more fields]\n",
      "      :        +- Filter (NOT (LAT#321 = 0.0) AND NOT (LON#322 = 0.0))\n",
      "      :           +- Relation [DR_NO#295,DateRptd#296,DATEOCC#297,TIMEOCC#298,AREA#299,AREANAME#300,RptDistNo#301,Part#302,CrmCd#303,Crm Cd Desc#304,Mocodes#305,Vict Age#306,VictSex#307,VictDescent#308,PremisCd#309,PremisDesc#310,WeaponUsedCd#311,WeaponDesc#312,Status#313,Status Desc#314,CrmCd1#315,CrmCd2#316,CrmCd3#317,CrmCd4#318,... 4 more fields] csv\n",
      "      +- Project [COMM#50, geometry#220, TotalPopulation#222L]\n",
      "         +- Filter (((isnotnull(COMM#50) AND NOT (COMM#50 = )) AND NOT (COMM#50 = NULL)) AND (TotalPopulation#222L > cast(0 as bigint)))\n",
      "            +- Aggregate [COMM#50], [COMM#50, st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@6de89f59, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, Some(ST_Union_Aggr)) AS geometry#220, first(POP20#62L, false) AS TotalPopulation#222L]\n",
      "               +- Filter (CITY#46 = Los Angeles)\n",
      "                  +- Project [properties#37.BG20 AS BG20#42, properties#37.BG20FIP_CURRENT AS BG20FIP_CURRENT#43, properties#37.BGFIP20 AS BGFIP20#44, properties#37.CB20 AS CB20#45, properties#37.CITY AS CITY#46, properties#37.CITYCOMM AS CITYCOMM#47, properties#37.CITYCOMM_CURRENT AS CITYCOMM_CURRENT#48, properties#37.CITY_CURRENT AS CITY_CURRENT#49, properties#37.COMM AS COMM#50, properties#37.COMM_CURRENT AS COMM_CURRENT#51, properties#37.COUNTY AS COUNTY#52, properties#37.CT20 AS CT20#53, properties#37.CTCB20 AS CTCB20#54, properties#37.FEAT_TYPE AS FEAT_TYPE#55, properties#37.FIP20 AS FIP20#56, properties#37.FIP_CURRENT AS FIP_CURRENT#57, properties#37.HD22 AS HD22#58L, properties#37.HD_NAME AS HD_NAME#59, properties#37.HOUSING20 AS HOUSING20#60L, properties#37.OBJECTID AS OBJECTID#61L, properties#37.POP20 AS POP20#62L, properties#37.SPA22 AS SPA22#63L, properties#37.SPA_NAME AS SPA_NAME#64, properties#37.SUP21 AS SUP21#65, ... 6 more fields]\n",
      "                     +- Project [features#33.geometry AS geometry#36, features#33.properties AS properties#37, features#33.type AS type#38]\n",
      "                        +- Project [features#33]\n",
      "                           +- Generate explode(features#25), false, [features#33]\n",
      "                              +- Relation [crs#24,features#25,name#26,type#27] geojson\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [COMM#50, year#352, TotalPopulation#222L], [COMM#50, year#352, TotalPopulation#222L, count(1) AS annual_crime_count#525L]\n",
      "+- Project [year#352, COMM#50, TotalPopulation#222L]\n",
      "   +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "      :- Project [year(cast(DATEOCC#297 as date)) AS year#352,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#382]\n",
      "      :  +- Filter (((isnotnull(LAT#321) AND isnotnull(LON#322)) AND ((NOT (LAT#321 = 0.0) AND NOT (LON#322 = 0.0)) AND year(cast(DATEOCC#297 as date)) IN (2020,2021))) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "      :     +- Relation [DR_NO#295,DateRptd#296,DATEOCC#297,TIMEOCC#298,AREA#299,AREANAME#300,RptDistNo#301,Part#302,CrmCd#303,Crm Cd Desc#304,Mocodes#305,Vict Age#306,VictSex#307,VictDescent#308,PremisCd#309,PremisDesc#310,WeaponUsedCd#311,WeaponDesc#312,Status#313,Status Desc#314,CrmCd1#315,CrmCd2#316,CrmCd3#317,CrmCd4#318,... 4 more fields] csv\n",
      "      +- Filter ((isnotnull(TotalPopulation#222L) AND (TotalPopulation#222L > 0)) AND isnotnull(geometry#220))\n",
      "         +- Aggregate [COMM#50], [COMM#50, st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@6de89f59, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, Some(ST_Union_Aggr)) AS geometry#220, first(POP20#62L, false) AS TotalPopulation#222L]\n",
      "            +- Project [features#33.properties.COMM AS COMM#50, features#33.properties.POP20 AS POP20#62L, features#33.geometry AS geometry#36]\n",
      "               +- Filter (isnotnull(features#33.properties.CITY) AND ((features#33.properties.CITY = Los Angeles) AND ((isnotnull(features#33.properties.COMM) AND NOT (features#33.properties.COMM = )) AND NOT (features#33.properties.COMM = NULL))))\n",
      "                  +- Generate explode(features#25), [0], false, [features#33]\n",
      "                     +- Project [features#25]\n",
      "                        +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "                           +- Relation [crs#24,features#25,name#26,type#27] geojson\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[COMM#50, year#352, TotalPopulation#222L], functions=[count(1)], output=[COMM#50, year#352, TotalPopulation#222L, annual_crime_count#525L], schema specialized)\n",
      "   +- Exchange hashpartitioning(COMM#50, year#352, TotalPopulation#222L, 1000), ENSURE_REQUIREMENTS, [plan_id=3540]\n",
      "      +- HashAggregate(keys=[COMM#50, year#352, TotalPopulation#222L], functions=[partial_count(1)], output=[COMM#50, year#352, TotalPopulation#222L, count#714L], schema specialized)\n",
      "         +- Project [year#352, COMM#50, TotalPopulation#222L]\n",
      "            +- RangeJoin crime_point#382: geometry, geometry#220: geometry, WITHIN\n",
      "               :- Project [year(cast(DATEOCC#297 as date)) AS year#352,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#382]\n",
      "               :  +- Filter (((((isnotnull(LAT#321) AND isnotnull(LON#322)) AND NOT (LAT#321 = 0.0)) AND NOT (LON#322 = 0.0)) AND year(cast(DATEOCC#297 as date)) IN (2020,2021)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "               :     +- FileScan csv [DATEOCC#297,LAT#321,LON#322] Batched: false, DataFilters: [isnotnull(LAT#321), isnotnull(LON#322), NOT (LAT#321 = 0.0), NOT (LON#322 = 0.0), year(cast(DATE..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON), Not(EqualTo(LAT,0.0)), Not(EqualTo(LON,0.0))], ReadSchema: struct<DATEOCC:timestamp,LAT:double,LON:double>\n",
      "               +- Filter ((isnotnull(TotalPopulation#222L) AND (TotalPopulation#222L > 0)) AND isnotnull(geometry#220))\n",
      "                  +- ObjectHashAggregate(keys=[COMM#50], functions=[st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@6de89f59, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, Some(ST_Union_Aggr)), first(POP20#62L, false)], output=[COMM#50, geometry#220, TotalPopulation#222L])\n",
      "                     +- Exchange hashpartitioning(COMM#50, 1000), ENSURE_REQUIREMENTS, [plan_id=3533]\n",
      "                        +- ObjectHashAggregate(keys=[COMM#50], functions=[partial_st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@6de89f59, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, Some(ST_Union_Aggr)), partial_first(POP20#62L, false)], output=[COMM#50, buf#716, first#719L, valueSet#720])\n",
      "                           +- Project [features#33.properties.COMM AS COMM#50, features#33.properties.POP20 AS POP20#62L, features#33.geometry AS geometry#36]\n",
      "                              +- Filter (isnotnull(features#33.properties.CITY) AND ((features#33.properties.CITY = Los Angeles) AND ((isnotnull(features#33.properties.COMM) AND NOT (features#33.properties.COMM = )) AND NOT (features#33.properties.COMM = NULL))))\n",
      "                                 +- Generate explode(features#25), false, [features#33]\n",
      "                                    +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "                                       +- FileScan geojson [features#25] Batched: false, DataFilters: [(size(features#25, true) > 0), isnotnull(features#25)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG20:string,BG20FIP_CURRENT:string..."
     ]
    }
   ],
   "source": [
    "crime_per_comm_year.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f172ba4-c815-4a46-b28e-e98b6022ef99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd35c026a43b4158b7f5ffc74532c189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['COMM], ['COMM, avg('crime_rate_per_person) AS annual_avg_crime_rate_per_person#542]\n",
      "+- Project [COMM#50, year#352, TotalPopulation#222L, annual_crime_count#525L, (cast(annual_crime_count#525L as double) / cast(TotalPopulation#222L as double)) AS crime_rate_per_person#530]\n",
      "   +- Project [COMM#50, year#352, TotalPopulation#222L, coalesce(annual_crime_count#516L, cast(0 as bigint)) AS annual_crime_count#525L]\n",
      "      +- Aggregate [COMM#50, year#352, TotalPopulation#222L], [COMM#50, year#352, TotalPopulation#222L, count(1) AS annual_crime_count#516L]\n",
      "         +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "            :- Project [DR_NO#295, DateRptd#296, DATEOCC#297, TIMEOCC#298, AREA#299, AREANAME#300, RptDistNo#301, Part#302, CrmCd#303, Crm Cd Desc#304, Mocodes#305, Vict Age#306, VictSex#307, VictDescent#308, PremisCd#309, PremisDesc#310, WeaponUsedCd#311, WeaponDesc#312, Status#313, Status Desc#314, CrmCd1#315, CrmCd2#316, CrmCd3#317, CrmCd4#318, ... 6 more fields]\n",
      "            :  +- Filter year#352 IN (2020,2021)\n",
      "            :     +- Project [DR_NO#295, DateRptd#296, DATEOCC#297, TIMEOCC#298, AREA#299, AREANAME#300, RptDistNo#301, Part#302, CrmCd#303, Crm Cd Desc#304, Mocodes#305, Vict Age#306, VictSex#307, VictDescent#308, PremisCd#309, PremisDesc#310, WeaponUsedCd#311, WeaponDesc#312, Status#313, Status Desc#314, CrmCd1#315, CrmCd2#316, CrmCd3#317, CrmCd4#318, ... 5 more fields]\n",
      "            :        +- Filter (NOT (LAT#321 = 0.0) AND NOT (LON#322 = 0.0))\n",
      "            :           +- Relation [DR_NO#295,DateRptd#296,DATEOCC#297,TIMEOCC#298,AREA#299,AREANAME#300,RptDistNo#301,Part#302,CrmCd#303,Crm Cd Desc#304,Mocodes#305,Vict Age#306,VictSex#307,VictDescent#308,PremisCd#309,PremisDesc#310,WeaponUsedCd#311,WeaponDesc#312,Status#313,Status Desc#314,CrmCd1#315,CrmCd2#316,CrmCd3#317,CrmCd4#318,... 4 more fields] csv\n",
      "            +- Project [COMM#50, geometry#220, TotalPopulation#222L]\n",
      "               +- Filter (((isnotnull(COMM#50) AND NOT (COMM#50 = )) AND NOT (COMM#50 = NULL)) AND (TotalPopulation#222L > cast(0 as bigint)))\n",
      "                  +- Aggregate [COMM#50], [COMM#50, st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@6de89f59, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, Some(ST_Union_Aggr)) AS geometry#220, first(POP20#62L, false) AS TotalPopulation#222L]\n",
      "                     +- Filter (CITY#46 = Los Angeles)\n",
      "                        +- Project [properties#37.BG20 AS BG20#42, properties#37.BG20FIP_CURRENT AS BG20FIP_CURRENT#43, properties#37.BGFIP20 AS BGFIP20#44, properties#37.CB20 AS CB20#45, properties#37.CITY AS CITY#46, properties#37.CITYCOMM AS CITYCOMM#47, properties#37.CITYCOMM_CURRENT AS CITYCOMM_CURRENT#48, properties#37.CITY_CURRENT AS CITY_CURRENT#49, properties#37.COMM AS COMM#50, properties#37.COMM_CURRENT AS COMM_CURRENT#51, properties#37.COUNTY AS COUNTY#52, properties#37.CT20 AS CT20#53, properties#37.CTCB20 AS CTCB20#54, properties#37.FEAT_TYPE AS FEAT_TYPE#55, properties#37.FIP20 AS FIP20#56, properties#37.FIP_CURRENT AS FIP_CURRENT#57, properties#37.HD22 AS HD22#58L, properties#37.HD_NAME AS HD_NAME#59, properties#37.HOUSING20 AS HOUSING20#60L, properties#37.OBJECTID AS OBJECTID#61L, properties#37.POP20 AS POP20#62L, properties#37.SPA22 AS SPA22#63L, properties#37.SPA_NAME AS SPA_NAME#64, properties#37.SUP21 AS SUP21#65, ... 6 more fields]\n",
      "                           +- Project [features#33.geometry AS geometry#36, features#33.properties AS properties#37, features#33.type AS type#38]\n",
      "                              +- Project [features#33]\n",
      "                                 +- Generate explode(features#25), false, [features#33]\n",
      "                                    +- Relation [crs#24,features#25,name#26,type#27] geojson\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "COMM: string, annual_avg_crime_rate_per_person: double\n",
      "Aggregate [COMM#50], [COMM#50, avg(crime_rate_per_person#530) AS annual_avg_crime_rate_per_person#542]\n",
      "+- Project [COMM#50, year#352, TotalPopulation#222L, annual_crime_count#525L, (cast(annual_crime_count#525L as double) / cast(TotalPopulation#222L as double)) AS crime_rate_per_person#530]\n",
      "   +- Project [COMM#50, year#352, TotalPopulation#222L, coalesce(annual_crime_count#516L, cast(0 as bigint)) AS annual_crime_count#525L]\n",
      "      +- Aggregate [COMM#50, year#352, TotalPopulation#222L], [COMM#50, year#352, TotalPopulation#222L, count(1) AS annual_crime_count#516L]\n",
      "         +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "            :- Project [DR_NO#295, DateRptd#296, DATEOCC#297, TIMEOCC#298, AREA#299, AREANAME#300, RptDistNo#301, Part#302, CrmCd#303, Crm Cd Desc#304, Mocodes#305, Vict Age#306, VictSex#307, VictDescent#308, PremisCd#309, PremisDesc#310, WeaponUsedCd#311, WeaponDesc#312, Status#313, Status Desc#314, CrmCd1#315, CrmCd2#316, CrmCd3#317, CrmCd4#318, ... 6 more fields]\n",
      "            :  +- Filter year#352 IN (2020,2021)\n",
      "            :     +- Project [DR_NO#295, DateRptd#296, DATEOCC#297, TIMEOCC#298, AREA#299, AREANAME#300, RptDistNo#301, Part#302, CrmCd#303, Crm Cd Desc#304, Mocodes#305, Vict Age#306, VictSex#307, VictDescent#308, PremisCd#309, PremisDesc#310, WeaponUsedCd#311, WeaponDesc#312, Status#313, Status Desc#314, CrmCd1#315, CrmCd2#316, CrmCd3#317, CrmCd4#318, ... 5 more fields]\n",
      "            :        +- Filter (NOT (LAT#321 = 0.0) AND NOT (LON#322 = 0.0))\n",
      "            :           +- Relation [DR_NO#295,DateRptd#296,DATEOCC#297,TIMEOCC#298,AREA#299,AREANAME#300,RptDistNo#301,Part#302,CrmCd#303,Crm Cd Desc#304,Mocodes#305,Vict Age#306,VictSex#307,VictDescent#308,PremisCd#309,PremisDesc#310,WeaponUsedCd#311,WeaponDesc#312,Status#313,Status Desc#314,CrmCd1#315,CrmCd2#316,CrmCd3#317,CrmCd4#318,... 4 more fields] csv\n",
      "            +- Project [COMM#50, geometry#220, TotalPopulation#222L]\n",
      "               +- Filter (((isnotnull(COMM#50) AND NOT (COMM#50 = )) AND NOT (COMM#50 = NULL)) AND (TotalPopulation#222L > cast(0 as bigint)))\n",
      "                  +- Aggregate [COMM#50], [COMM#50, st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@6de89f59, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, Some(ST_Union_Aggr)) AS geometry#220, first(POP20#62L, false) AS TotalPopulation#222L]\n",
      "                     +- Filter (CITY#46 = Los Angeles)\n",
      "                        +- Project [properties#37.BG20 AS BG20#42, properties#37.BG20FIP_CURRENT AS BG20FIP_CURRENT#43, properties#37.BGFIP20 AS BGFIP20#44, properties#37.CB20 AS CB20#45, properties#37.CITY AS CITY#46, properties#37.CITYCOMM AS CITYCOMM#47, properties#37.CITYCOMM_CURRENT AS CITYCOMM_CURRENT#48, properties#37.CITY_CURRENT AS CITY_CURRENT#49, properties#37.COMM AS COMM#50, properties#37.COMM_CURRENT AS COMM_CURRENT#51, properties#37.COUNTY AS COUNTY#52, properties#37.CT20 AS CT20#53, properties#37.CTCB20 AS CTCB20#54, properties#37.FEAT_TYPE AS FEAT_TYPE#55, properties#37.FIP20 AS FIP20#56, properties#37.FIP_CURRENT AS FIP_CURRENT#57, properties#37.HD22 AS HD22#58L, properties#37.HD_NAME AS HD_NAME#59, properties#37.HOUSING20 AS HOUSING20#60L, properties#37.OBJECTID AS OBJECTID#61L, properties#37.POP20 AS POP20#62L, properties#37.SPA22 AS SPA22#63L, properties#37.SPA_NAME AS SPA_NAME#64, properties#37.SUP21 AS SUP21#65, ... 6 more fields]\n",
      "                           +- Project [features#33.geometry AS geometry#36, features#33.properties AS properties#37, features#33.type AS type#38]\n",
      "                              +- Project [features#33]\n",
      "                                 +- Generate explode(features#25), false, [features#33]\n",
      "                                    +- Relation [crs#24,features#25,name#26,type#27] geojson\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [COMM#50], [COMM#50, avg(crime_rate_per_person#530) AS annual_avg_crime_rate_per_person#542]\n",
      "+- Aggregate [COMM#50, year#352, TotalPopulation#222L], [COMM#50, (cast(count(1) as double) / cast(TotalPopulation#222L as double)) AS crime_rate_per_person#530]\n",
      "   +- Project [year#352, COMM#50, TotalPopulation#222L]\n",
      "      +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "         :- Project [year(cast(DATEOCC#297 as date)) AS year#352,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#382]\n",
      "         :  +- Filter (((isnotnull(LAT#321) AND isnotnull(LON#322)) AND ((NOT (LAT#321 = 0.0) AND NOT (LON#322 = 0.0)) AND year(cast(DATEOCC#297 as date)) IN (2020,2021))) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "         :     +- Relation [DR_NO#295,DateRptd#296,DATEOCC#297,TIMEOCC#298,AREA#299,AREANAME#300,RptDistNo#301,Part#302,CrmCd#303,Crm Cd Desc#304,Mocodes#305,Vict Age#306,VictSex#307,VictDescent#308,PremisCd#309,PremisDesc#310,WeaponUsedCd#311,WeaponDesc#312,Status#313,Status Desc#314,CrmCd1#315,CrmCd2#316,CrmCd3#317,CrmCd4#318,... 4 more fields] csv\n",
      "         +- Filter ((isnotnull(TotalPopulation#222L) AND (TotalPopulation#222L > 0)) AND isnotnull(geometry#220))\n",
      "            +- Aggregate [COMM#50], [COMM#50, st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@6de89f59, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, Some(ST_Union_Aggr)) AS geometry#220, first(POP20#62L, false) AS TotalPopulation#222L]\n",
      "               +- Project [features#33.properties.COMM AS COMM#50, features#33.properties.POP20 AS POP20#62L, features#33.geometry AS geometry#36]\n",
      "                  +- Filter (isnotnull(features#33.properties.CITY) AND ((features#33.properties.CITY = Los Angeles) AND ((isnotnull(features#33.properties.COMM) AND NOT (features#33.properties.COMM = )) AND NOT (features#33.properties.COMM = NULL))))\n",
      "                     +- Generate explode(features#25), [0], false, [features#33]\n",
      "                        +- Project [features#25]\n",
      "                           +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "                              +- Relation [crs#24,features#25,name#26,type#27] geojson\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[COMM#50], functions=[avg(crime_rate_per_person#530)], output=[COMM#50, annual_avg_crime_rate_per_person#542], schema specialized)\n",
      "   +- Exchange hashpartitioning(COMM#50, 1000), ENSURE_REQUIREMENTS, [plan_id=3807]\n",
      "      +- HashAggregate(keys=[COMM#50], functions=[partial_avg(crime_rate_per_person#530)], output=[COMM#50, sum#711, count#712L], schema specialized)\n",
      "         +- HashAggregate(keys=[COMM#50, year#352, TotalPopulation#222L], functions=[count(1)], output=[COMM#50, crime_rate_per_person#530], schema specialized)\n",
      "            +- Exchange hashpartitioning(COMM#50, year#352, TotalPopulation#222L, 1000), ENSURE_REQUIREMENTS, [plan_id=3803]\n",
      "               +- HashAggregate(keys=[COMM#50, year#352, TotalPopulation#222L], functions=[partial_count(1)], output=[COMM#50, year#352, TotalPopulation#222L, count#714L], schema specialized)\n",
      "                  +- Project [year#352, COMM#50, TotalPopulation#222L]\n",
      "                     +- RangeJoin crime_point#382: geometry, geometry#220: geometry, WITHIN\n",
      "                        :- Project [year(cast(DATEOCC#297 as date)) AS year#352,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#382]\n",
      "                        :  +- Filter (((((isnotnull(LAT#321) AND isnotnull(LON#322)) AND NOT (LAT#321 = 0.0)) AND NOT (LON#322 = 0.0)) AND year(cast(DATEOCC#297 as date)) IN (2020,2021)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "                        :     +- FileScan csv [DATEOCC#297,LAT#321,LON#322] Batched: false, DataFilters: [isnotnull(LAT#321), isnotnull(LON#322), NOT (LAT#321 = 0.0), NOT (LON#322 = 0.0), year(cast(DATE..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON), Not(EqualTo(LAT,0.0)), Not(EqualTo(LON,0.0))], ReadSchema: struct<DATEOCC:timestamp,LAT:double,LON:double>\n",
      "                        +- Filter ((isnotnull(TotalPopulation#222L) AND (TotalPopulation#222L > 0)) AND isnotnull(geometry#220))\n",
      "                           +- ObjectHashAggregate(keys=[COMM#50], functions=[st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@6de89f59, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, Some(ST_Union_Aggr)), first(POP20#62L, false)], output=[COMM#50, geometry#220, TotalPopulation#222L])\n",
      "                              +- Exchange hashpartitioning(COMM#50, 1000), ENSURE_REQUIREMENTS, [plan_id=3796]\n",
      "                                 +- ObjectHashAggregate(keys=[COMM#50], functions=[partial_st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@6de89f59, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, Some(ST_Union_Aggr)), partial_first(POP20#62L, false)], output=[COMM#50, buf#716, first#719L, valueSet#720])\n",
      "                                    +- Project [features#33.properties.COMM AS COMM#50, features#33.properties.POP20 AS POP20#62L, features#33.geometry AS geometry#36]\n",
      "                                       +- Filter (isnotnull(features#33.properties.CITY) AND ((features#33.properties.CITY = Los Angeles) AND ((isnotnull(features#33.properties.COMM) AND NOT (features#33.properties.COMM = )) AND NOT (features#33.properties.COMM = NULL))))\n",
      "                                          +- Generate explode(features#25), false, [features#33]\n",
      "                                             +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "                                                +- FileScan geojson [features#25] Batched: false, DataFilters: [(size(features#25, true) > 0), isnotnull(features#25)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG20:string,BG20FIP_CURRENT:string..."
     ]
    }
   ],
   "source": [
    "crime_per_comm.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e97f045-5fad-4804-af21-afeac4e87dbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90a916f21c214f6f9bdaca2fbdcaeafa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Join UsingJoin(Inner, [COMM])\n",
      ":- Aggregate [COMM#50], [COMM#50, avg(crime_rate_per_person#530) AS annual_avg_crime_rate_per_person#542]\n",
      ":  +- Project [COMM#50, year#352, TotalPopulation#222L, annual_crime_count#525L, (cast(annual_crime_count#525L as double) / cast(TotalPopulation#222L as double)) AS crime_rate_per_person#530]\n",
      ":     +- Project [COMM#50, year#352, TotalPopulation#222L, coalesce(annual_crime_count#516L, cast(0 as bigint)) AS annual_crime_count#525L]\n",
      ":        +- Aggregate [COMM#50, year#352, TotalPopulation#222L], [COMM#50, year#352, TotalPopulation#222L, count(1) AS annual_crime_count#516L]\n",
      ":           +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      ":              :- Project [DR_NO#295, DateRptd#296, DATEOCC#297, TIMEOCC#298, AREA#299, AREANAME#300, RptDistNo#301, Part#302, CrmCd#303, Crm Cd Desc#304, Mocodes#305, Vict Age#306, VictSex#307, VictDescent#308, PremisCd#309, PremisDesc#310, WeaponUsedCd#311, WeaponDesc#312, Status#313, Status Desc#314, CrmCd1#315, CrmCd2#316, CrmCd3#317, CrmCd4#318, ... 6 more fields]\n",
      ":              :  +- Filter year#352 IN (2020,2021)\n",
      ":              :     +- Project [DR_NO#295, DateRptd#296, DATEOCC#297, TIMEOCC#298, AREA#299, AREANAME#300, RptDistNo#301, Part#302, CrmCd#303, Crm Cd Desc#304, Mocodes#305, Vict Age#306, VictSex#307, VictDescent#308, PremisCd#309, PremisDesc#310, WeaponUsedCd#311, WeaponDesc#312, Status#313, Status Desc#314, CrmCd1#315, CrmCd2#316, CrmCd3#317, CrmCd4#318, ... 5 more fields]\n",
      ":              :        +- Filter (NOT (LAT#321 = 0.0) AND NOT (LON#322 = 0.0))\n",
      ":              :           +- Relation [DR_NO#295,DateRptd#296,DATEOCC#297,TIMEOCC#298,AREA#299,AREANAME#300,RptDistNo#301,Part#302,CrmCd#303,Crm Cd Desc#304,Mocodes#305,Vict Age#306,VictSex#307,VictDescent#308,PremisCd#309,PremisDesc#310,WeaponUsedCd#311,WeaponDesc#312,Status#313,Status Desc#314,CrmCd1#315,CrmCd2#316,CrmCd3#317,CrmCd4#318,... 4 more fields] csv\n",
      ":              +- Project [COMM#50, geometry#220, TotalPopulation#222L]\n",
      ":                 +- Filter (((isnotnull(COMM#50) AND NOT (COMM#50 = )) AND NOT (COMM#50 = NULL)) AND (TotalPopulation#222L > cast(0 as bigint)))\n",
      ":                    +- Aggregate [COMM#50], [COMM#50, st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@6de89f59, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, Some(ST_Union_Aggr)) AS geometry#220, first(POP20#62L, false) AS TotalPopulation#222L]\n",
      ":                       +- Filter (CITY#46 = Los Angeles)\n",
      ":                          +- Project [properties#37.BG20 AS BG20#42, properties#37.BG20FIP_CURRENT AS BG20FIP_CURRENT#43, properties#37.BGFIP20 AS BGFIP20#44, properties#37.CB20 AS CB20#45, properties#37.CITY AS CITY#46, properties#37.CITYCOMM AS CITYCOMM#47, properties#37.CITYCOMM_CURRENT AS CITYCOMM_CURRENT#48, properties#37.CITY_CURRENT AS CITY_CURRENT#49, properties#37.COMM AS COMM#50, properties#37.COMM_CURRENT AS COMM_CURRENT#51, properties#37.COUNTY AS COUNTY#52, properties#37.CT20 AS CT20#53, properties#37.CTCB20 AS CTCB20#54, properties#37.FEAT_TYPE AS FEAT_TYPE#55, properties#37.FIP20 AS FIP20#56, properties#37.FIP_CURRENT AS FIP_CURRENT#57, properties#37.HD22 AS HD22#58L, properties#37.HD_NAME AS HD_NAME#59, properties#37.HOUSING20 AS HOUSING20#60L, properties#37.OBJECTID AS OBJECTID#61L, properties#37.POP20 AS POP20#62L, properties#37.SPA22 AS SPA22#63L, properties#37.SPA_NAME AS SPA_NAME#64, properties#37.SUP21 AS SUP21#65, ... 6 more fields]\n",
      ":                             +- Project [features#33.geometry AS geometry#36, features#33.properties AS properties#37, features#33.type AS type#38]\n",
      ":                                +- Project [features#33]\n",
      ":                                   +- Generate explode(features#25), false, [features#33]\n",
      ":                                      +- Relation [crs#24,features#25,name#26,type#27] geojson\n",
      "+- Aggregate [COMM#557], [COMM#557, avg(Income#271) AS avg_income#292]\n",
      "   +- Join Inner, (ZipCode#265 = ZCTA20#577)\n",
      "      :- Project [ZipCode#265, Income#271]\n",
      "      :  +- Filter (((isnotnull(ZipCode#265) AND NOT (ZipCode#265 = )) AND NOT (ZipCode#265 = NULL)) AND isnotnull(Income#271))\n",
      "      :     +- Project [Zip Code#259, Community#260, Estimated Median Income#261, ZipCode#265, cast(regexp_replace(Estimated Median Income#261, [$,], , 1) as float) AS Income#271]\n",
      "      :        +- Filter RLIKE(Community#260, (?i)Los Angeles)\n",
      "      :           +- Project [Zip Code#259, Community#260, Estimated Median Income#261, trim(Zip Code#259, None) AS ZipCode#265]\n",
      "      :              +- Relation [Zip Code#259,Community#260,Estimated Median Income#261] csv\n",
      "      +- Deduplicate [ZCTA20#577, COMM#557]\n",
      "         +- Filter ((isnotnull(ZCTA20#577) AND NOT (ZCTA20#577 = )) AND NOT (ZCTA20#577 = NULL))\n",
      "            +- Project [ZCTA20#577, COMM#557]\n",
      "               +- Filter (CITY#553 = Los Angeles)\n",
      "                  +- Project [properties#37.BG20 AS BG20#549, properties#37.BG20FIP_CURRENT AS BG20FIP_CURRENT#550, properties#37.BGFIP20 AS BGFIP20#551, properties#37.CB20 AS CB20#552, properties#37.CITY AS CITY#553, properties#37.CITYCOMM AS CITYCOMM#554, properties#37.CITYCOMM_CURRENT AS CITYCOMM_CURRENT#555, properties#37.CITY_CURRENT AS CITY_CURRENT#556, properties#37.COMM AS COMM#557, properties#37.COMM_CURRENT AS COMM_CURRENT#558, properties#37.COUNTY AS COUNTY#559, properties#37.CT20 AS CT20#560, properties#37.CTCB20 AS CTCB20#561, properties#37.FEAT_TYPE AS FEAT_TYPE#562, properties#37.FIP20 AS FIP20#563, properties#37.FIP_CURRENT AS FIP_CURRENT#564, properties#37.HD22 AS HD22#565L, properties#37.HD_NAME AS HD_NAME#566, properties#37.HOUSING20 AS HOUSING20#567L, properties#37.OBJECTID AS OBJECTID#568L, properties#37.POP20 AS POP20#569L, properties#37.SPA22 AS SPA22#570L, properties#37.SPA_NAME AS SPA_NAME#571, properties#37.SUP21 AS SUP21#572, ... 6 more fields]\n",
      "                     +- Project [features#33.geometry AS geometry#36, features#33.properties AS properties#37, features#33.type AS type#38]\n",
      "                        +- Project [features#33]\n",
      "                           +- Generate explode(features#546), false, [features#33]\n",
      "                              +- Relation [crs#545,features#546,name#547,type#548] geojson\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "COMM: string, annual_avg_crime_rate_per_person: double, avg_income: double\n",
      "Project [COMM#50, annual_avg_crime_rate_per_person#542, avg_income#292]\n",
      "+- Join Inner, (COMM#50 = COMM#557)\n",
      "   :- Aggregate [COMM#50], [COMM#50, avg(crime_rate_per_person#530) AS annual_avg_crime_rate_per_person#542]\n",
      "   :  +- Project [COMM#50, year#352, TotalPopulation#222L, annual_crime_count#525L, (cast(annual_crime_count#525L as double) / cast(TotalPopulation#222L as double)) AS crime_rate_per_person#530]\n",
      "   :     +- Project [COMM#50, year#352, TotalPopulation#222L, coalesce(annual_crime_count#516L, cast(0 as bigint)) AS annual_crime_count#525L]\n",
      "   :        +- Aggregate [COMM#50, year#352, TotalPopulation#222L], [COMM#50, year#352, TotalPopulation#222L, count(1) AS annual_crime_count#516L]\n",
      "   :           +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "   :              :- Project [DR_NO#295, DateRptd#296, DATEOCC#297, TIMEOCC#298, AREA#299, AREANAME#300, RptDistNo#301, Part#302, CrmCd#303, Crm Cd Desc#304, Mocodes#305, Vict Age#306, VictSex#307, VictDescent#308, PremisCd#309, PremisDesc#310, WeaponUsedCd#311, WeaponDesc#312, Status#313, Status Desc#314, CrmCd1#315, CrmCd2#316, CrmCd3#317, CrmCd4#318, ... 6 more fields]\n",
      "   :              :  +- Filter year#352 IN (2020,2021)\n",
      "   :              :     +- Project [DR_NO#295, DateRptd#296, DATEOCC#297, TIMEOCC#298, AREA#299, AREANAME#300, RptDistNo#301, Part#302, CrmCd#303, Crm Cd Desc#304, Mocodes#305, Vict Age#306, VictSex#307, VictDescent#308, PremisCd#309, PremisDesc#310, WeaponUsedCd#311, WeaponDesc#312, Status#313, Status Desc#314, CrmCd1#315, CrmCd2#316, CrmCd3#317, CrmCd4#318, ... 5 more fields]\n",
      "   :              :        +- Filter (NOT (LAT#321 = 0.0) AND NOT (LON#322 = 0.0))\n",
      "   :              :           +- Relation [DR_NO#295,DateRptd#296,DATEOCC#297,TIMEOCC#298,AREA#299,AREANAME#300,RptDistNo#301,Part#302,CrmCd#303,Crm Cd Desc#304,Mocodes#305,Vict Age#306,VictSex#307,VictDescent#308,PremisCd#309,PremisDesc#310,WeaponUsedCd#311,WeaponDesc#312,Status#313,Status Desc#314,CrmCd1#315,CrmCd2#316,CrmCd3#317,CrmCd4#318,... 4 more fields] csv\n",
      "   :              +- Project [COMM#50, geometry#220, TotalPopulation#222L]\n",
      "   :                 +- Filter (((isnotnull(COMM#50) AND NOT (COMM#50 = )) AND NOT (COMM#50 = NULL)) AND (TotalPopulation#222L > cast(0 as bigint)))\n",
      "   :                    +- Aggregate [COMM#50], [COMM#50, st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@6de89f59, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, Some(ST_Union_Aggr)) AS geometry#220, first(POP20#62L, false) AS TotalPopulation#222L]\n",
      "   :                       +- Filter (CITY#46 = Los Angeles)\n",
      "   :                          +- Project [properties#37.BG20 AS BG20#42, properties#37.BG20FIP_CURRENT AS BG20FIP_CURRENT#43, properties#37.BGFIP20 AS BGFIP20#44, properties#37.CB20 AS CB20#45, properties#37.CITY AS CITY#46, properties#37.CITYCOMM AS CITYCOMM#47, properties#37.CITYCOMM_CURRENT AS CITYCOMM_CURRENT#48, properties#37.CITY_CURRENT AS CITY_CURRENT#49, properties#37.COMM AS COMM#50, properties#37.COMM_CURRENT AS COMM_CURRENT#51, properties#37.COUNTY AS COUNTY#52, properties#37.CT20 AS CT20#53, properties#37.CTCB20 AS CTCB20#54, properties#37.FEAT_TYPE AS FEAT_TYPE#55, properties#37.FIP20 AS FIP20#56, properties#37.FIP_CURRENT AS FIP_CURRENT#57, properties#37.HD22 AS HD22#58L, properties#37.HD_NAME AS HD_NAME#59, properties#37.HOUSING20 AS HOUSING20#60L, properties#37.OBJECTID AS OBJECTID#61L, properties#37.POP20 AS POP20#62L, properties#37.SPA22 AS SPA22#63L, properties#37.SPA_NAME AS SPA_NAME#64, properties#37.SUP21 AS SUP21#65, ... 6 more fields]\n",
      "   :                             +- Project [features#33.geometry AS geometry#36, features#33.properties AS properties#37, features#33.type AS type#38]\n",
      "   :                                +- Project [features#33]\n",
      "   :                                   +- Generate explode(features#25), false, [features#33]\n",
      "   :                                      +- Relation [crs#24,features#25,name#26,type#27] geojson\n",
      "   +- Aggregate [COMM#557], [COMM#557, avg(Income#271) AS avg_income#292]\n",
      "      +- Join Inner, (ZipCode#265 = ZCTA20#577)\n",
      "         :- Project [ZipCode#265, Income#271]\n",
      "         :  +- Filter (((isnotnull(ZipCode#265) AND NOT (ZipCode#265 = )) AND NOT (ZipCode#265 = NULL)) AND isnotnull(Income#271))\n",
      "         :     +- Project [Zip Code#259, Community#260, Estimated Median Income#261, ZipCode#265, cast(regexp_replace(Estimated Median Income#261, [$,], , 1) as float) AS Income#271]\n",
      "         :        +- Filter RLIKE(Community#260, (?i)Los Angeles)\n",
      "         :           +- Project [Zip Code#259, Community#260, Estimated Median Income#261, trim(Zip Code#259, None) AS ZipCode#265]\n",
      "         :              +- Relation [Zip Code#259,Community#260,Estimated Median Income#261] csv\n",
      "         +- Deduplicate [ZCTA20#577, COMM#557]\n",
      "            +- Filter ((isnotnull(ZCTA20#577) AND NOT (ZCTA20#577 = )) AND NOT (ZCTA20#577 = NULL))\n",
      "               +- Project [ZCTA20#577, COMM#557]\n",
      "                  +- Filter (CITY#553 = Los Angeles)\n",
      "                     +- Project [properties#37.BG20 AS BG20#549, properties#37.BG20FIP_CURRENT AS BG20FIP_CURRENT#550, properties#37.BGFIP20 AS BGFIP20#551, properties#37.CB20 AS CB20#552, properties#37.CITY AS CITY#553, properties#37.CITYCOMM AS CITYCOMM#554, properties#37.CITYCOMM_CURRENT AS CITYCOMM_CURRENT#555, properties#37.CITY_CURRENT AS CITY_CURRENT#556, properties#37.COMM AS COMM#557, properties#37.COMM_CURRENT AS COMM_CURRENT#558, properties#37.COUNTY AS COUNTY#559, properties#37.CT20 AS CT20#560, properties#37.CTCB20 AS CTCB20#561, properties#37.FEAT_TYPE AS FEAT_TYPE#562, properties#37.FIP20 AS FIP20#563, properties#37.FIP_CURRENT AS FIP_CURRENT#564, properties#37.HD22 AS HD22#565L, properties#37.HD_NAME AS HD_NAME#566, properties#37.HOUSING20 AS HOUSING20#567L, properties#37.OBJECTID AS OBJECTID#568L, properties#37.POP20 AS POP20#569L, properties#37.SPA22 AS SPA22#570L, properties#37.SPA_NAME AS SPA_NAME#571, properties#37.SUP21 AS SUP21#572, ... 6 more fields]\n",
      "                        +- Project [features#33.geometry AS geometry#36, features#33.properties AS properties#37, features#33.type AS type#38]\n",
      "                           +- Project [features#33]\n",
      "                              +- Generate explode(features#546), false, [features#33]\n",
      "                                 +- Relation [crs#545,features#546,name#547,type#548] geojson\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [COMM#50, annual_avg_crime_rate_per_person#542, avg_income#292]\n",
      "+- Join Inner, (COMM#50 = COMM#557)\n",
      "   :- Aggregate [COMM#50], [COMM#50, avg(crime_rate_per_person#530) AS annual_avg_crime_rate_per_person#542]\n",
      "   :  +- Aggregate [COMM#50, year#352, TotalPopulation#222L], [COMM#50, (cast(count(1) as double) / cast(TotalPopulation#222L as double)) AS crime_rate_per_person#530]\n",
      "   :     +- Project [year#352, COMM#50, TotalPopulation#222L]\n",
      "   :        +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Within**\n",
      "   :           :- Project [year(cast(DATEOCC#297 as date)) AS year#352,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#382]\n",
      "   :           :  +- Filter (((isnotnull(LAT#321) AND isnotnull(LON#322)) AND ((NOT (LAT#321 = 0.0) AND NOT (LON#322 = 0.0)) AND year(cast(DATEOCC#297 as date)) IN (2020,2021))) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "   :           :     +- Relation [DR_NO#295,DateRptd#296,DATEOCC#297,TIMEOCC#298,AREA#299,AREANAME#300,RptDistNo#301,Part#302,CrmCd#303,Crm Cd Desc#304,Mocodes#305,Vict Age#306,VictSex#307,VictDescent#308,PremisCd#309,PremisDesc#310,WeaponUsedCd#311,WeaponDesc#312,Status#313,Status Desc#314,CrmCd1#315,CrmCd2#316,CrmCd3#317,CrmCd4#318,... 4 more fields] csv\n",
      "   :           +- Filter ((isnotnull(TotalPopulation#222L) AND (TotalPopulation#222L > 0)) AND isnotnull(geometry#220))\n",
      "   :              +- Aggregate [COMM#50], [COMM#50, st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@6de89f59, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, Some(ST_Union_Aggr)) AS geometry#220, first(POP20#62L, false) AS TotalPopulation#222L]\n",
      "   :                 +- Project [features#33.properties.COMM AS COMM#50, features#33.properties.POP20 AS POP20#62L, features#33.geometry AS geometry#36]\n",
      "   :                    +- Filter (((((isnotnull(features#33.properties.CITY) AND (features#33.properties.CITY = Los Angeles)) AND isnotnull(features#33.properties.COMM)) AND NOT (features#33.properties.COMM = )) AND NOT (features#33.properties.COMM = NULL)) AND bloomfilter#1520 of [COMM#557] filtering [features#33.properties.COMM])\n",
      "   :                       :  +- Aggregate [ZCTA20#577, COMM#557], [ZCTA20#577, COMM#557]\n",
      "   :                       :     +- Project [features#33.properties.ZCTA20 AS ZCTA20#577, features#33.properties.COMM AS COMM#557]\n",
      "   :                       :        +- Filter ((isnotnull(features#33.properties.CITY) AND ((features#33.properties.CITY = Los Angeles) AND ((isnotnull(features#33.properties.ZCTA20) AND NOT (features#33.properties.ZCTA20 = )) AND NOT (features#33.properties.ZCTA20 = NULL)))) AND ((NOT (features#33.properties.COMM = ) AND NOT (features#33.properties.COMM = NULL)) AND isnotnull(features#33.properties.COMM)))\n",
      "   :                       :           +- Generate explode(features#546), [0], false, [features#33]\n",
      "   :                       :              +- Project [features#546]\n",
      "   :                       :                 +- Filter ((size(features#546, true) > 0) AND isnotnull(features#546))\n",
      "   :                       :                    +- Relation [crs#545,features#546,name#547,type#548] geojson\n",
      "   :                       +- Generate explode(features#25), [0], false, [features#33]\n",
      "   :                          +- Project [features#25]\n",
      "   :                             +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "   :                                +- Relation [crs#24,features#25,name#26,type#27] geojson\n",
      "   +- Aggregate [COMM#557], [COMM#557, avg(Income#271) AS avg_income#292]\n",
      "      +- Project [Income#271, COMM#557]\n",
      "         +- Join Inner, (ZipCode#265 = ZCTA20#577)\n",
      "            :- Project [trim(Zip Code#259, None) AS ZipCode#265, cast(regexp_replace(Estimated Median Income#261, [$,], , 1) as float) AS Income#271]\n",
      "            :  +- Filter ((isnotnull(Community#260) AND isnotnull(Estimated Median Income#261)) AND (RLIKE(Community#260, (?i)Los Angeles) AND (((isnotnull(trim(Zip Code#259, None)) AND NOT (trim(Zip Code#259, None) = )) AND NOT (trim(Zip Code#259, None) = NULL)) AND isnotnull(cast(regexp_replace(Estimated Median Income#261, [$,], , 1) as float)))))\n",
      "            :     +- Relation [Zip Code#259,Community#260,Estimated Median Income#261] csv\n",
      "            +- Aggregate [ZCTA20#577, COMM#557], [ZCTA20#577, COMM#557]\n",
      "               +- Project [features#33.properties.ZCTA20 AS ZCTA20#577, features#33.properties.COMM AS COMM#557]\n",
      "                  +- Filter ((isnotnull(features#33.properties.CITY) AND ((features#33.properties.CITY = Los Angeles) AND ((isnotnull(features#33.properties.ZCTA20) AND NOT (features#33.properties.ZCTA20 = )) AND NOT (features#33.properties.ZCTA20 = NULL)))) AND ((NOT (features#33.properties.COMM = ) AND NOT (features#33.properties.COMM = NULL)) AND isnotnull(features#33.properties.COMM)))\n",
      "                     +- Generate explode(features#546), [0], false, [features#33]\n",
      "                        +- Project [features#546]\n",
      "                           +- Filter ((size(features#546, true) > 0) AND isnotnull(features#546))\n",
      "                              +- Relation [crs#545,features#546,name#547,type#548] geojson\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [COMM#50, annual_avg_crime_rate_per_person#542, avg_income#292]\n",
      "   +- SortMergeJoin [COMM#50], [COMM#557], Inner\n",
      "      :- Sort [COMM#50 ASC NULLS FIRST], false, 0\n",
      "      :  +- HashAggregate(keys=[COMM#50], functions=[avg(crime_rate_per_person#530)], output=[COMM#50, annual_avg_crime_rate_per_person#542], schema specialized)\n",
      "      :     +- Exchange hashpartitioning(COMM#50, 1000), ENSURE_REQUIREMENTS, [plan_id=4124]\n",
      "      :        +- HashAggregate(keys=[COMM#50], functions=[partial_avg(crime_rate_per_person#530)], output=[COMM#50, sum#711, count#712L], schema specialized)\n",
      "      :           +- HashAggregate(keys=[COMM#50, year#352, TotalPopulation#222L], functions=[count(1)], output=[COMM#50, crime_rate_per_person#530], schema specialized)\n",
      "      :              +- Exchange hashpartitioning(COMM#50, year#352, TotalPopulation#222L, 1000), ENSURE_REQUIREMENTS, [plan_id=4121]\n",
      "      :                 +- HashAggregate(keys=[COMM#50, year#352, TotalPopulation#222L], functions=[partial_count(1)], output=[COMM#50, year#352, TotalPopulation#222L, count#714L], schema specialized)\n",
      "      :                    +- Project [year#352, COMM#50, TotalPopulation#222L]\n",
      "      :                       +- RangeJoin crime_point#382: geometry, geometry#220: geometry, WITHIN\n",
      "      :                          :- Project [year(cast(DATEOCC#297 as date)) AS year#352,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS crime_point#382]\n",
      "      :                          :  +- Filter (((((isnotnull(LAT#321) AND isnotnull(LON#322)) AND NOT (LAT#321 = 0.0)) AND NOT (LON#322 = 0.0)) AND year(cast(DATEOCC#297 as date)) IN (2020,2021)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "      :                          :     +- FileScan csv [DATEOCC#297,LAT#321,LON#322] Batched: false, DataFilters: [isnotnull(LAT#321), isnotnull(LON#322), NOT (LAT#321 = 0.0), NOT (LON#322 = 0.0), year(cast(DATE..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(LAT), IsNotNull(LON), Not(EqualTo(LAT,0.0)), Not(EqualTo(LON,0.0))], ReadSchema: struct<DATEOCC:timestamp,LAT:double,LON:double>\n",
      "      :                          +- Filter ((isnotnull(TotalPopulation#222L) AND (TotalPopulation#222L > 0)) AND isnotnull(geometry#220))\n",
      "      :                             +- ObjectHashAggregate(keys=[COMM#50], functions=[st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@6de89f59, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, Some(ST_Union_Aggr)), first(POP20#62L, false)], output=[COMM#50, geometry#220, TotalPopulation#222L])\n",
      "      :                                +- Exchange hashpartitioning(COMM#50, 1000), ENSURE_REQUIREMENTS, [plan_id=4115]\n",
      "      :                                   +- ObjectHashAggregate(keys=[COMM#50], functions=[partial_st_union_aggr(geometry#36, org.apache.spark.sql.sedona_sql.expressions.ST_Union_Aggr@6de89f59, class[value[0]: geometry], class[value[0]: array<geometry>], true, true, 0, 0, Some(ST_Union_Aggr)), partial_first(POP20#62L, false)], output=[COMM#50, buf#716, first#719L, valueSet#720])\n",
      "      :                                      +- Project [features#33.properties.COMM AS COMM#50, features#33.properties.POP20 AS POP20#62L, features#33.geometry AS geometry#36]\n",
      "      :                                         +- Filter ((((isnotnull(features#33.properties.CITY) AND (features#33.properties.CITY = Los Angeles)) AND isnotnull(features#33.properties.COMM)) AND NOT (features#33.properties.COMM = )) AND NOT (features#33.properties.COMM = NULL))\n",
      "      :                                            +- Generate explode(features#25), false, [features#33]\n",
      "      :                                               +- Filter ((size(features#25, true) > 0) AND isnotnull(features#25))\n",
      "      :                                                  +- FileScan geojson [features#25] Batched: false, DataFilters: [(size(features#25, true) > 0), isnotnull(features#25)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG20:string,BG20FIP_CURRENT:string...\n",
      "      +- Sort [COMM#557 ASC NULLS FIRST], false, 0\n",
      "         +- HashAggregate(keys=[COMM#557], functions=[avg(Income#271)], output=[COMM#557, avg_income#292], schema specialized)\n",
      "            +- Exchange hashpartitioning(COMM#557, 1000), ENSURE_REQUIREMENTS, [plan_id=4087]\n",
      "               +- HashAggregate(keys=[COMM#557], functions=[partial_avg(Income#271)], output=[COMM#557, sum#723, count#724L], schema specialized)\n",
      "                  +- Project [Income#271, COMM#557]\n",
      "                     +- BroadcastHashJoin [ZipCode#265], [ZCTA20#577], Inner, BuildLeft, false\n",
      "                        :- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=4082]\n",
      "                        :  +- Project [trim(Zip Code#259, None) AS ZipCode#265, cast(regexp_replace(Estimated Median Income#261, [$,], , 1) as float) AS Income#271]\n",
      "                        :     +- Filter ((((((isnotnull(Community#260) AND isnotnull(Estimated Median Income#261)) AND RLIKE(Community#260, (?i)Los Angeles)) AND isnotnull(trim(Zip Code#259, None))) AND NOT (trim(Zip Code#259, None) = )) AND NOT (trim(Zip Code#259, None) = NULL)) AND isnotnull(cast(regexp_replace(Estimated Median Income#261, [$,], , 1) as float)))\n",
      "                        :        +- FileScan csv [Zip Code#259,Community#260,Estimated Median Income#261] Batched: false, DataFilters: [isnotnull(Community#260), isnotnull(Estimated Median Income#261), RLIKE(Community#260, (?i)Los A..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_i..., PartitionFilters: [], PushedFilters: [IsNotNull(Community), IsNotNull(Estimated Median Income)], ReadSchema: struct<Zip Code:string,Community:string,Estimated Median Income:string>\n",
      "                        +- HashAggregate(keys=[ZCTA20#577, COMM#557], functions=[], output=[ZCTA20#577, COMM#557], schema specialized)\n",
      "                           +- Exchange hashpartitioning(ZCTA20#577, COMM#557, 1000), ENSURE_REQUIREMENTS, [plan_id=4079]\n",
      "                              +- HashAggregate(keys=[ZCTA20#577, COMM#557], functions=[], output=[ZCTA20#577, COMM#557], schema specialized)\n",
      "                                 +- Project [features#33.properties.ZCTA20 AS ZCTA20#577, features#33.properties.COMM AS COMM#557]\n",
      "                                    +- Filter ((isnotnull(features#33.properties.CITY) AND ((features#33.properties.CITY = Los Angeles) AND ((isnotnull(features#33.properties.ZCTA20) AND NOT (features#33.properties.ZCTA20 = )) AND NOT (features#33.properties.ZCTA20 = NULL)))) AND ((NOT (features#33.properties.COMM = ) AND NOT (features#33.properties.COMM = NULL)) AND isnotnull(features#33.properties.COMM)))\n",
      "                                       +- Generate explode(features#546), false, [features#33]\n",
      "                                          +- Filter ((size(features#546, true) > 0) AND isnotnull(features#546))\n",
      "                                             +- FileScan geojson [features#546] Batched: false, DataFilters: [(size(features#546, true) > 0), isnotnull(features#546)], Format: GEOJSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/project_data/LA_C..., PartitionFilters: [], PushedFilters: [IsNotNull(features)], ReadSchema: struct<features:array<struct<geometry:binary,properties:struct<BG20:string,BG20FIP_CURRENT:string..."
     ]
    }
   ],
   "source": [
    "final_df.explain(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
